<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>A Statistical Approach to the Spam Problem</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Using Bayesian statistics to detect an e-mail's spamminess.&#10;    "><meta name="keywords" content="spam, statistics, e-mail, Spambayes"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x178e580.0x1885ab0"></a>A Statistical Approach to the Spam Problem</h1></div><div><div class="author"><h3 class="author">Gary Robinson</h3></div><div class="issuemoyr">Issue #107, March 2003</div></div><div><p>
    Using Bayesian statistics to detect an e-mail's spamminess.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x178e580.0x1886500"></a></h2></div></div><p>Most people are spending significant time
daily on the task of distinguishing spam from useful e-mail. I
don't think I'm alone in feeling that we have better things to do.
Spam-filtering software can help.
</p><p>This article discusses one of many possible mathematical
foundations for a key aspect of spam filtering&mdash;generating an
indicator of &ldquo;spamminess&rdquo; from a collection of tokens
representing the content of an e-mail.</p><p>The approach described here truly has been a distributed
effort in the best open-source tradition. Paul Graham, an author of
books on Lisp, suggested an approach to filtering spam in his
on-line article, &ldquo;A Plan for Spam&rdquo;. I took his approach for
generating probabilities associated with words, altered it slightly
and proposed a Bayesian calculation for dealing with words that
hadn't appeared very often. Then I suggested an approach based on
the chi-square distribution for combining the individual word
probabilities into a combined probability (actually a pair of
probabilities&mdash;see below) representing an e-mail. Finally, Tim
Peters of the Spambayes Project proposed a way of generating a
particularly useful spamminess indicator based on the combined
probabilities. All along the way the work was guided by ongoing
testing of embodiments written in Python by Tim Peters for
Spambayes and in C by Greg Louis of the Bogofilter Project. The
testing was done by a number of people involved with those
projects.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x178e580.0x18866b8"></a>Generating Word Probabilities</h2></div></div><p>We will assume the existence of a body of e-mails (the
corpus) for training, together with software capable of parsing
each e-mail into its constituent words. We will further assume that
each training e-mail has been classified manually as either &ldquo;ham&rdquo;
(the e-mail you want to read) or &ldquo;spam&rdquo; (the e-mail you don't).
We will use this data and software to train our system by
generating a probability for each word that represents its
spamminess.</p><p>For each word that appears in the corpus, we
calculate:</p><div class="itemizedlist"><ul type="disc"><li><p><span   class="emphasis"><em>b</em></span>(<span   class="emphasis"><em>w</em></span>) =
(the number of spam e-mails containing the word
<span   class="emphasis"><em>w</em></span>) / (the total number of spam
e-mails).</p></li><li><p><span   class="emphasis"><em>g</em></span>(<span   class="emphasis"><em>w</em></span>) =
(the number of ham e-mails containing the word
<span   class="emphasis"><em>w</em></span>) / (the total number of ham e-mails).</p></li><li><p><span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) =
<span   class="emphasis"><em>b</em></span>(<span   class="emphasis"><em>w</em></span>) /
(<span   class="emphasis"><em>b</em></span>(<span   class="emphasis"><em>w</em></span>) +
<span   class="emphasis"><em>g</em></span>(<span   class="emphasis"><em>w</em></span>))</p></li></ul></div><p><span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) can be roughly
interpreted as the probability that a randomly chosen e-mail
containing word <span   class="emphasis"><em>w</em></span> will be a spam.
Spam-filtering programs can compute
<span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) for every word in an
e-mail and use that information as the basis for further
calculations to determine whether the e-mail is ham or spam.</p><p>However, there is one notable wrinkle. In the real world, a
particular person's inbox may contain 10% spam or 90% spam.
Obviously, this will have an impact on the probability that, for
that individual, an e-mail containing a given word will be spam or
ham.</p><p>That impact is ignored in the calculation above. Rather, that
calculation, in effect, approximates the probabilitily that a
randomly chosen e-mail containing <span   class="emphasis"><em>w</em></span> would be
spam in a world where half the e-mails were spams and half were
hams. The merit of that approach is based on the assumption that we
don't want it to be harder or easier for an e-mail to be classified
as spam just because of the relative proportion of spams and hams
we happen to receive. Rather, we want e-mails to be judged purely
based on their own characteristics. In practice, this assumption
has worked out quite well.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x178e580.0x1887268"></a>Dealing with Rare Words</h2></div></div><p>There is a problem with probabilities calculated as above
when words are very rare. For instance, if a word appears in
exactly one e-mail, and it is a spam, the calculated
<span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) is 1.0. But clearly
it is not absolutely certain that all future e-mail containing that
word will be spam; in fact, we simply don't have enough data to
know the real probability.</p><p>Bayesian statistics gives us a powerful technique for dealing
with such cases. This branch of statistics is based on the idea
that we are interested in a person's degree of belief in a
particular event; that is the Bayesian probability of the
event.</p><p>When exactly one e-mail contains a particular word and that
e-mail is spam, our degree of belief that the next time we see that
word it will be in a spam is not 100%. That's because we also have
our own background information that guides us. We know from
experience that virtually any word can appear in either a spam or
non-spam context, and that one or a handful of data points is not
enough to be completely certain we know the real probability. The
Bayesian approach lets us combine our general background
information with the data we have collected for a word in such a
way that both aspects are given their proper importance. In this
way, we determine an appropriate degree of belief about whether,
when we see the word again, it will be in a spam.</p><p>We calculate this degree of belief,
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>), as follows:</p><div       class="mediaobject"><img src="6467f1.jpg"><div class="caption"><p>
Equation 1
</p></div></div><p>where:</p><div class="itemizedlist"><ul type="disc"><li><p><span   class="emphasis"><em>s</em></span> is the strength we want to
give to our background information.</p></li><li><p><span   class="emphasis"><em>x</em></span> is our assumed probability,
based on our general backround information, that a word we don't
have any other experience of will first appear in a spam.</p></li><li><p><span   class="emphasis"><em>n</em></span> is the number of e-mails we
have received that contain word <span   class="emphasis"><em>w</em></span>.</p></li></ul></div><p>This gives us the convenient use of <span   class="emphasis"><em>x</em></span> to
represent our assumed probability from background information and
<span   class="emphasis"><em>s</em></span> as the strength we will give that
assumption. In practice, the values for <span   class="emphasis"><em>s</em></span> and
<span   class="emphasis"><em>x</em></span> are found through testing to optimize
performance. Reasonable starting points are 1 and .5 for
<span   class="emphasis"><em>s</em></span> and <span   class="emphasis"><em>x</em></span>,
respectively.</p><p>We will use <span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)
rather than <span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) in our
calculations so that we are working with reasonable probabilities
rather than the unrealistically extreme values that can often occur
when we don't have enough data. This formula gives us a simple way
of handling the case of no data at all; in that case,
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) is exactly our
assumed probability based on background information.</p><p>Those who are interested in the derivation of the above
formula, read on; others may want to skip down to the next
section.</p><p>If you are already familiar with the principles of Bayesian
statistics, you will probably have no trouble understanding the
derivation. Otherwise, I suggest that you read sections 1 and 2 of
David Heckerman's &ldquo;A Tutorial on Learning with Bayesian Networks&rdquo;
(see Resources) before continuing.</p><p>The formula above is based on assuming that spam/ham
classification for e-mails containing word <span   class="emphasis"><em>w</em></span>
is a binomial random variable with a beta distribution prior. We
calculate the posterior expectation after incorporating the
observed data. We are going to do a test that is the equivalent of
the &ldquo;experiment&rdquo; of tossing a coin multiple times to see whether
it appears to be biased. There are <span   class="emphasis"><em>n</em></span> trials.
If we were tossing a coin, each coin toss would be a trial, and
we'd be counting the number of heads. But in our case, the trial is
looking at the next e-mail in the training corpus that contains the
word &ldquo;porn&rdquo; and seeing whether the e-mail it contains is a spam.
If it is, the experiment is considered to have been successful.
Clearly, it's a binomial experiment: there are two values, yes or
no. And they are independent: the fact that one e-mail contains
&ldquo;porn&rdquo; isn't correlated to the question of whether the next one
will. Now, it happens that if you have a binomial experiment and
assume a beta distribution for the prior, you can express the
expectation that the <span   class="emphasis"><em>n</em></span>th + 1 trial will be
successful as shown in Equation 2.</p><div       class="mediaobject"><img src="6467f2.jpg"><div class="caption"><p>
Equation 2
</p></div></div><div class="itemizedlist"><ul type="disc"><li><p><span   class="emphasis"><em>q</em></span> is the number of
successes.</p></li><li><p><span   class="emphasis"><em>n</em></span> is the number of
trials.</p></li><li><p><span   class="emphasis"><em>u</em></span> and <span   class="emphasis"><em>v</em></span>
are the parameters of the beta distribution.</p></li></ul></div><p>We want to show that Equation 1 is equivalent to Equation 2.
First perform the following substitutions:</p><p><span   class="emphasis"><em>s</em></span> = <span   class="emphasis"><em>u</em></span> +
<span   class="emphasis"><em>v</em></span> <span   class="emphasis"><em>s</em></span> *
<span   class="emphasis"><em>x</em></span> = <span   class="emphasis"><em>u</em></span></p><p>The next step is to replace <span   class="emphasis"><em>q</em></span> with
<span   class="emphasis"><em>n</em></span> *
<span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>). We have already
discussed the fact that
<span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) is an approximation
of the probability that a randomly chosen e-mail containing
<span   class="emphasis"><em>w</em></span> is spam in an imaginary world where there
are the same number of spams as hams. So <span   class="emphasis"><em>n</em></span> *
<span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) approximates the
count of spams containing <span   class="emphasis"><em>w</em></span> in that world.
This approximates the count of successes in our experiment and is
therefore the equivalent of <span   class="emphasis"><em>q</em></span>. This completes
our demonstration of the equivalence of Equations 1 and 2.</p><p>In testing, replacing
<span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) with
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) in all calculations
where <span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>) would
otherwise be used, has uniformly resulted in more reliable spam/ham
classification.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x178e580.0x187fa40"></a>Combining the Probabilities</h2></div></div><p>At this point, we can compute a probability,
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>), for each word that
may appear in a new e-mail. This probability reflects our degree of
belief, based on our background knowledge and on data in our
training corpus, that an e-mail chosen randomly from those that
contain <span   class="emphasis"><em>w</em></span> will be a spam.</p><p>So each e-mail is represented by a set of of probabilities.
We want to combine these individual probabilities into an overall
combined indicator of spamminess or hamminess for the e-mail as a
whole.</p><p>In the field of statistics known as meta-analysis, probably
the most common way of combining probabilities is due to R. A.
Fisher. If we have a set probabilities, <span   class="emphasis"><em>p</em></span>1,
<span   class="emphasis"><em>p</em></span>2, ..., <span   class="emphasis"><em>p</em></span>n, we can do
the following. First, calculate -2ln <span   class="emphasis"><em>p</em></span>1 *
<span   class="emphasis"><em>p</em></span>2 * ... * <span   class="emphasis"><em>p</em></span>n. Then,
consider the result to have a chi-square distribution with
2<span   class="emphasis"><em>n</em></span> degrees of freedom, and use a chi-square
table to compute the probability of getting a result as extreme, or
more extreme, than the one calculated. This &ldquo;combined&rdquo;
probability meaningfully summarizes all the individual
probabilities.</p><p>The initial set of probabilities are considered to be with
respect to a null hypothesis. For instance, if we make the null
hypothesis assumption that a coin is unbiased, and then flip it ten
times and get ten heads in a row, there would be a resultant
probability of (1/2)10 = 1/1024. It would be an unlikely event with
respect to the null hypothesis, but of course, if the coin actually
were to be biased, it wouldn't necessarily be quite so unlikely to
have an extreme outcome. Therefore, we might reject the null
hypothesis and instead choose to accept the alternate hypothesis
that the coin is biased.</p><p>We can use the same calculation to combine our
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s. The
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s are not real
physical probabilities. Rather, they can be thought of as our best
guess about those probabilities. But in traditional meta-anaytical
uses of the Fisher calculation they are not known to be the real
probabilities either. Instead, they are assumed to be, as part of
the null hypothesis.</p><p>Let our null hypothesis be &ldquo;The
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s are accurate, and
the present e-mail is a random collection of words, each
independent of the others, such that the
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s are not in a
uniform distribution.&rdquo; Now suppose we have a word, &ldquo;Python&rdquo;, for
which <span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) is .01. We
believe it occurs in spams only 1% of the time. Then to the extent
that our belief is correct, an unlikely event occurred; one with a
probability of .01. Similarly, every word in the e-mail has a
probability associated with it. Very spammy words like porn might
have probabilities of .99 or greater.</p><p>Then we use the Fisher calculation to compute an overall
probability for the whole set of words. If the e-mail is a ham, it
is likely that it will have a number of very low probabilities and
relatively few very high probabilities to balance them, with the
result that the Fisher calculation will give a very low combined
probability. This will allow us to reject the null hypothesis and
assume instead the alternative hypothesis that the e-mail is a
ham.</p><p>Let us call this combined probability
<span   class="emphasis"><em>H</em></span>:</p><div       class="mediaobject"><img src="6467f3.jpg"><div class="caption"><p>
Equation 3
</p></div></div><p>where <span   class="emphasis"><em>C</em></span>-1() is the inverse chi-square
function, used to derive a p-value from a chi-square-distributed
random variable.</p><p>Of course, we know from the outset the null hypothesis is
false. Virtually no e-mail is actually a random collection of words
unbiased with regard to hamminess or spamminess; an e-mail usually
has a telling number of words of one type or the other. And
certainly words are not independent. E-mails containing &ldquo;sex&rdquo; are
more likely to contain &ldquo;porn&rdquo;, and e-mails containing the name of
a friend who programs Python are more likely also to contain
&ldquo;Python&rdquo;. And, the
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s are not in a
uniform distribution. But for purposes of spam detection, those
departures from reality usually work in our favor. They cause the
probabilities to have a nonrandom tendency to be either high or low
in a given e-mail, giving us a strong statistical basis to reject
the null hypothesis in favor of one alternative hypothesis or the
other. Either the e-mail is a ham or it is a spam.</p><p>So the null hypothesis is a kind of straw dog. We set it up
only in order for it to be knocked down in one direction or the
other.</p><p>It may be worthwhile to mention here a key difference between
this approach and many other combining approaches that assume the
probabilities are independent. This difference applies, for
example, to such approaches as the &ldquo;Bayesian chain rule&rdquo; and
&ldquo;Na&iuml;ve Bayesian classification&rdquo;. Those approaches have
been tested in head-to-head combat against the approach described
here using large numbers of human-classified e-mails and didn't
fare as well (i.e., didn't agree as consistently with human
judgment).</p><p>The calculations for those approaches are technically invalid
due to requiring an independence of the data points that is not
actually present. That problem does not occur when using the Fisher
technique because the validity of the calculations doesn't depend
on the data being independent. The Fisher calculation is structured
such that we are rejecting a null hypothesis that includes
independence in favor of one of the two alternative hypotheses that
are of interest to us. These alternative hypotheses each involve
non-independence, such as the correlation between &ldquo;sex&rdquo; and
&ldquo;porn&rdquo;, as part of the phenomenon that creates the extreme
combined probabilities.</p><p>These combined probabilities are only probabilities under the
assumption that the null hypothesis&mdash;known to be almost certainly
false&mdash;is true. In the real world, the Fisher combined probability
is not a probability at all, but rather an abstract indicator of
how much (or little) we should be inclined to accept the null
hypothesis. It is not meant to be a true probability when the null
hypothesis is false, so the fact that it isn't doesn't cause
computational problems for us. The na&iuml;ve Bayesian
classifier is known to be resistant to distortions due to a lack of
independence, but it doesn't avoid them entirely.</p><p>Whether these differing responses to a lack of independence
in the data are responsible for the superior performance of Fisher
in spam/ham classification testing to date is not known, but it is
something to consider as one possible factor.</p><p>The individual
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s are only
approximations to real probabilities (i.e., when there is very
little data about a word, our best guess about its spam probability
as given by <span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) may not
reflect its actual reality). But if you consider how
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) is calculated, you
will see that this uncertainty diminishes asymptotically as
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) approaches 0 or 1,
because such extreme values can be achieved only by words that have
occurred quite frequently in the training data and either almost
always appear in spams or almost always appear in hams. And,
conveniently, it's the numbers near 0 that have by far the greatest
impact on the calculations. To see this, consider the influence on
the product .01 * .5 if the first term is changed to .001, vs. the
influence on the product .51 *.5 if the first term is changed to
.501, and recall that the Fisher technique is based on multiplying
the probabilities. So our null hypothesis is violated by the fact
that the <span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s are not
completely reliable, but in a way that matters vanishingly little
for the words of the most interest in the search for evidence of
hamminess: the words with
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) near 0.</p><p>Alert readers will wonder at this point: &ldquo;Okay, I understand
that these Fisher calculations seem to make sense for hammy words
with correspondingly near-0 probabilities, but what about spammy
words with probabilities near 1?&rdquo; Good question! Read on for the
answer that will complete our discussion.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x178e580.0x1b94480"></a>The Indicator of Hamminess or Spamminess</h2></div></div><p>The calculation described above is sensitive to evidence of
hamminess, particularly when it's in the form of words that show up
in far more hams than spams. This is because probabilities near 0
have a great influence on the product of probabilities, which is at
the heart of Fisher's calculation. In fact, there is a 1971 theorem
that says the Fisher technique is, under certain circumstances, as
powerful as any technique can possibly be for revealing underlying
trends in a product of possibilities (see Resources).</p><p>However, very spam-oriented words have
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s near 1, and
therefore have a much less significant effect on the calculations.
Now, it might be assumed that this is a good thing. After all, for
many people, misclassifying a good e-mail as spam seems a lot worse
than misclassifying a bad e-mail as a ham, because no great harm is
done if a single spam gets through but significant harm might
result from a single good e-mail being wrongly classified as spam
and therefore ignored by the recipient. So it may seem good to be
sensitive to indications of hamminess and less sensitive to
indications of spamminess.</p><p>However, there are ways to deal with this problem that in
real-world testing do not add a noticeable tendency to wrongly
classify good e-mail as spam, but do significantly reduce the
tendency to misclassify spam as ham.</p><p>The most effective technique that has been identified in
recent testing efforts follows.</p><p>First, &ldquo;reverse&rdquo; all the probabilities by subtracting them
from 1 (that is, for each word, calculate 1 -
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)). Because
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) represents the
probability that a randomly chosen e-mail from the set of e-mails
containing <span   class="emphasis"><em>w</em></span> is a spam, 1 -
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>) represents the
probability that such a randomly chosen e-mail will be a
ham.</p><p>Now do the same Fisher calculation as before, but on the (1 -
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>))s rather than on the
<span   class="emphasis"><em>f</em></span>(<span   class="emphasis"><em>w</em></span>)s. This will result
in near-0 combined probabilities, in rejection of the null
hypothesis, when a lot of very spammy words are present. Call this
combined probability <span   class="emphasis"><em>S</em></span>.</p><p>Now calculate:</p><div       class="mediaobject"><img src="6467f4.jpg"><div class="caption"><p>
Equation 4
</p></div></div><p><span   class="emphasis"><em>I</em></span> is an indicator that is near 1 when
the preponderance of the evidence is in favor of the conclusion
that the e-mail is spam and near 0 when the evidence points to the
conclusion that it's ham. This indicator has a couple of
interesting characteristics.</p><p>Suppose an e-mail has a number of very spammy words and also
a number of very hammy words. Because the Fisher technique is
sensitive to values near 0 and less sensitive to values near 1, the
result might be that both <span   class="emphasis"><em>S</em></span> and
<span   class="emphasis"><em>H</em></span> are very near 0. For instance,
<span   class="emphasis"><em>S</em></span> might be on the order of .00001 and
<span   class="emphasis"><em>H</em></span> might be on the order of .000000001. In
fact, those kinds of results are not as infrequent as one might
assume in real-world e-mails. One example is when a friend forwards
a spam to another friend as part of an e-mail conversation about
spam. In such a case, there will be strong evidence in favor of
both possible conclusions.</p><p>In many approaches, such as those based on the Bayesian chain
rule, the fact that there may be more spammy words than hammy words
in an example will tend to make the classifier absolutely certain
that the e-mail is spam. But in fact, it's not so clear; for
instance, the forwarded e-mail example is not spam.</p><p>So it a useful characteristic of <span   class="emphasis"><em>I</em></span> that
it is near .5 in such cases, just as it is near .5 when there is no
particular evidence in one direction or the other. When there is
significant evidence in favor of both conclusions,
<span   class="emphasis"><em>I</em></span> takes the cautious approach. In real-world
testing, human examination of these mid-valued e-mails tends to
support the conclusion that they really should be classified
somewhere in the middle rather than being subject to the
black-or-white approach of most classifiers.</p><p>The Spambayes Project, described in Richie Hindle's article
on page 52, takes advantage of this by marking e-mails with
<span   class="emphasis"><em>I</em></span> near .5 as uncertain. This allows the e-mail
recipient to give a bit more attention to e-mails that can't be
classified with confidence. This lessens the chance of a good
e-mail being ignored due to incorrect classification.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x178e580.0x1b95298"></a>Future Directions</h2></div></div><p>To date, the software using this approach is based on one
word per token. Other approaches are possible, such as building a
hash table of phrases. It is expected that the math described here
can be employed in those contexts as well, and there is reason to
believe that phrase-based systems will have performance advantages,
although there is controversy about that idea. Future
<i  >Linux Journal</i> articles can be expected to
cover any developments in such directions. CRM114 (see Resources)
is an example of a phrase-based system that has performed very
well, but at the time of this writing it hasn't been directly
tested against other approaches on the same corpus. (At the time of
this writing, CRM114 is using the Bayesian chain rule to combine
<span   class="emphasis"><em>p</em></span>(<span   class="emphasis"><em>w</em></span>)s.)</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x178e580.0x1b954a8"></a>Conclusion</h2></div></div><p>The techniques described here have been used in projects such
as Spambayes and Bogofilter to improve performance of the
spam-filtering task significantly. Future developments, which may
include integrating these calculations with a phrase-based
approach, can be expected to achieve even better
performance.</p><p><a href="6467s2.html" target="_self">A Python Implementation of the
Inverse Chi-Square Function</a></p><p><a href="6467s1.html" target="_self">Resources</a></p></div></div>
<div class="authorblurb"><p><span   class="bold"><b>Gary Robinson</b></span> is
      CEO of Transpose, LLC
      (<a href="http://www.transpose.com" target="_self">www.transpose.com</a>),
      a company specializing in internet trust and reputation solutions.
      He has worked in the field of collaborative filtering since 1985.
      His personal weblog, which frequently covers spam-related
      developments, is
      <a href="http://radio.weblogs.com/0101454" target="_self">radio.weblogs.com/0101454</a>,
      and he can be contacted at
      <a href="mailto:grobinson@transpose.com">grobinson@transpose.com</a>.</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../107/toc107.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>