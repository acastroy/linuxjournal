<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
High-Availability Storage with HA-LVM
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Deploy a storage solution with zero downtime.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1748580.0x183fac0"></a>
High-Availability Storage with HA-LVM
</h1></div><div><div class="author"><h3 class="author">
Petros
 
Koutoupis
</h3></div><div class="issuemoyr">Issue #247, November 2014</div></div><div><p>
Deploy a storage solution with zero downtime.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x18401f8"></a></h2></div></div><p>
In recent years, there has been a trend in which data centers have been
opting for commodity hardware and software over proprietary solutions. Why
shouldn't they? It offers extremely low costs and the flexibility to
build an ecosystem the way it is preferred. The only limitation is the extent
of the administrator's imagination. However, a question needs to be
asked: &ldquo;How would such a customized solution compare to its proprietary
and more costly counterpart?&rdquo;
</p><p>
Open-source projects have evolved and matured enough to stay competitive
and provide the same feature-rich solutions that include volume
management, data snapshots, data deduplication and so on. Although an often
overlooked and longtime-supported concept is high availability.
</p><p>
The idea behind high availability is simple: eliminate any single
point of failure. This ensures that if a server node or a path to the
underlying storage goes down (planned or unplanned), data requests 
still can be served. Now there are multiple layers to a storage-deployed
solution that can be configured for high availability and that is why
this article focuses strictly on HA-LVM.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x18404b8"></a>
HA-LVM</h2></div></div><p>
High Availability Logical Volume Manager (HA-LVM) is an add-on to the
already integrated LVM suite. It enables a failover configuration for
shared volumes&mdash;that is, if one server in a cluster fails or is taken down
for maintenance, the shared storage configuration will fail over to the
secondary server where all I/O requests will resume, uninterrupted. An
HA-LVM configuration is an active/passive configuration. This means that
a single server accesses the shared storage at any one time. In many
cases, this is an ideal approach, as some of advanced LVM features, such
as snapshot and data deduplication, are not supported in an active/active
environment (when more than one server accesses the shared storage).
</p><p>
A very important component to HA-LVM is the CLVM or Clustered LVM
d&aelig;mon. When enabled, the CLVM d&aelig;mon prevents corruption of LVM
metadata and its logical volumes, which occurs if multiple machines make
overlapping changes. Although in an active/passive configuration, this
becomes less of a concern. To accomplish this, the d&aelig;mon relies on a
Distributed Lock Manager or DLM. The purpose of the DLM is to coordinate
disk access for CLVM.
</p><p>
The following example will cluster two servers that have access to
the same external storage (Figure 1). This external storage could
be a RAID-enabled or JBOD enclosure of disk drives, connected to the
servers via a Fibre Channel, Serial Attached SCSI (SAS), iSCSI or
other Storage Area Network (SAN) mapping. The configuration is storage
protocol-agnostic and requires only that the clustered servers see the
same shared block devices.
</p><div       class="mediaobject"><a href="11723f1.large.jpg"><img src="11723f1.jpg"></a><div class="caption"><p>
Figure 1. A Sample Configuration
of Two Servers Accessing the Same Shared Storage
</p></div></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x1840880"></a></h2></div></div><div class="sidebar"><p class="title"><b>CLVM</b></p><p>
CLVM is not compatible with MD RAID, as it does not support clusters yet.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x1840a90"></a></h2></div></div><div class="sidebar"><p class="title"><b>CLVM D&aelig;mon</b></p><p>
The CLVM d&aelig;mon distributes LVM metadata updates across the cluster,
and it must be running on all nodes in that cluster.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x1840ca0"></a></h2></div></div><div class="sidebar"><p class="title"><b>JBOD</b></p><p>
A JBOD (or Just a Bunch Of Disks) is an architecture using multiple hard
drives, but not in a redundant configuration.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x1840eb0"></a>
Configuring the Cluster</h2></div></div><p>
Almost all Linux distributions offer the required packages. However, the
names may differ in each. You need to install lvm2-cluster (in
some distributions, the package may be named clvm), the Corosync cluster
engine, the Red Hat cluster manager (or cman), the Resource Group manager
d&aelig;mon (or rgmanager) and all their dependencies on all participating
servers. Even though the Red Hat cluster manager contains the Linux
distribution of the same name in its package description, most modern
distributions unrelated to Red Hat will list it in their repositories.
</p><p>
Once the appropriate clustering packages have been installed on
all participating servers, the cluster configuration file must be
configured to enable the cluster. To accomplish this, create and modify
/etc/cluster/cluster.conf with the following:

<pre     class="programlisting">

&lt;cluster name="lvm-cluster" config_version="1"&gt;
    &lt;cman two_node="1" expected_votes="1" /&gt;
    &lt;clusternodes&gt;
        &lt;clusternode name="serv-0001" nodeid="1"&gt;
            &lt;fence&gt;
            &lt;/fence&gt;
        &lt;/clusternode&gt;
        &lt;clusternode name="serv-0002" nodeid="2"&gt;
            &lt;fence&gt;
            &lt;/fence&gt;
        &lt;/clusternode&gt;
    &lt;/clusternodes&gt;
    &lt;logging debug="on"&gt;
    &lt;/logging&gt;
    &lt;dlm protocol="tcp" timewarn="500"&gt;
    &lt;/dlm&gt;
    &lt;fencedevices&gt;
    &lt;/fencedevices&gt;
    &lt;rm&gt;
    &lt;/rm&gt;
&lt;/cluster&gt;

</pre>
</p><p>
Note that the clusternode name is the server's hostname (change where
necessary). Also, make sure the cluster.conf file is identical on all
servers in the cluster.
</p><p>
The Red Hat cluster manager needs to be started:


<pre     class="programlisting">
$ sudo /etc/rc.d/init.d/cman start
Starting cluster: 
   Checking if cluster has been disabled at boot...   [  OK  ]
   Checking Network Manager...                        [  OK  ]
   Global setup...                                    [  OK  ]
   Loading kernel modules...                          [  OK  ]
   Mounting configfs...                               [  OK  ]
   Starting cman...                                   [  OK  ]
   Waiting for quorum...                              [  OK  ]
   Starting fenced...                                 [  OK  ]
   Starting dlm_controld...                           [  OK  ]
   Tuning DLM kernel config...                        [  OK  ]
   Starting gfs_controld...                           [  OK  ]
   Unfencing self...                                  [  OK  ]
   Joining fence domain...                            [  OK  ]
</pre>
</p><p>
If a single node in the cluster is not active, it will appear as off-line:


<pre     class="programlisting">
$ sudo clustat
Cluster Status for lvm-cluster @ Sun Aug  3 11:31:51 2014
Member Status: Quorate

 Member Name                 ID   Status
 ------ ----                 ---- ------
 serv-0001                        1 Online, Local
 serv-0002                        2 Offline
</pre>
</p><p>
Otherwise, when all servers are configured appropriately and the cman
service is enabled, all nodes will appear with an
<tt  >Online</tt> status:


<pre     class="programlisting">
$ sudo clustat
Cluster Status for lvm-cluster @ Sun Aug  3 11:36:43 2014
Member Status: Quorate

 Member Name                 ID   Status
 ------ ----                 ---- ------
 serv-0001                        1 Online
 serv-0002                        2 Online, Local
</pre>
</p><p>
You now have a working cluster. The next step is to enable the Clustered
LVM in High Availability mode. In this scenario, you have a single volume
from the shared storage enclosure mapped to both servers. Both servers
are able to observe and access this volume as /dev/sdb.
</p><p>
The /etc/lvm/lvm.conf file needs to be modified for this. The locking_type
parameter in the global section has to be set to the value 3. It is
set to 1 by default:


<pre     class="programlisting">
# Type of locking to use. Defaults to local file-based 
# locking (1).
# Turn locking off by setting to 0 (dangerous: risks metadata 
# corruption if LVM2 commands get run concurrently).
# Type 2 uses the external shared library locking_library.
# Type 3 uses built-in clustered locking.
# Type 4 uses read-only locking which forbids any operations 
# that might change metadata.
locking_type = 3
</pre>
</p><p>
On one of the servers, create a volume group, logical volume and filesystem from the designated shared volume:

<pre     class="programlisting">
$ sudo pvcreate /dev/sdb

$ sudo vgcreate -cy shared_vg /dev/sdb

$ sudo lvcreate -L 50G -n ha_lv shared_vg

$ sudo mkfs.ext4 /dev/shared_vg/ha_lv

$ sudo lvchange -an shared_vg/ha_lv
</pre>
</p><p>
The example above carves out a 50GB logical volume from the volume
group and then formats it with an Extended 4 filesystem. The
<tt  >cy</tt>
option used with the <tt  >vgcreate</tt> (volume group create) command enables the
volume group for clustered locking. The <tt  >an</tt> option
with the <tt  >lvchange</tt>
(logical volume change) command deactivates the logical volume. You
will be relying on the CLVM and resource manager (read below) d&aelig;mons
to handle activations based on the failover feature additions made in
the same /etc/cluster/cluster.conf file created earlier. When active,
the the shared volume will be accessible from /dev/shared_vg/ha_lv.
</p><p>
Add the necessary failover details to the cluster.conf file:


<pre     class="programlisting">
&lt;rm&gt;  
   &lt;failoverdomains&gt;
       &lt;failoverdomain name="FD" ordered="1" restricted="0"&gt;
          &lt;failoverdomainnode name="serv-0001" priority="1"/&gt;
          &lt;failoverdomainnode name="serv-0002" priority="2"/&gt;
       &lt;/failoverdomain&gt;
   &lt;/failoverdomains&gt;
   &lt;resources&gt;
       &lt;lvm name="lvm" vg_name="shared_vg" lv_name="ha-lv"/&gt;
       &lt;fs name="FS" device="/dev/shared_vg/ha-lv" 
         &#8618;force_fsck="0" force_unmount="1" fsid="64050" 
         &#8618;fstype="ext4" mountpoint="/mnt" options="" 
         &#8618;self_fence="0"/&gt;
   &lt;/resources&gt;
   &lt;service autostart="1" domain="FD" name="serv" 
    &#8618;recovery="relocate"&gt;
       &lt;lvm ref="lvm"/&gt;
       &lt;fs ref="FS"/&gt;
   &lt;/service&gt;
&lt;/rm&gt;
</pre>
</p><p>
The &ldquo;rm&rdquo; portion of the cluster.conf file utilizes the resource
manager (or rgmanager). In this addition to the configuration file,
you inform the cluster manager that serv-0001 should have ownership and
sole access to the shared volume first. It will be mounted locally at
the /mnt absolute path. If and when serv-0001 goes down for any reason,
the resource manager then will perform a failover that will enable sole
access to the shared volume, mounted at /mnt on serv-0002. All pending
I/O requests sent to serv-0001 will resume on serv-0002.
</p><p>
On all servers, restart the cman service to enable the new configuration:

<pre     class="programlisting">
$ sudo /etc/rc.d/init.d/cman restart
</pre>
</p><p>
Also, on all servers, start the rgmanager and clvmd services:

<pre     class="programlisting">
$ sudo /etc/rc.d/init.d/rgmanager start
Starting Cluster Service Manager:           [  OK  ]

$ sudo /etc/rc.d/init.d/clvmd start
Starting clvmd:                             [  OK  ]
</pre>
</p><p>
Assuming that no errors were observed, you now should have a running
cluster configured in an active/passive configuration. You can
validate this by checking the accessibility of the shared volume on
all servers. It should be seen, enabled and mounted on serv-0001 and
not on serv-0002. Now comes the moment of truth&mdash;that is, testing the
failover. Manually power down serv-0001. You will notice the rgmanager
kicking in and enabling/mounting the volume on serv-0002.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x1841bc0"></a></h2></div></div><div class="sidebar"><p class="title"><b>Note:</b></p><p>
To enable these services automatically on reboot, use
chkconfig to start the services on all appropriate runlevels.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x1c3a020"></a>
Summary</h2></div></div><p>
In an ideal configuration, fencing agents will need to be configured in
the /etc/cluster/cluster.conf file. The purpose of the fencing agent is
to handle a problematic node before it causes noticeable issues to the
cluster. For example, if a server suffers from a kernel panic, is not
communicating with the other servers in the cluster, or something else
just as devastating, the IPMI utilities can be configured to reboot the
server in question:


<pre     class="programlisting">
&lt;clusternode name="serv-0001" nodeid="1"&gt;
     &lt;fence&gt;
         &lt;method name="1"&gt;
             &lt;device name="ipmirecover1"/&gt;
         &lt;/method&gt;
     &lt;/fence&gt;
 &lt;/clusternode&gt;
 &lt;clusternode name="serv-0002" nodeid="2"&gt;
     &lt;fence&gt;
         &lt;method name="1"&gt;
             &lt;device name="ipmirecover2"/&gt;
         &lt;/method&gt;
     &lt;/fence&gt;
 &lt;/clusternode&gt;

[ ... ]

   &lt;fencedevices&gt;
       &lt;fencedevice agent="fence_ipmilan" ipaddr="192.168.1.50" 
        &#8618;login="ADMIN" passwd="ADMIN" name="ipmirecover1"/&gt;
       &lt;fencedevice agent="fence_ipmilan" ipaddr="192.168.10" 
        &#8618;login="ADMIN" passwd="ADMIN" name="ipmirecover2"/&gt;
   &lt;/fencedevices&gt;
</pre>
</p><p>
The primary objective of HA-LVM is to provide the data center with
enterprise-class fault tolerance at a fraction of the price. No one
ever wants to experience server downtimes, and with an appropriate
configuration, no one has to. From the data center to your home office,
this solution can be deployed almost anywhere.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x1c3a288"></a></h2></div></div><div class="sidebar"><p class="title"><b>Resources</b></p><p>
clvmd(8): Linux man page
</p><p>
Appendix F. High Availability LVM (HA-LVM): <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Cluster_Administration/ap-ha-halvm-CA.html" target="_self">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Cluster_Administration/ap-ha-halvm-CA.html</a>
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1748580.0x1c3a548"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Petros Koutoupis is a full-time Linux kernel, device driver and
application developer for embedded and server platforms. He has been
working in the data storage industry for more than eight years and enjoys discussing
the same technologies.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../247/toc247.html">Issue Table of Contents</a>
    <a class="link3" href="../247/11723.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>