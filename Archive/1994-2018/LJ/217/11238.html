<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Parallel Programming in C and Python
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;A fast-track guide to writing parallel programs in C and Python.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0xeea580.0xfe1ac0"></a>
Parallel Programming in C and Python
</h1></div><div><div class="author"><h3 class="author">
Amit
 
Saha
</h3></div><div class="issuemoyr">Issue #217, May 2012</div></div><div><p>
A fast-track guide to writing parallel programs in C and Python.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfe22a8"></a></h2></div></div><p>
Advancements in computing technologies have enabled even
&ldquo;resource-constrained&rdquo; Netbooks to have four CPU cores. Thus, parallel
computing is not about having a large number of computer nodes
in a cluster room anymore. To write programs that take advantage of such easily
available parallel computing resources, C and Python programmers have
libraries at their disposal, such as OpenMP (C) and multiprocessing
(Python) for shared memory parallel programming, and OpenMPI (C/Python)
for distributed memory parallel programming. And, in case you don't
have hardware resources beyond your quad-core personal computer, cloud-computing solutions, such as PiCloud, can be of tremendous use. 
</p><p>
For the purposes of this article, I assume you have one computer at your
disposal and, thus, focus on OpenMP, multiprocessing and PiCloud. Note
that 
the article's code examples demonstrate parallel programming and may not
be the most optimal strategy possible for parallelization. I also
have refrained from any benchmarking to show speedups of the parallel programs
versus their serial counterparts.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfe2510"></a>
Shared Memory Parallel Programming</h2></div></div><p>
Shared memory parallel programming libraries enable writing programs
that can take advantage of the multiple CPUs on your computer.
</p><p>
OpenMP (Open specifications for Multi-Processing) is a standard API
specification that may be used explicitly to achieve multithreaded,
shared memory parallelism. The API is defined for C/C++ and FORTRAN, and
it is composed of three primary API components: compiler directives (#pragma
in C), runtime libraries and environment variables. An OpenMP program
begins life as the master thread until it encounters a parallel block. A team
of parallel threads is then created; this operation is called the fork
operation. The statements in this parallel block are executed in parallel
by these threads. Once the team of threads has finished execution, the
master thread is back in control, which is called the Join operation. This model
of execution is referred to as the Fork-Join model.
</p><p>
Let's use
the GNU implementation of OpenMP, called libgomp. Your distribution's package
manager is the easiest way to install this library. To enable
the compiler directives that you use in your OpenMP programs,
specify <tt  >-fopenmp</tt>, and to link to the runtime
library, a <tt  >-lgomp</tt>
has to be added during the compilation of your OpenMP programs. 
</p><p>
Let's
start by creating a team of threads (Listing 1). First, use the
<tt  >omp_get_num_procs()</tt> function to get the number of processors available,
which you use to set the number of threads using the
<tt  >omp_set_num_threads()</tt>
function. Next, specify the parallel block of the program using <tt  >#pragma
omp parallel</tt>. You want each thread to have a private copy of the tid
variable, so append this information to the #pragma directive. In
the parallel block, let's simply print a hello world message from all of the
threads, including the master thread. The master thread has a tid of 0,
and it can be used to execute statements specific to it. The thread id of
a particular thread is obtained using the
<tt  >omp_get_thread()</tt> function.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfe2988"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 1. ompdemo.c</b></p><pre     class="programlisting">
#include &lt;stdio.h&gt;
#include &lt;omp.h&gt;

int main (int argc, char **argv)  {

  int nthreads, tid, i;

  /* Get the number of processors */
  printf("Number of processors available:: 
  &#8618;%d\n",omp_get_num_procs());

  /* Set the number of threads to the number of processors */
  omp_set_num_threads(omp_get_num_procs());

  /* Fork a team of threads with each thread having a 
     private tid variable * */
#pragma omp parallel  private(tid)
  {
    /* Obtain and print thread id */
    tid = omp_get_thread_num();
    printf("Hello World from thread = %d\n", tid);

    /* Only master thread does this */
    if (tid == 0)
      {
        nthreads = omp_get_num_threads();
        printf("Number of threads = %d\n", nthreads);
      }
  }  /* All threads join master thread and terminate */

  return 0;
}
</pre></div><p>
Once you compile and execute this program, the output, depending on
the number of processors on your computer, will be similar to the
following:

<pre     class="programlisting">
$ gcc -o ompdemo ompdemo.c -lgomp -fopenmp
$ ./ompdemo

Number of processors available:: 2
Hello World from thread = 0
Number of threads = 2
Hello World from thread = 1
</pre>
</p><p>
Next, let's consider a code snippet in your program that may
look like this:

<pre     class="programlisting">
A -&gt; Large array
For every element e in array A
 call function, fun(e)
end for 
</pre>
</p><p>
Instead of calling the function, <tt  >fun</tt> for every
element, <tt  >e</tt>, serially,
what if you could do it in parallel? OpenMP's work-sharing constructs
enable you to do that. Consider the code snippet below:

<pre     class="programlisting">
A -&gt; Large array
#pragma omp for schedule(dynamic,chunk)
For every element e in array A
 call function, fun(e)
end for 
</pre>
</p><p>
The directive, <tt  >#pragma omp for
schedule(dynamic,chunk)</tt> specifies that the
ensuing for loop will be executed in parallel by the thread team, with
the workload specified by the <tt  >schedule(dynamic,
chunk)</tt> clause. The <tt  >chunk</tt>
variable specifies the unit of division&mdash;that is, the number of iterations
of the for loop that will be executed by a thread, and
<tt  >dynamic</tt> specifies
that the chunks will be assigned dynamically to the threads. 
</p><p>
Let's use
this directive to process a large array, calling a function for each
array element (Listing 2). The parallel section of the code begins at
the directive <tt  >#pragma omp parallel</tt> where we specify that the arrays a
and b and the variable <tt  >chunk</tt> will be shared among the threads. This
is required, because all the threads will be accessing the arrays a and b, and
<tt  >chunk</tt> will be used by the ensuing parallel for loop to determine the
unit of division. The variable, i, specifying the loop, will be private
to each thread.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfe31c8"></a></h2></div></div><div class="sidebar"><p class="title"><b>Listing 2. omp_for_eval.c</b></p><pre     class="programlisting">
/* Work Sharing construct
https://computing.llnl.gov/tutorials/openMP/#DO

Distributes an array of elements across threads, where each
element is passed as a parameter to a function to be evaluated

*/

#include &lt;omp.h&gt;
#include &lt;stdio.h&gt;
#define N 100000
#define CHUNKSIZE 100
/* dummy function*/
float myfun(float a)
{
  return a*a;
}

int main (int argc, char **argv)
{

  int i, chunk,tid;
  float a[N], b[N];

  for (i=0; i &lt; N; i++)
    a[i] = i * 1.0;


  /* Get the number of processors */
  printf("Number of processors available:: 
  &#8618;%d\n",omp_get_num_procs());

  /* Set the chunk size*/
  chunk = CHUNKSIZE;

  /* Set the number of threads to the number of processors*/
  omp_set_num_threads(omp_get_num_procs());

#pragma omp parallel shared(a,b,chunk) private(i)
  {

#pragma omp for schedule(dynamic,chunk)
    for (i=0; i&lt; N; i++)
      {
        b[i] = myfun(a[i]);
      }
  }  /* end of parallel section */

  printf("For evaluation completed, the result 
  &#8618;has been stored in array B\n");

  return 0;
}
</pre></div><p>
The result of the function evaluation will be available at the end of
the parallel section in the array b:


<pre     class="programlisting">
$ gcc -o omp_for_eval omp_for_eval.c -lgomp -fopenmp
$ ./omp_for_eval 
Number of processors available:: 2
For evaluation completed, the result has been stored in array B
</pre>
</p><p>
For the next example, let's calculate the
value of Pi using a Monte Carlo sampling technique.
The
core of the algorithm involves generating pairs of random points in
the range (0,1) and keeping a count of the number of points that lie
in the first quadrant of a unit circle. The number of samples generated
have a direct effect on the accuracy of the estimate of the value of Pi.
With the help of OpenMP, let's divide the number of samples equally
among multiple threads and finally accumulate the result in the master
thread to calculate the value of Pi (Listing 3).

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfe3590"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 3. pi_openmp.c</b></p><pre     class="programlisting">
/* Program to compute Pi using Monte Carlo method:
(http://math.fullerton.edu/mathews/n2003/montecarlopimod.html)
 */

#include &lt;stdlib.h&gt;
#include &lt;stdio.h&gt;
#include &lt;math.h&gt;
#include &lt;string.h&gt;

/* Returns the value of count with niter iterations*/
int part_count(int niter)
{
  int i, count=0;
  float x,y,z;

  for ( i=0; i&lt;niter; i++)
    {
      x = (double)rand()/RAND_MAX;
      y = (double)rand()/RAND_MAX;
      z = x*x+y*y;
      if (z&lt;=1) count++;
    }

  return count;
}


int main(int argc, char* argv)
{
  int niter=0,chunk;
  double x,y;
  int i,count=0; /* # of points in the 1st 
                    quadrant of unit circle */
  double z;
  double pi;

  /* Get the number of processors */
  printf("Number of processors available:: 
  &#8618;%d\n",omp_get_num_procs());

  printf("Enter the number of iterations used to 
  &#8618;estimate pi (multiple of %d please): 
  &#8618;",omp_get_num_procs());
  scanf("%d",&amp;niter);


  /* Set the number of threads to the number of processors*/
  omp_set_num_threads(omp_get_num_procs());
  chunk = niter/omp_get_num_procs();

#pragma omp parallel shared(chunk) reduction(+:count)
  {
    count = part_count(chunk);
  }

  pi=(double)count/niter*4;
  printf("# of iterations = %d , estimate of pi is 
  &#8618;%g \n",niter,pi);

  return 0;
}
</pre></div><p>
In this code, notice the use of the reduction clause, <tt  >#pragma omp parallel
shared(chunk) reduction(+:count)</tt>. This effectively means to
perform the + operation on the private copies of the variable
<tt  >count</tt>,
and store it in the global shared variable <tt  >count</tt>.
The value of <tt  >count</tt>
then is used to calculate the value of Pi:

<pre     class="programlisting">
$ gcc -o pi_openmp pi_openmp.c -lgomp -fopenmp
$ ./pi_openmp 
Number of processors available:: 2
Enter the number of iterations used to estimate pi 
&#8618;(multiple of 2 please): 10000
# of iterations = 10000 , estimate of pi is 3.1636 
[gene@zion openmp]$ ./pi_openmp 
Number of processors available:: 2
Enter the number of iterations used to estimate pi 
&#8618;(multiple of 2 please): 1000000
# of iterations = 1000000 , estimate of pi is 3.14222 
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfe39b0"></a>
Python's Multiprocessing Module</h2></div></div><p>
Python's multiprocessing module
effectively
side-steps the Global Interpreter Lock that inhibits true multithreaded
Python programs. With this module, your Python programs can make use
of multiple processors on your computer. Listing 4 is a Python program
that uses this module to get the number of CPUs on your computer and
creates that number of processes.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfe3b68"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 4. mpdemo.py</b></p><pre     class="programlisting">
'''
Create number of processes using the multiprocessing module
'''

import multiprocessing
from multiprocessing import Process

# dummy function
def f(id):
    #This is a dummy function taking a parameter
    return

if __name__ == '__main__':

    # get the number of CPUs
    np = multiprocessing.cpu_count()
    print 'You have {0:1d} CPUs'.format(np)

    # Create the processes
    p_list=[]
    for i in range(1,np+1):
        p = Process(target=f, name='Process'+str(i), args=(i,))
        p_list.append(p)
        print 'Process:: ', p.name,
        p.start()
        print 'Was assigned PID:: ', p.pid

    # Wait for all the processes to finish
    for p in p_list:
        p.join()
</pre></div><p>
A process is created by creating an object of the Process class:
<tt  >p= Process(target=f, name='Process'+str(i),
args=(i,))</tt>. The <tt  >target</tt>
argument specifies the callable object to be called by the process's
<tt  >run()</tt> method, which is invoked by the
<tt  >start()</tt> method; <tt  >name</tt> specifies
a custom name for the process; and <tt  >args</tt> specifies the argument tuple for
the target. Depending on the number of processors on your computer, you
should see output similar to the following:

<pre     class="programlisting">
$ python mpdemo.py
You have 2 CPUs
Process::  Process1 Was assigned PID::  29269
Process::  Process2 Was assigned PID::  29270
</pre>
</p><p>
This program maintains a list of the created process objects so as to
wait for them to finish using the <tt  >join()</tt> method. If you attempt to use
this example as a starting point to actually do something, you will
want a way to send back information from the spawned processes
to the master process. There are a few ways to achieve this. The easiest,
but not really recommended way, is to use shared state variables.
Another way to establish a communication channel
between the master and the spawned processes is to use Queues.
Listing 5 uses the multiprocessing module to calculate the dot product of
a large array by splitting the calculation across multiple processes.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfda9d0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 5. mp_queue.py</b></p><pre     class="programlisting">
'''
Queue: http://docs.python.org/library/
&#8618;multiprocessing.html#pipes-and-queues

Demonstrates the usage of Queue to share data between processes.
Splits up a large array into chunks and calculates the partial
dot products.
'''

import multiprocessing
from multiprocessing import Process, Queue
from numpy import *

# dot product of the partial data chunks
def add(chunk1,chunk2,product):
    a = chunk1.get()
    b = chunk2.get()

    prod = a*b
    product.put(sum(prod))

if __name__ == '__main__':

    #size of the arrays
    num_el = 100000

    # Create two arrays
    a = linspace(0,100,num_el);
    b = linspace(0,1,num_el);

    # get the number of CPUs and assign it as the number of
    # processes to create
    np = multiprocessing.cpu_count()
    print 'You have {0:1d} CPUs'.format(np)

    # chunk size
    if num_el%np != 0:
        print "The current chunking mechanism will not work"
        exit
    else:
        chunk = num_el/np

    # Create the processes
    p_list=[]

    # Create the Queue which will have the partial products
    product=Queue()

    for i in range(1,np+1):

        # A pair of queues per process for the two arrays
        aq = Queue()
        bq = Queue()

        # push the chunks into the queue
        aq.put(a[(i-1)*chunk:i*chunk])
        bq.put(b[(i-1)*chunk:i*chunk])

        # create the process
        p = Process(target=add, args=(aq,bq,product))
        p.start()
        p.join()

    # collect the individual sums
    items=[]
    for i in range(product.qsize()):
        items.append(product.get())

    # final product: sum of individual products
    print "Dot product:: ",sum(items)
</pre></div><p>
For every process created, a pair of queue objects are created, one each
for the two arrays. The whole array is divided into equal chunks, and a
process is assigned a chunk to calculate the partial product. The partial
products are stored in the queue object <tt  >product</tt>. After the processes
have finished execution, partial products are retrieved from the queue
and summed to get the actual dot product. Here is sample output:

<pre     class="programlisting">
$ python mp_queue.py
You have 2 CPUs
Dot product::  3333350.00017
</pre>
</p><p>
Next, let's take a look at Process pools. As the name implies, a pool of
processes is created using the <tt  >Pool</tt> class, and each member of this process
pool is then assigned a task to execute using methods, such as
<tt  >apply()</tt>
or <tt  >map()</tt>. Listing 6 demonstrates using Process pools to calculate the
value of Pi using the same algorithm as shown in Listing 3.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfdaea0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 6. pi_mp.py</b></p><pre     class="programlisting">
'''
Multiprocessing-based code to estimate the value of PI
using Monte Carlo sampling.
Ref: http://math.fullerton.edu/mathews/n2003/montecarlopimod.html
Uses workers:
http://docs.python.org/library/
&#8618;multiprocessing.html#module-multiprocessing.pool
'''

import random
import multiprocessing
from multiprocessing import Pool


#calculate the number of points in the unit circle
#out of n points
def monte_carlo_pi_part(n):

    count = 0
    for i in range(n):
        x=random.random()
        y=random.random()

        # if it is within the unit circle
        if x*x + y*y &lt;= 1:
            count=count+1

    #return
    return count


if __name__=='__main__':

    np = multiprocessing.cpu_count()
    print 'You have {0:1d} CPUs'.format(np)

    # Number of points to use for the Pi estimation
    n = 10000000

    # iterable with a list of points to generate in each worker
    # each worker process gets n/np number of points to 
    # calculate Pi from

    part_count=[n/np for i in range(np)]

    #Create the worker pool
    # http://docs.python.org/library/
      &#8618;multiprocessing.html#module-multiprocessing.pool
    pool = Pool(processes=np)

    # parallel map
    count=pool.map(monte_carlo_pi_part, part_count)

    print "Estimated value of Pi:: ", sum(count)/(n*1.0)*4
</pre></div><p>
The pool of processes is created using the statement <tt  >pool =
Pool(processes=np)</tt>, and each process in this pool is asked to invoke the
function <tt  >monte_carlo_pi_part</tt> using <tt  >count=pool.map(monte_carlo_pi_part,
part_count)</tt>, where <tt  >part_count</tt> is a list
with the number of samples
to be generated in each process. The returned value from each process
is stored in the list, <tt  >count</tt>. Finally, let's sum this list and calculate
the estimated value of Pi:

<pre     class="programlisting">
$ python pi_mp.py
You have 2 CPUs
Estimated value of Pi::  3.1412128
</pre>
</p><p>
The call to <tt  >map()</tt> is a blocking call&mdash;that is, the process doesn't
terminate until the result has been received. For other applications,
the asynchronous version may be desirable.
</p><p>
The multiprocessing module has other features, such as the ability to
execute remote jobs by the use of managers, proxy objects and a number
of synchronization primitives. See the module
documentation for more information.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfdb478"></a>
Parallel Computing in the Cloud with PiCloud</h2></div></div><p>
If you have run out of cores on your workstation and want to
make more computing power accessible to your program easily, it's worth
looking into PiCloud.
</p><p>
To start using PiCloud, create an account on the
project Web site, and then install the client from the PyPi package index. The Quickstart documentation should help you set up PiCloud
on your computer (see Resources).
</p><p>
Let's write a simple function and run it on PiCloud.
First, define a function <tt  >sort_num()</tt> in the file demo.py as follows:

<pre     class="programlisting">
import numpy 

def sort_num(num): 
    sort_num = numpy.sort(num) 
    return sort_num 
</pre>
</p><p>
Fire up the Python interpreter and import the function you just defined
and the modules cloud and numpy:

<pre     class="programlisting">
&gt;&gt;&gt; from demo import *
&gt;&gt;&gt; import cloud
&gt;&gt;&gt; import numpy
</pre>
</p><p>
Next, create an array of 50000 integers with each integer chosen
randomly from the range (10, 10000):

<pre     class="programlisting">
&gt;&gt;&gt; num=numpy.random.random_integers(10,10000,50000)
</pre>
</p><p>
Now comes the important step, invoking the
<tt  >cloud.call</tt> method to specify
the function that you want to run and its arguments:


<pre     class="programlisting">
&gt;&gt;&gt; jid=cloud.call(sort_num,num)
</pre>
</p><p>
This implies that you want the function <tt  >sort_num</tt> to be executed with the
argument num. This method returns an integer that is an identifier for
this particular job. The formal specification of the
<tt  >cloud.call</tt> method
is available at <a href="http://docs.picloud.com/moduledoc.html#cloud.call" target="_self">docs.picloud.com/moduledoc.html#cloud.call</a>:

<pre     class="programlisting">
&gt;&gt;&gt; jid
58
</pre>
</p><p>
The returned value of the executed function can be obtained using the
function <tt  >cloud.result</tt> using the obtained jid:


<pre     class="programlisting">
&gt;&gt;&gt; cloud.result(jid)
array([   10,    11,    11, ..., 10000, 10000, 10000])
</pre>
</p><p>
As you can see, the returned array is obtained. Next, consider a function
(defined as a file mapdemo.py), which returns the volume of a cylinder
when the radius(r) and height(h) is passed to it:


<pre     class="programlisting">
import numpy

def vol_cylinder(r,h):
    return numpy.pi*r*r*h
</pre>
</p><p>
If you had one cylinder configuration&mdash;that is, one pair of
(r,h)&mdash;the
<tt  >cloud.call</tt> method can be used for the purpose.
What if you had
100 different configurations to square? You could use 100
<tt  >cloud.call</tt>
invocations, but there is a more efficient way of doing it: using the
<tt  >cloud.map</tt> function,
which accepts a sequence of parameters (or sequences of parameters):

<pre     class="programlisting">
&gt;&gt;&gt; from mapdemo import * 
&gt;&gt;&gt; import cloud 
&gt;&gt;&gt; import numpy 
&gt;&gt;&gt; r=numpy.random.random_sample(100) 
&gt;&gt;&gt; h=numpy.random.random_sample(100) 
&gt;&gt;&gt; jids=cloud.map(vol_cylinder,r,h) 
&gt;&gt;&gt; jids
xrange(359, 459)
</pre>
</p><p>
As you can see, <tt  >jids</tt> is a list of 100 elements with values from 359 to
459 (note that the values you have may be different). Here,
<tt  >cloud.map</tt> has created 100 jobs with the arguments formed from
the 100 pairs of the two sequences, r and h. You can pass this list of
<tt  >jids</tt> directly to <tt  >cloud.result</tt> to get the results as a list:

<pre     class="programlisting">
&gt;&gt;&gt; result=code.result(jids)
&gt;&gt;&gt; result 
[0.35045338986267927, 0.0043144690799585004, 
&#8618;0.094018119621969765, 0.93579722612039329, 
&#8618;0.0045154147876736109, 0.018836324478609345, 
&#8618;0.0027243595262778321, 1.5049675511377265, 
&#8618;0.37383416636274164, 0.24435487403102638, 
&#8618;0.28248315493701553, 1.2879340600324913, 
&#8618;0.68406526971023041, 0.14338739850272786,...
</pre>
</p><p>
In more practical situations, the function computation may not
be so trivial, and you may not have a definite idea when
all the jobs will be over. In such a case, you can wait until all
the jobs have finished to retrieve the results using the function
<tt  >cloud.join</tt>. The
PiCloud documentation uses the map function to calculate
the value of Pi (yes, again!) using the <tt  >map()</tt> method
(see Resources).
</p><p>
Among a host of other interesting features, PiCloud allows your Python
application to expose your functions in PiCloud via a REST API, which will
allow you to call this function from any other programming language. See
the Resources section of this article for an example. PiCloud allows you to set up
cron jobs, use different computing resources and use environments to
install any native libraries for use in your application. 
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfdc2e8"></a>
Distributed Memory Parallel Programming</h2></div></div><p>
The past few sections in this article give a taste of parallel
programming from a device as resource-constrained (relatively!) as a
dual-core desktop. What if you already have a 30-node computing cluster
at your disposal? In that case, you would use the tried-and-tested OpenMPI, an implementation of the MPI-2
specification. Another solution for setting up a computing cluster is the
Parallel Virtual Machine (PVM).
If you don't have multiple computers and still want to try out OpenMPI
programming, you simply can run it on a standalone computer or use
a virtualization solution, such as VirtualBox, to create a cluster of
virtual machines. 
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfdc3f0"></a>
Conclusion</h2></div></div><p>
My intention with this article was to provide an introduction to parallel
programming. Because the hardware requirements are really basic,
I mostly looked at shared memory parallel programming. I also
covered a more modern way of parallel computing using a cloud-computing service. Another very interesting project,
Parallel Python, enables
writing shared memory as well as distributed memory parallel
programs in Python. I hope the examples in this article 
help you get started exploring parallel programming from the
comfort of your Netbook! 
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0xeea580.0xfdc4f8"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
The code from this article is available at
<a href="https://bitbucket.org/amitksaha/articles_code/src/6c95473182a5/parallel_article" target="_self">https://bitbucket.org/amitksaha/articles_code/src/6c95473182a5/parallel_article</a>.
</p><p>
Parts of the section on PiCloud were adapted from my article on PiCloud,
titled &ldquo;PiCloud: Easy way to the Cloud&rdquo; published in
<span   class="emphasis"><em>Linux For You</em></span>
(<a href="http://www.lfymag.com" target="_self">www.lfymag.com</a>), March 2012.
</p><p>
OpenMP Tutorial: <a href="https://computing.llnl.gov/tutorials/openMP" target="_self">https://computing.llnl.gov/tutorials/openMP</a>
</p><p>
GNU OpenMP Implementation: <a href="http://gcc.gnu.org/onlinedocs/libgomp" target="_self">gcc.gnu.org/onlinedocs/libgomp</a>
</p><p>
Monte Carlo Sampling Technique: <a href="http://math.fullerton.edu/mathews/n2003/montecarlopimod.html" target="_self">math.fullerton.edu/mathews/n2003/montecarlopimod.html</a>
</p><p>
Process Class: <a href="http://docs.python.org/library/multiprocessing.html#process-and-exceptions" target="_self">docs.python.org/library/multiprocessing.html#process-and-exceptions</a>
</p><p>
Shared State Variables: <a href="http://docs.python.org/library/multiprocessing.html#sharing-state-between-processes" target="_self">docs.python.org/library/multiprocessing.html#sharing-state-between-processes</a>
</p><p>
Queues: <a href="http://docs.python.org/library/multiprocessing.html#exchanging-objects-between-processes" target="_self">docs.python.org/library/multiprocessing.html#exchanging-objects-between-processes</a>
</p><p>
Python Multiprocessing
Documentation: <a href="http://docs.python.org/library/multiprocessing.html" target="_self">docs.python.org/library/multiprocessing.html</a>
</p><p>
PiCloud Home Page: <a href="http://www.picloud.com" target="_self">www.picloud.com</a>
</p><p>
PiCloud Documentation: <a href="http://docs.picloud.com" target="_self">docs.picloud.com</a>
</p><p>
PyPi Package Index: <a href="http://pypi.python.org/pypi/cloud" target="_self">pypi.python.org/pypi/cloud</a>
</p><p>
PiCloud Quickstart Documentation: <a href="http://docs.picloud.com/quickstart.html" target="_self">docs.picloud.com/quickstart.html</a>
</p><p>
cloud.map Function: <a href="http://docs.picloud.com/client_basic.html#mapping" target="_self">docs.picloud.com/client_basic.html#mapping</a>
</p><p>
cloud.join Function: <a href="http://docs.picloud.com/moduledoc.html#cloud.join" target="_self">docs.picloud.com/moduledoc.html#cloud.join</a>
</p><p>
Calculating
the Value of Pi Using the map() Method: <a href="http://docs.picloud.com/basic_examples.html#calculating-pi" target="_self">docs.picloud.com/basic_examples.html#calculating-pi</a>
</p><p>
PiCloud Features: <a href="http://www.picloud.com/product/#features" target="_self">www.picloud.com/product/#features</a>
</p><p>
Using a C Client to Invoke a Function Published via the REST API: <a href="http://echorand.me/2012/01/27/picloud-and-rest-api-with-c-client" target="_self">echorand.me/2012/01/27/picloud-and-rest-api-with-c-client</a>
</p><p>
Running an Evolutionary Algorithm in the Cloud with PiCloud: <a href="http://echorand.me/2012/01/26/picloud-pyevolve-evolutionary-algorithms-in-the-cloud" target="_self">echorand.me/2012/01/26/picloud-pyevolve-evolutionary-algorithms-in-the-cloud</a>
</p><p>
OpenMPI Home Page: <a href="http://www.open-mpi.org" target="_self">www.open-mpi.org</a>
</p><p>
Parallel Programming Tutorial: <a href="https://computing.llnl.gov/tutorials/parallel_com" target="_self">https://computing.llnl.gov/tutorials/parallel_com</a>
</p><p>
Beginner MPI Tutorial: <a href="http://www.mpitutorial.com/beginner-mpi-tutorial" target="_self">www.mpitutorial.com/beginner-mpi-tutorial</a>
</p><p>
MPI Python Bindings (mpi4py): <a href="http://mpi4py.scipy.org" target="_self">mpi4py.scipy.org</a>
</p><p>
PVM Home Page: <a href="http://www.csm.ornl.gov/pvm" target="_self">www.csm.ornl.gov/pvm</a>
</p><p>
PVM Book: <a href="http://www.netlib.org/pvm3/book/pvm-book.html" target="_self">www.netlib.org/pvm3/book/pvm-book.html</a>
</p><p>
Parallel Python: <a href="http://www.parallelpython.com" target="_self">www.parallelpython.com</a>
</p></div></div></div>
<div class="authorblurb"><p>
Amit Saha is currently a PhD research student in the area of
Evolutionary Algorithms and Optimization. Like his random echoes 
show (<a href="http://echorand.me" target="_self">echorand.me</a>), he has been writing on myriad Linux and
open-source technologies for the past five years. He welcomes comments
on this article and beyond at <a href="mailto:amitsaha.in@gmail.com">amitsaha.in@gmail.com</a>.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../217/toc217.html">Issue Table of Contents</a>
    <a class="link3" href="../217/11238.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>