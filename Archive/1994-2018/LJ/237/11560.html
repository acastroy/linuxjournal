<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Solid-State Drives: Get One Already!
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Brian describes how SSDs compare to HDDs with regard to longevity and&#10;reliability and provides the results from some real-world performance&#10;benchmarking. &#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x280b580.0x2902ac0"></a>
Solid-State Drives: Get One Already!
</h1></div><div><div class="author"><h3 class="author">
Brian
 
Trapp
</h3></div><div class="issuemoyr">Issue #237, January 2014</div></div><div><p>
Brian describes how SSDs compare to HDDs with regard to longevity and
reliability and provides the results from some real-world performance
benchmarking. 
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x280b580.0x29031f8"></a></h2></div></div><p>
I've been building computers since the 1990s, so I've seen a lot of
new technologies work their way into the mainstream. Most were the
steady, incremental improvements predicted by Moore's law, but others were
game-changers, innovations that really rocketed performance forward in a
surprising way. I remember booting up <span   class="emphasis"><em>Quake</em></span> after installing my first
3-D card&mdash;what a difference! My first boot off a solid-state drive
(SSD) brought back that same feeling&mdash;wow, what a difference!
</p><p>
However, at a recent gathering of like-minded Linux users, I learned that
many of my peers hadn't actually made the move to SSDs yet. Within that
group, the primary reluctance to try a SSD boiled down to three main
concerns:
</p><div class="itemizedlist"><ul type="disc"><li><p>
I'm worried about their reliability; I hear they wear out.
</p></li><li><p>
I'm not sure if they work well with Linux.
</p></li><li><p>
I'm not sure an SSD really would make much of a difference on my system.
</p></li></ul></div><p>
Luckily, these three concerns are based either on misunderstandings,
outdated data, exaggeration or are just not correct.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x280b580.0x2903720"></a>
SSD Reliability Overview</h2></div></div><p>
<span   class="bold"><b>How SSDs Differ from Hard Drives:</b></span>
</p><p>
Traditional hard disk drives (HDDs) have two mechanical delays that can
come into play when reading or writing files: pivoting the read/write
head to be at the right radius and waiting until the platter rotates
until the start of the file reaches the head (Figure 1).
The time it takes for the drive to get in place to read a new file is
called seek time. When you hear that unique hard drive chatter,
that's the actuator arm moving around to access lots of different file
locations. For example, my hard drive (a pretty typical 7,200 RPM
consumer drive from 2011) has an average seek time of around 9ms.
</p><div       class="mediaobject"><img src="11560f1.jpg"><div class="caption"><p>
Figure 1. Hard Drive
</p></div></div><p>
Instead of rotating platters and read/write heads, solid-state drives
store data to an array of Flash memory chips. As a result, when a new
file is requested, the SSD's internal memory can find and start accessing
the correct storage memory locations in sub-milliseconds. Although reading
from Flash isn't terribly fast by itself, SSDs can read from several
different chips in parallel to boost performance. This parallelism and
the near-instantaneous seek times make solid-state drives significantly
faster than hard drives in most benchmarks. My SSD (a pretty typical
unit from 2012) has a seek time of 0.1ms&mdash;quite an improvement!
</p><p>
<span   class="bold"><b>Reliability and Longevity:</b></span>
</p><p>
Reliability numbers comparing HDDs and SSDs are surprisingly hard to
find. Fail rate comparisons either didn't have enough years of data,
or were based on old first-generation SSDs that don't represent drives
currently on the market. Though SSDs reap the benefits of not having any
moving parts (especially beneficial for mobile devices like laptops),
the conventional wisdom is that current SSD fail rates are close to HDDs.
Even if they're a few percentage points higher or lower, considering that
<span   class="emphasis"><em>both</em></span> drive types have a nonzero failure rate, you're going to
need to have a backup solution in <span   class="emphasis"><em>either</em></span> case.
</p><p>
Apart from reliability, SSDs do have a unique longevity issue, as the
NAND Flash cells in storage have a unique life expectancy limitation.
The longevity of each cell depends on what type of cell it is.
Currently, there are three types of NAND Flash cells: 
</p><div class="itemizedlist"><ul type="disc"><li><p>
SLC (Single Later Cell) NAND: 
one bit per cell, ~100k writes. 
</p></li><li><p>
MLC (Multi-Layer Cell) NAND: two
bits per cell, ~10k to 3k writes, slower than SLC. The range in writes
depends on the physical size of the cell&mdash;smaller cells are cheaper
to manufacture, but can handle fewer writes.
</p></li><li><p>
TLC (Three-Layer Cell) NAND: ~1k writes, slower than MLC.
</p></li></ul></div><p>
Interestingly, all three types of cells are using the same transistor
structure behind the scenes. Clever engineers have found a way to make
that single Flash cell hold more information in MLC or TLC mode, however.
At programming time, they can use a low, medium-low, medium-high or high
voltage to represent four unique states (two bits) in one single cell.
The downside is that as the cell is written several thousand times, the
oxide insulator at the bottom of the floating gate starts to degrade,
and the amount of voltage required for each state increases (Figure 2).
For SLC it's not a huge deal because the gap between
states is so big, but for MLC, there are four states instead of
two, so the amount of room between each state's voltage is shortened.
For TLC's three bits of information there are six states, so the distances
between each voltage range is even shorter.
</p><div       class="mediaobject"><a href="11560f2.large.jpg"><img src="11560f2.jpg"></a><div class="caption"><p>
Figure 2. A NAND Flash Cell
</p></div></div><p>
The final twist is write amplification. Even though the OS is sending 
1MB of data, the SSD actually may be doing more writes behind the scenes
for things like wear leveling and inefficient garbage collection if
TRIM support isn't enabled (see the TRIM section later in this article).
Most real-world write amplification values I've seen are in the 1.1 to
3.0 range, depending on how compressible the data is and how clever the
SSD is at garbage collection and wear leveling.
</p><p>
So, how long can you expect an SSD to last for you? Longevity depends on
how much data you write, and the tune2fs utility makes it really
easy to estimate that from your existing filesystems. Run <tt  >tune2fs
-l /dev/&lt;device&gt;</tt>. (Tip: if you're using LVM, the stats will be under
the dm-X device instead of the sdaX device.) The key fields of
interest are &ldquo;Filesystem created&rdquo; and &ldquo;Lifetime
writes&rdquo;. Use
those to figure out the average GB/day since the filesystem was created.
For my laptop, it was 2.7GB/day, and for my workstation it was 6.3GB/day.
With those rates, plus a rough guess for write amplification, you can
estimate how much life you'd get out of any SSD.


<pre     class="programlisting">
Est. Lifespan (y) = SSDCapacity(GB) * (WriteLimit based on cell type)
                    -------------------------------------------------
                 DailyWriteRate (GB/day) * WriteAmplification * 365 (days/yr)
</pre>
</p><p>
So if I was sizing a 256GB Samsung 840 Evo (which uses TLC cells), with
a 6.3GB/day write rate and a write amplification of 3, it should give
me around 37 years of service before losing the ability to write new data.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x280b580.0x29044e0"></a>
SSD Considerations for Linux</h2></div></div><p>
<span   class="bold"><b>TRIM:</b></span>
</p><p>
Undelete utilities work because when you delete a file, you're really only
removing the filesystem's pointer to that file, leaving the file contents
behind on the disk. The filesystem knows about the newly freed space and
eventually will reuse it, but the drive doesn't. HDDs can overwrite data
just as efficiently as writing to a new sector, so it doesn't really hurt
them, but this can slow down SSDs' write operations, because they can't
overwrite data efficiently.
</p><p>
An SSD organizes data internally into 4k pages and groups 128 pages into a
512k block. SSDs can write only into empty 4k pages and erase in
big 512k block increments. This means that although SSDs can write very
quickly, <span   class="emphasis"><em>overwriting</em></span> is a much slower process. The TRIM command
keeps your SSD running at top speed by giving the filesystem a way to tell
the SSD about deleted pages. This gives the drive a chance to do the slow
overwriting procedures in the backgroupd, ensuring that you always have a large
pool of empty 4k pages at your disposal.
</p><p>
Linux TRIM support is not enabled by default, but it's easy to add. One
catch is that if you have additional software layers between your
filesystem and SSD, those layers need to be TRIM-enabled too. For example,
most of my systems have an SSD, with LUKS/dm-crypt for whole disk
encryption, LVM for simple volume management and then, finally, an ext4
formatted filesystem. Here's how to turn on TRIM support, starting at the
layer closest to the drive.
</p><p>
<span   class="bold"><b>dm-crypt and LUKS:</b></span>
</p><p>
If you're not using an encrypted filesystem, you can skip ahead to the LVM instructions. TRIM has been supported in dm-crypt since kernel 3.1. 
Modify /etc/crypttab, adding the discard keyword for the devices on SSDs:


<pre     class="programlisting">
#TargetName Device                             KeyFile  Options
sda5_crypt  UUID=9ebb4c49-37c3...d514ae18be09  none     luks,discard 
</pre>
</p><p>
Note: enabling TRIM on an encrypted partition does make
it easier for attackers to brute-force attack the device, since they
would now know which blocks are not in use.
</p><p>
<span   class="bold"><b>LVM:</b></span>
</p><p>
If you're not using LVM, you can skip ahead to the filesystem section. TRIM
has been supported in LVM since kernel 2.6.36.
</p><p>
In the &ldquo;devices&rdquo; section of /etc/lvm/lvm.conf, add a line
<tt  >issue_discards = 1</tt>:

<pre     class="programlisting">
devices {
        ...
        issue_discards = 1
        ..
}
...
</pre>
</p><p>
<span   class="bold"><b>Filesystem:</b></span>
</p><p>
Once you've done any required dm-crypt and LVM edits, update initramfs, then
reboot:

<pre     class="programlisting">
sudo update-initramfs -u -k all
</pre>
</p><p>
Although Btrfs, XFS, JFS and ext4 all support TRIM, I cover only ext4 here,
as
that seems to be the most widely used. To test ext4 TRIM support, try the
manual TRIM command: <tt  >fstrim &lt;mountpoint&gt;</tt>. If all goes well, the
command will work for a while and exit. If it exits with any error,
you know there's something wrong in the setup between the filesystem and
the device. Recheck your LVM and dm-crypt setup.
</p><p>
Here's an example of the output for / (which is set up for TRIM) and /boot
(which is not): 


<pre     class="programlisting">
~$ sudo fstrim / 
~$ sudo fstrim /boot 
fstrim: /boot: FITRIM ioctl failed: Inappropriate ioctl for device 
</pre>
</p><p>
If the manual command works, you can decide between between using the
automatic TRIM built in to the ext4 filesystem or running the
<tt  >fstrim</tt> command. The primary benefits of using automatic TRIM is
that you don't have to think about it, and it nearly instantly will reclaim
free space. One down side of automatic TRIM is that if your drive doesn't
have good garbage-collection logic, file deletion can be slow. Another
negative is that if the drive runs TRIM quickly, you have no chance of
getting your data back via an undelete utility. On drives where I have
plenty of free space, I use the fstrim command via cron. On drives where
space is tight, I use the automatic ext4 method.
</p><p>
If you want to go the automatic route, enabling automatic TRIM is
easy&mdash;just add the <tt  >discard</tt> option to the options section of the relevant
/etc/fstab entries. For manual TRIM, just put the <tt  >fstrim
&lt;mountpoint&gt;</tt> in a cron job or run it by hand at your leisure.
</p><p>
Regardless of whether you use the <tt  >discard</tt> option, you probably want to add
the <tt  >noatime</tt> option to /etc/fstab. With atime on (the default), each time
a file is accessed, the access time is updated, consuming some of your
precious write cycles. (Some tutorials ask you to include nodiratime too,
but noatime is sufficient.) Because most applications don't use the atime
timestamp, turning it off should improve the drive's longevity:


<pre     class="programlisting">
/dev/mapper/baldyl-root	/  ext4  noatime,discard,errors=remount-ro 0 1
</pre>
</p><p>
<span   class="bold"><b>Partition alignment:</b></span>
</p><p>
When SSDs first were released, many of the disk partitioning systems 
still were based on old sector-based logic for placing partitions. This could
cause a problem if the partition boundary didn't line up nicely with the
SSD's internal 512k block erase size. Luckily, the major partitioning
tools now default to 512k-compatible ranges:
</p><div class="itemizedlist"><ul type="disc"><li><p>
fdisk uses a one megabyte boundary since util-linux version 2.17.1
(January 2010).
</p></li><li><p>
LVM uses a one megabyte boundary as the default since version 2.02.73
(August 2010).
</p></li></ul></div><p>
If you're curious whether your partitions are aligned to the right boundaries,
here's example output from an Intel X25-M SSD with an erase block size of
512k:


<pre     class="programlisting">
~$ sudo sfdisk -d /dev/sda 
Warning: extended partition does not start at a cylinder boundary. 
DOS and Linux will interpret the contents differently. 
# partition table of /dev/sda 
unit: sectors 

/dev/sda1 : start=     2048, size=   497664, Id=83, bootable 
/dev/sda2 : start=   501758, size=155799554, Id= 5 
/dev/sda3 : start=        0, size=        0, Id= 0 
/dev/sda4 : start=        0, size=        0, Id= 0 
/dev/sda5 : start=   501760, size=155799552, Id=83 
</pre>
</p><p>
Since the primary partition (sda5) starts and ends at a number evenly
divisible by 512, things look good.
</p><p>
<span   class="bold"><b>Monitoring SSDs in Linux:</b></span>
</p><p>
I already covered running <tt  >tune2fs -l &lt;device&gt;</tt> as a good place to get
statistics on a filesystem device, but those are reset each time you
reformat the filesystem. What if you want to get a longer range of
statistics, at the drive level? smartctl is the tool for that.
SMART (Self-Monitoring, Analysis and Report Technology) is part of the ATA
standard that provides a way for drives to track and report key statistics,
originally for the purposes of predicting drive failures. Because drive
write volume is so important to SSDs, most manufacturers are including this
in the SMART output. Run <tt  >sudo smartctl -a
/dev/&lt;device&gt;</tt> on an SSD
device, and you'll get a whole host of interesting statistics. If you see
the message &ldquo;Not in smartctl database&rdquo; in the smartctl output, try
building the latest version of smartmontools.
</p><p>
Each vendor's label for the statistic may be different, but you should be
able to find fields like &ldquo;Media_Wearout_Indicator&rdquo; that will count
down from 100 as the drive approaches the Flash wear limit and fields like
&ldquo;Lifetime_Writes&rdquo; or &ldquo;Host_Writes_32MiB&rdquo; that indicate how
much data has been written to the drive (Figure 3).
</p><div       class="mediaobject"><a href="11560f3.large.jpg"><img src="11560f3.jpg"></a><div class="caption"><p>
Figure 3. smartctl Output (Trimmed)
</p></div></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x280b580.0x2cfde90"></a>
Other Generic Tips</h2></div></div><p>
<span   class="emphasis"><em>Swap:</em></span> if your computer is actively using swap space, additional RAM 
probably is a better upgrade than an SSD. Given the fact that longevity is so
tightly coupled with writes, the last thing you want is to be pumping
multiple gigabytes of swap on and off the drive.
</p><p>
<span   class="emphasis"><em>HDDs still have a role:</em></span> if you have the space, you can get the best of both
worlds by keeping your hard drive around. It's a great place for storing
music, movies and other media that doesn't require fast I/O. Depending on
how militant you want to be about SSD writes, you even can mount folders
like /tmp, /var or even just /var/log on the HDD to keep SSD writes down.
Linux's flexible mounting and partitioning tools make this a breeze.
</p><p>
<span   class="emphasis"><em>SSD free space:</em></span> SSDs run best when there's plenty of free space for them
to use for wear leveling and garbage collection. Size up and manage your SSD
to keep it less than 80% full.
</p><p>
<span   class="emphasis"><em>Things that break TRIM:</em></span> RAID setups can't pass TRIM through to the
underlying drives, so use this mode with caution. In the BIOS, make sure
your controller is set to AHCI mode and not IDE emulation, as IDE mode
doesn't support TRIM and is slower in general.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x280b580.0x2cfe200"></a>
SSD Performance</h2></div></div><p>
Now let's get to the heart of the matter&mdash;practical, real-world examples
of how an SSD will make common tasks faster.
</p><p>
<span   class="bold"><b>Test Setup</b></span>
</p><p>
Prior to benchmarking, I had one SSD for my Linux OS, another SSD for when
I needed to boot in to Windows 7 and an HDD for storing media files and for
doing low-throughput, high-volume work (like debugging JVM dumps or
encoding video). I used <tt  >partimage</tt> to back up the HDD, and then
I used a Clonezilla bootable CD to clone my Linux SSD onto the HDD. Although
most sources say you don't have to worry about fragmentation on ext4, I
used the ext4 defrag utility <tt  >e4defrag</tt> on the HDD just to give it
the best shot at keeping up with the SSD.
</p><p>
Here's the hardware on the development workstation I used for
benchmarking&mdash;pretty standard stuff: 
</p><div class="itemizedlist"><ul type="disc"><li><p>
CPU: 3.3GHz Intel Core i5-2500k CPU.
</p></li><li><p>
Motherboard: Gigabyte Z68A-D3H-B3 (Z68 chipset).
</p></li><li><p>
RAM: 8GB (2x4GB) of 1333 DDR3.
</p></li><li><p>
OS: Ubuntu 12.04 LTS (64-bit, kernel 3.5.0-39).
</p></li><li><p>
SSD: 128GB OCZ Vertex4.
</p></li><li><p>
HDD: 1TB Samsung Spinpoint F3, 7200 RPM, 32MB cache.
</p></li></ul></div><p>
I picked a set of ten tests to try to showcase some typical Linux
operations. I cleared the disk cache after each test with <tt  >echo 3 |
sudo tee /proc/sys/vm/drop_caches</tt> and rebooted after completing a set.
I ran the set five times for each drive, and plotted the mean plus a 95%
confidence interval on the bar charts shown below.
</p><p><span   class="bold"><b>
Boot Times:</b></span>
</p><p>
Because I'm the only user on the test workstation and use whole-disk
encryption, X is set up with automatic login. Once cryptsetup prompts me
for my disk password, the system will go right past the typical GDM user
login to my desktop. This complicates how to measure boot times, so to get
the most accurate measurements, I used the bootchart
package that provides a really cool Gantt chart showing the boot time of each
component (partial output shown in Figure 4).
I used the Xorg process start to indicate when X starts up, the start of
the Dropbox panel applet to indicate when X is usable and subtracted the
time spent in cryptsetup (its duration depends more on how many tries it
takes me to type in my disk password than how fast any of the disks are).
The SSD crushes the competition here. 
</p><div       class="mediaobject"><a href="11560f4.large.jpg"><img src="11560f4.jpg"></a><div class="caption"><p>
Figure 4. bootchart Output
</p></div></div><div class="table"><a name="N0x280b580.0x2cfed00"></a><p class="title"><b>Table 1. Boot Times</b></p><table     summary="Table 1. Boot Times11560t1.qrk" border="1"><colgroup><col><col><col><col></colgroup><thead><tr><th>Test</th><th>HDD (s)</th><th>SSD (s)</th><th>% Faster</th></tr></thead><tbody><tr><td>Xorg Start</td><td>19.4</td><td>4.9</td><td>75%</td></tr><tr><td>Desktop Ready</td><td>33.4</td><td>6.6</td><td>80%</td></tr></tbody></table></div><div       class="mediaobject"><img src="11560f5.jpg"><div class="caption"><p>
Figure 5. Boot Times
</p></div></div><p><span   class="bold"><b>
Application Start Times:</b></span>
</p><p>
To test application start times, I measured the start times for Eclipse 4.3
(J2EE version), <span   class="emphasis"><em>Team Fortress 2</em></span>
(<span   class="emphasis"><em>TF2</em></span>) and Tomcat 7.0.42. Tomcat had four
WAR files at about 50MB each to unpackage at start. Tomcat provides the
server startup time in the logs, but I had to measure Eclipse and <span   class="emphasis"><em>Team
Fortress</em></span>
manually. I stopped timing Eclipse once the workspace was
visible. For <span   class="emphasis"><em>TF2</em></span>, I used the time between pressing &ldquo;Play&rdquo; in the
Steam client and when the <span   class="emphasis"><em>TF2</em></span> &ldquo;Play&rdquo; menu appears.
</p><div class="table"><a name="N0x280b580.0x2c10270"></a><p class="title"><b>Table 2. Application Launch Times</b></p><table     summary="Table 2. Application Launch Times11560t2.qrk" border="1"><colgroup><col><col><col><col></colgroup><thead><tr><th>Test</th><th>HDD (s)</th><th>SSD (s)</th><th>% Faster</th></tr></thead><tbody><tr><td>Eclipse</td><td>26.8</td><td>11.0</td><td>59%</td></tr><tr><td>Tomcat</td><td>19.6</td><td>17.7</td><td>10%</td></tr><tr><td>TF2</td><td>72.2</td><td>67.1</td><td>7%</td></tr></tbody></table></div><div       class="mediaobject"><img src="11560f6.jpg"><div class="caption"><p>
Figure 6. Application Launch Times
</p></div></div><p>
There was quite a bit of variation between the three applications, where
Eclipse benefited from an SSD the most, and the gains in Tomcat and
<span   class="emphasis"><em>TF2</em></span> were
present but less noticeable.
</p><p><span   class="bold"><b>
Single-File Operations:</b></span>
</p><p>
To test single-file I/O speed, I created a ~256MB file via <tt  >time dd
if=/dev/zero of=f1 bs=1048576 count=256</tt>, copied it to a new file and
then read it via <tt  >cat</tt>, redirecting to /dev/null. I used the time
utility to capture the real elapsed time for each test.
</p><div class="table"><a name="N0x280b580.0x2c10f80"></a><p class="title"><b>Table 3. File I/O</b></p><table     summary="Table 3. File I/O11560t3.qrk" border="1"><colgroup><col><col><col><col></colgroup><thead><tr><th>Test</th><th>HDD (s)</th><th>SSD (s)</th><th>% Faster</th></tr></thead><tbody><tr><td>create</td><td>1.5</td><td>0.5</td><td>67%</td></tr><tr><td>copy</td><td>3.3</td><td>1.1</td><td>69%</td></tr><tr><td>read</td><td>2.2</td><td>0.2</td><td>63%</td></tr></tbody></table></div><div       class="mediaobject"><img src="11560f7.jpg"><div class="caption"><p>
Figure 7. File I/O
</p></div></div><p><span   class="bold"><b>
Multiple File Operations:</b></span>
</p><p>
First, I archived the 200k files in my 1.1GB Eclipse workspace via
<tt  >tar
-c ~/workspace &gt; w.tar</tt> to test archiving speed. Second, I used
<tt  >find -name "*.java" -exec fgrep "Foo" {} &gt; /dev/null</tt> to simulate
looking for a keyword in the 7k java files. I used the time
utility to capture the real elapsed time for each test. Both tests made
the HDD quite noisy, so I wasn't surprised to see a significant delta.
</p><div class="table"><a name="N0x280b580.0x2c11be0"></a><p class="title"><b>Table 4. Multi-File I/O</b></p><table     summary="Table 4. Multi-File I/O11560t4.qrk" border="1"><colgroup><col><col><col><col></colgroup><thead><tr><th>Test</th><th>HDD (s)</th><th>SSD (s)</th><th>% Faster</th></tr></thead><tbody><tr><td>tar</td><td>123.2</td><td>17.5</td><td>86%</td></tr><tr><td>find &amp; fgrep</td><td>34.3</td><td>12.3</td><td>64%</td></tr></tbody></table></div><div       class="mediaobject"><img src="11560f8.jpg"><div class="caption"><p>
Figure 8. Multi-File I/O
</p></div></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x280b580.0x2e8e1b0"></a>
Summary</h2></div></div><p>
If you haven't considered an SSD, or were holding back for any of the
reasons mentioned here, I hope this article prompts you to take the plunge and try
one out.
</p><p>
For reliability, modern SSDs are performing on par with HDDs. (You need a
good backup, either way.) If you were concerned about longevity, you can
use data from your existing system to approximate how long a current
generation MLC or TLC drive would last.
</p><p>
SSD support has been in place in Linux for a while, and it works well
even if you just do a default installation of a major Linux distribution.
TRIM support, some ext4 tweaks and monitoring via tune2fs and
smartctl are there to help you maintain and monitor overall SSD
health.
</p><p>
Finally, some real-world performance benchmarks illustrate how an SSD will
boost performance for <span   class="emphasis"><em>any</em></span> operation that uses disk storage, but
especially ones that involve many different files.
</p><p>
Because even OS-only budget-sized SSDs can provide significant performance
gains, I hope if you've been on the fence, you'll now give one a try.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x280b580.0x2e8e470"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Brian Trapp serves up a spicy gumbo of Web-based yield reporting
and analysis tools for hungry semiconductor engineers at one of the
leading semiconductor research and development consortiums. His signature
dish has a Java base with a dash of JavaScript, Perl, Bash and R, and
his kitchen has been powered by Linux ever since 1998. He works from
home in Buffalo, New York, which is a shame only because that doesn't
really
fit the whole chef metaphor.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../237/toc237.html">Issue Table of Contents</a>
    <a class="link3" href="../237/11560.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>