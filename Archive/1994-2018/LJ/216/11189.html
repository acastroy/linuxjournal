<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Ahead of the Pack: the Pacemaker High-Availability Stack
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;A high-availability stack serves one purpose: through a&#10;redundant setup of two or more nodes, ensure service availability and&#10;recover services automatically in case of a problem. Florian Haas&#10;explores Pacemaker, the state-of-the-art high-availability stack on&#10;Linux.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1199580.0x1290ac0"></a>
Ahead of the Pack: the Pacemaker High-Availability Stack
</h1></div><div><div class="author"><h3 class="author">
Florian
 
Haas
</h3></div><div class="issuemoyr">Issue #216, April 2012</div></div><div><p>
A high-availability stack serves one purpose: through a
redundant setup of two or more nodes, ensure service availability and
recover services automatically in case of a problem. Florian Haas
explores Pacemaker, the state-of-the-art high-availability stack on
Linux.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x1291250"></a></h2></div></div><p>
Hardware and software are error-prone. Eventually, a hardware issue or
software bug will affect any application. And yet, we're increasingly
expecting services&mdash;the applications that run on top of our
infrastructure&mdash;to be up 24/7 by default. And if we're not
expecting that, our bosses and our customers are. What makes
this possible is a high-availability stack: it automatically recovers
applications and services in the face of software and hardware issues,
and it ensures service availability and uptime. The definitive
open-source high-availability stack for the Linux platform builds upon
the Pacemaker cluster resource manager. And to ensure maximum
service availability, that stack consists of four layers: storage,
cluster communications, resource management and applications.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x1291408"></a>
Cluster Storage</h2></div></div><p>
The storage layer is where we keep our data. Individual cluster nodes
access this data in a joint and coordinated fashion. There are two
fundamental types of cluster storage:
</p><div class="orderedlist"><ol type="1"><li><p>
Single-instance storage is perhaps the more conventional form of
cluster storage. The cluster stores all its data in one centralized
instance, typically a volume on a SAN. Access to this data is either
from one node at a time (active/passive) or from multiple nodes
simultaneously (active/active). The latter option normally requires
the use of a shared-cluster filesystem, such as GFS2 or OCFS2. To
prevent uncoordinated access to data&mdash;a sure-fire way of shredding
it&mdash;all single-instance storage cluster architectures require the
use of fencing. Single-instance storage is very easy to set up,
specifically if you already have a SAN at your disposal, but it has a
very significant downside: if, for any reason, data becomes
inaccessible or is even destroyed, all server redundancy in your
high-availability architecture comes to naught. With no data to serve,
a server becomes just a piece of iron with little use.
</p></li><li><p>
Replicated storage solves this problem. In this architecture, there
are two or more replicas of the cluster data set, with each cluster
node having access to its own copy of the data. An underlying
replication facility then guarantees that the copies are exactly
identical at the block layer. This effectively makes replicated
storage a drop-in replacement for single-instance storage, albeit with
added redundancy at the data level. Now you can lose entire
nodes&mdash;with their data&mdash;and still have more nodes to fail over to. Several
proprietary (hardware-based) solutions exist for this purpose, but the
canonical way of achieving replicated block storage on Linux is the
Distributed Replicated Block Device (DRBD). Storage replication 
also may happen at the filesystem level, with GlusterFS and Ceph being the
most prominent implementations at this time.
</p></li></ol></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x1291720"></a>
Cluster Communications</h2></div></div><p>
The cluster communications layer serves three primary purposes: it
provides reliable message passing between cluster nodes, establishes
the cluster membership and determines quorum. The default cluster
communications layer in the Linux HA stack is Corosync, which evolved
out of the earlier, now all but defunct, OpenAIS Project.
</p><p>
Corosync implements the Totem single-ring ordering and membership
protocol, a well-studied message-passing algorithm with almost 20
years of research among its credentials. It provides a secure,
reliable means of message passing that guarantees in-order delivery
of messages to cluster nodes. Corosync normally transmits cluster
messages over Ethernet links by UDP multicast, but it also can use
unicast or broadcast messaging, and even direct RDMA over InfiniBand
links. It also supports redundant rings, meaning clusters can use two
physically independent paths to communicate and transparently fail
over from one ring to another.
</p><p>
Corosync also establishes the cluster membership by mutually
authenticating nodes, optionally using a simple pre-shared key
authentication and encryption scheme. Finally, Corosync establishes
quorum&mdash;it detects whether sufficiently many nodes have joined the
cluster to be operational.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x12918d8"></a>
Cluster Resource Management</h2></div></div><p>
In high availability, a resource can be something as simple as an IP
address that &ldquo;floats&rdquo; between cluster nodes, or something as complex
as a database instance with a very intricate configuration. Put
simply, a resource is anything that the cluster starts, stops,
monitors, recovers or moves around. Cluster resource management is
what performs these tasks for us&mdash;in an automated, transparent,
highly configurable way. The canonical cluster resource manager in
high-availability Linux is Pacemaker.
</p><p>
Pacemaker is a spin-off of Heartbeat, the high-availability stack
formerly driven primarily by Novell (which then owned SUSE) and
IBM. It re-invented itself as an independent and much more
community-driven project in 2008, with developers from Red Hat, SUSE
and NTT now being the most active contributors.
</p><p>
Pacemaker provides a distributed Cluster Information Base (CIB) in
which it records the configuration and status of all cluster
resources. The CIB replicates automatically to all cluster nodes from
the Designated Coordinator (DC)&mdash;one node that Pacemaker
automatically elects from all available cluster nodes.
</p><p>
The CIB uses an XML-based configuration format, which in releases
prior to Pacemaker 1.0 was the only way to configure the cluster&mdash;something that rightfully made potential users run away
screaming. Since these humble beginnings, however, Pacemaker has grown
into a tremendously useful, hierarchical, self-documenting text-based
shell, somewhat akin to the &ldquo;virsh&rdquo; subshell that many readers will be
familiar with from libvirt. This shell&mdash;unimaginatively called
&ldquo;crm&rdquo;
by its developers&mdash;hides all that nasty XML from users and makes the
cluster much simpler and easier to configure.
</p><p>
In Pacemaker, the shell allows us to configure cluster resources&mdash;no
surprise there&mdash;and operations (things the cluster does with
resources). Besides, we can set per-node and cluster-wide attributes,
send nodes into a standby mode where they are temporarily ineligible
for running resources, manipulate resource placement in the cluster,
and do a plethora of other things to manage our cluster.
</p><p>
Finally, Pacemaker's Policy Engine (PE) recurrently checks the cluster
configuration against the cluster status and initiates actions as
required. The PE would, for example, kick off a recurring monitor
operation on a resource (such as, &ldquo;check whether this database is
still alive&rdquo;); evaluate its status (&ldquo;hey, it's not!&rdquo;); take into
account other items in the cluster configuration (&ldquo;don't attempt to
recover this specific resource in place if it fails more than three times
in 24 hours&rdquo;); and initiate a follow-up action (&ldquo;move this
database to a different node&rdquo;). All these steps are entirely automatic
and require no human intervention, ensuring quick resource recovery
and maximum uptime.
</p><p>
At the cluster resource management level, Pacemaker uses an abstract
model where resources all support predefined, generic operations (such
as start, stop or check the status) and produce standardized return
codes. To translate these abstract operations into something that is
actually meaningful to an application, we need resource agents.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x1291eb0"></a>
Resource Agents</h2></div></div><p>
Resource agents are small pieces of code that allow Pacemaker to
interact with an application and manage it as a cluster
resource. Resource agents can be written in any language, with the
vast majority being simple shell scripts. At the time of this writing, more
than
70 individual resource agents ship with the high-availability stack
proper. Users can, however, easily drop in custom resource agents&mdash;a
key design principle in the Pacemaker stack is to make resource
management easily accessible to third parties.
</p><p>
Resource agents translate Pacemaker's generic actions into operations
meaningful for a specific resource type. For something as simple as a
virtual &ldquo;floating&rdquo; IP address, starting up the resource amounts to
assigning that address to a network interface. More complex resource
types, such as those managing database instances, come with much more
intricate startup operations. The same applies to varying
implementations of resource shutdown, monitoring and migration: all
these operations can range from simple to complex, depending on
resource type.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x1292068"></a>
Highly Available KVM: a Simple Pacemaker Cluster</h2></div></div><p>
This reference configuration consists of a three-node cluster with
single-instance iSCSI storage. Such a configuration is easily capable
of supporting more than 20 highly available virtual machine instances,
although for the sake of simplicity, the configuration shown here 
includes only three. You can complete this configuration on any recent
Linux distribution&mdash;the Corosync/Pacemaker stack is universally
available on CentOS 6,[1] Fedora, OpenSUSE and SLES, Debian, Ubuntu
and other platforms. It is also available in RHEL 6, albeit as a
currently unsupported Technology Preview. Installing the
<tt  >pacemaker</tt>,
<tt  >corosync</tt>, <tt  >libvirt</tt>,
<tt  >qemu-kvm</tt> and <tt  >open-iscsi</tt> packages should be
enough on all target platforms&mdash;your preferred package manager will
happily pull in all package dependencies.
</p><p>
This example assumes that all three cluster nodes&mdash;alice, bob and
charlie&mdash;have iSCSI access to a portal at 192.168.122.100:3260, and
are allowed to connect to the iSCSI target whose IQN is
<tt  >iqn.2011-09.com.hastexo:virtcluster</tt>. Further, three libvirt/KVM virtual
machines&mdash;xray, yankee and zulu&mdash;have been pre-installed, and each
uses one of the volumes (LUNs) on the iSCSI target as its virtual
block device. Identical copies of the domain configuration files exist in the
default configuration directory, /etc/libvirt/qemu, on all three
physical nodes.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x12923d8"></a>
Corosync</h2></div></div><p>
Corosync's configuration files live in /etc/corosync, and the central
configuration is in /etc/corosync/corosync.conf. Here's an example of
the contents of this file:

<pre     class="programlisting">

totem {
  # Enable node authentication &amp; encryption
  secauth: on

  # Redundant ring protocol: none, active, passive.
  rrp_mode: active
  
  # Redundant communications interfaces
  interface {
    ringnumber: 0
    bindnetaddr: 192.168.0.0
    mcastaddr: 239.255.29.144
    mcastport: 5405
  }
  interface {
    ringnumber: 1
    bindnetaddr: 192.168.42.0
    mcastaddr: 239.255.42.0
    mcastport: 5405
  }
}

amf {
  mode: disabled
}

service {
  # Load Pacemaker
  ver: 1
  name: pacemaker
}

logging {
  fileline: off
  to_stderr: yes
  to_logfile: no
  to_syslog: yes
  syslog_facility: daemon
  debug: off
  timestamp: on
}

</pre>
</p><p>
The important bits here are the two interface declarations enabling
redundant cluster communications and the corresponding
<tt  >rrp_mode</tt>
declaration. Mutual node authentication and encryption (<tt  >secauth
on</tt>) is
good security practice. And finally, the <tt  >service</tt> stanza loads the
Pacemaker cluster manager as a Corosync plugin.
</p><p>
With secauth enabled, Corosync also requires a shared secret for
mutual node authentication. Corosync uses a simple 128-byte secret
that it stores as /etc/corosync/authkey, and which you easily
can generate with the <tt  >corosync-keygen</tt> utility.
</p><p>
Once corosync.conf and authkey are in shape, copy them over to all
nodes in your prospective cluster. Then, fire up Corosync cluster
communications&mdash;a simple <tt  >service corosync start</tt> will do.
</p><p>
Once the service is running on all nodes, the command
<tt  >corosync-cfgtool -s</tt> will display both rings as healthy, and the
cluster is ready to communicate:

<pre     class="programlisting">
Printing ring status.
Local node ID 303938909
RING ID 0
        id      = 192.168.0.1
        status  = ring 0 active with no faults
RING ID 1
        id      = 192.168.42.1
        status  = ring 1 active with no faults
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x1292900"></a>
Pacemaker</h2></div></div><p>
Once Corosync runs, we can start Pacemaker with the <tt  >service pacemaker
start</tt> command. After a few seconds, Pacemaker elects a Designated
Coordinator (DC) node among the three available nodes and commences
full cluster operations. The <tt  >crm_mon</tt> utility, executable on any
cluster node, then produces output similar to this:


<pre     class="programlisting">
============
Last updated: Fri Feb  3 18:40:15 2012
Stack: openais
Current DC: bob - partition with quorum
Version: 1.1.6-4.el6-89678d4947c5bd466e2f31acd58ea4e1edb854d5
3 Nodes configured, 3 expected votes
0 Resources configured.
============
</pre>
</p><p>
The output produced by <tt  >crm_mon</tt> is a more user-friendly
representation of the internal cluster configuration and status stored
in a distributed XML database, the Cluster Information Base (CIB). Those interested and brave enough to care about the internal
representation are welcome to make use of the <tt  >cibadmin
-Q</tt>
command. But be warned, before you do so, you may want to instruct the
junior sysadmin next to you to get some coffee&mdash;the
avalanche of XML gibberish that <tt  >cibadmin</tt> produces can be
intimidating to the uninitiated novice.
</p><p>
Much less intimidating is the standard configuration facility for
Pacemaker, the crm shell. This self-documenting, hierarchical,
scriptable subshell is the simplest and most universal way of
manipulating Pacemaker clusters. In its configure submenu, the shell
allows us to load and import configuration snippets&mdash;or even
complete configurations, as below:

<pre     class="programlisting">
primitive p_iscsi ocf:heartbeat:iscsi \
  params portal="192.168.122.100:3260" \
    target="iqn.2011-09.com.hastexo:virtcluster" \
  op monitor interval="10"
primitive p_xray ocf:heartbeat:VirtualDomain \
  params config="/etc/libvirt/qemu/xray.xml" \
  op monitor interval="30" timeout="30" \
  op start interval="0" timeout="120" \
  op stop interval="0" timeout="120"
primitive p_yankee ocf:heartbeat:VirtualDomain \
  params config="/etc/libvirt/qemu/yankee.xml" \
  op monitor interval="30" timeout="30" \
  op start interval="0" timeout="120" \
  op stop interval="0" timeout="120"
primitive p_zulu ocf:heartbeat:VirtualDomain \
  params config="/etc/libvirt/qemu/zulu.xml" \
  op monitor interval="30" timeout="30" \
  op start interval="0" timeout="120" \
  op stop interval="0" timeout="120"
clone cl_iscsi p_iscsi
colocation c_xray_on_iscsi inf: p_xray cl_iscsi
colocation c_yankee_on_iscsi inf: p_yankee cl_iscsi
colocation c_zulu_on_iscsi inf: p_zulu cl_iscsi
order o_iscsi_before_xray inf: cl_iscsi p_xray
order o_iscsi_before_yankee inf: cl_iscsi p_yankee
order o_iscsi_before_zulu inf: cl_iscsi p_zulu
</pre>
</p><p>
Besides defining our three virtual domains as resources under full
cluster management and monitoring (<tt  >p_xray</tt>,
<tt  >p_yankee</tt> and <tt  >p_zulu</tt>),
this configuration also ensures that all domains can find their
storage (the <tt  >cl_iscsi</tt> clone), and that they wait until iSCSI storage
becomes available (the <tt  >order</tt> and
<tt  >colocation</tt> constraints).
</p><p>
This being a single-instance storage cluster, it's imperative that we
also employ safeguards against shredding our data. This is commonly
known as node fencing, but Pacemaker uses the more endearing term
STONITH (Shoot The Other Node In The Head) for the same concept. A
ubiquitous means of node fencing is controlling nodes via their IPMI
Baseboard Management Controllers, and Pacemaker supports this natively:

<pre     class="programlisting">
primitive p_ipmi_alice stonith:external/ipmi \
  params hostname="alice" ipaddr="192.168.15.1" \
    userid="admin" passwd="foobar" \
  op start interval="0" timeout="60" \
  op monitor interval="120" timeout="60"
primitive p_ipmi_bob stonith:external/ipmi \
  params hostname="bob" ipaddr="192.168.15.2" \
    userid="admin" passwd="foobar" \
  op start interval="0" timeout="60" \
  op monitor interval="120" timeout="60"
primitive p_ipmi_charlie stonith:external/ipmi \
  params hostname="charlie" ipaddr="192.168.15.3" \
    userid="admin" passwd="foobar" \
  op start interval="0" timeout="60" \
  op monitor interval="120" timeout="60"
location l_ipmi_alice p_ipmi_alice -inf: alice
location l_ipmi_bob p_ipmi_bob -inf: bob
location l_ipmi_charlie p_ipmi_charlie -inf: charlie
property stonith-enabled="true"
</pre>
</p><p>
The three location constraints here ensure that no node has to shoot
itself.
</p><p>
Once that configuration is active, Pacemaker fires up resources as
determined by the cluster configuration. Again, we can query the cluster
state with the <tt  >crm_mon</tt> command, which now produces much more
interesting output than before:


<pre     class="programlisting">
============
Last updated: Fri Feb  3 19:46:29 2012
Stack: openais
Current DC: bob - partition with quorum
Version: 1.1.6-4.el6-89678d4947c5bd466e2f31acd58ea4e1edb854d5
3 Nodes configured, 3 expected votes
9 Resources configured.
============

Online: [ alice bob charlie ]

 Clone Set: cl_iscsi [p_iscsi]
     Started: [ alice bob charlie ]
 p_ipmi_alice   (stonith:external/ipmi):        Started bob
 p_ipmi_bob     (stonith:external/ipmi):        Started charlie
 p_ipmi_charlie	(stonith:external/ipmi):        Started alice
 p_xray	(ocf::heartbeat:VirtualDomain):	Started alice
 p_yankee       (ocf::heartbeat:VirtualDomain):	Started bob
 p_zulu	(ocf::heartbeat:VirtualDomain):	Started charlie
</pre>
</p><p>
Note that by default, Pacemaker clusters are symmetric. The resource
manager balances resources in a round-robin fashion among cluster
nodes.
</p><p>
This configuration protects against both resource and node failure. If
one of the virtual domains crashes, Pacemaker recovers the KVM
instance in place. If a whole node goes down, Pacemaker reshuffles the
resources so the remaining nodes take over the services that the
failed node hosted. In the screen dump below, charlie has failed and
bob has duly taken over the virtual machine that charlie had hosted:


<pre     class="programlisting">
============
Last updated: Sat Feb  4 16:18:00 2012
Stack: openais
Current DC: bob - partition with quorum
Version: 1.1.6-4.el6-89678d4947c5bd466e2f31acd58ea4e1edb854d5
3 Nodes configured, 2 expected votes
9 Resources configured.
============

Online: [ alice bob ]
OFFLINE: [ charlie ]

Full list of resources:

 Clone Set: cl_iscsi [p_iscsi]
     Started: [ alice bob ]
     Stopped: [ p_iscsi:2 ]
 p_ipmi_alice   (stonith:external/ipmi):        Started bob
 p_ipmi_bob     (stonith:external/ipmi):        Started alice
 p_ipmi_charlie (stonith:external/ipmi):        Started alice
 p_xray	(ocf::heartbeat:VirtualDomain):	Started bob
 p_yankee       (ocf::heartbeat:VirtualDomain):	Started bob
 p_zulu	(ocf::heartbeat:VirtualDomain):	Started alice
</pre>
</p><div       class="mediaobject"><a href="11189f1.large.jpg"><img src="11189f1.jpg"></a><div class="caption"><p>
Figure 1. Normal operation; virtual domains spread across all three cluster
nodes. 
</p></div></div><div       class="mediaobject"><a href="11189f2.large.jpg"><img src="11189f2.jpg"></a><div class="caption"><p>
Figure 2. Node charlie has failed; alice has automatically taken over
virtual domain zulu.
</p></div></div><p>

Once the host charlie recovers, resources can optionally shift back
to the recovered host automatically, or they can stay in place until an
administrator reassigns them at the time of her choosing.
</p><p>
In this article, I barely scratched the surface of the Linux
high-availability stack's capabilities. Pacemaker supports a diverse
set of recovery policies, resource placement strategies and cluster
constraints, making the stack enormously powerful. 
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x128a108"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Glossary</b></p><p>
<span   class="bold"><b>Node</b></span>: in cluster terminology, any system (typically a server) that
participates in cluster communications and can potentially host
cluster resources.
</p><p>
<span   class="bold"><b>Fencing</b></span>: a means of coordinating access to shared resources in the
face of communications failure. Once a node stops responding to cluster
messages unexpectedly (as opposed to gracefully signing off), other
nodes shut it down to ensure it no longer has access to any shared
resources. Usually enabled by making an out-of-band connection to the
offending node and flipping the virtual power switch, IPMI-over-LAN
being the most widely used implementation.
</p><p>
<span   class="bold"><b>Resource</b></span>: anything that a cluster typically manages. Resources can be
very diverse, from simple IP addresses to complex database instances
or d&aelig;mons.
</p><p>
<span   class="bold"><b>Ring</b></span>: in Totem protocol terminology, one of the (typically redundant)
links over which cluster messaging communicates.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1199580.0x128a580"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
&ldquo;The Totem Single-Ring Ordering and Membership Protocol&rdquo; (research
paper explaining the Totem protocol):
<a href="http://www.cs.jhu.edu/~yairamir/tocs.ps" target="_self">www.cs.jhu.edu/~yairamir/tocs.ps</a>
</p><p>
&ldquo;Clusters From Scratch&rdquo; (hands-on documentation for Pacemaker
novices):
<a href="http://www.clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Clusters_from_Scratch" target="_self">www.clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Clusters_from_Scratch</a>
</p><p>
&ldquo;Pacemaker Configuration Explained&rdquo; (reference documentation of
Pacemaker configuration internals, not for the faint at heart):
<a href="http://www.clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained" target="_self">www.clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained</a>
</p></div></div></div>
<div class="authorblurb"><p>
Florian Haas is a Principal Consultant and co-founder at hastexo, an
independent professional services organization specializing in Linux
high availability, storage replication and the open-source cloud. He
has been active in this community for half a decade, and has been
called one of &ldquo;the jedi grandmasters&rdquo; of Linux high availability. He
frequently speaks on high availability and cloud topics at technical
conferences.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../216/toc216.html">Issue Table of Contents</a>
    <a class="link3" href="../216/11189.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>