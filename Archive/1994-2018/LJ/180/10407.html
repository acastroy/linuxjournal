<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Paranoid Penguin
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Nurture your inner control freak with Squid.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x27ad580.0x28a4ac0"></a>
Paranoid Penguin
</h1></div><div><h3 class="subtitle"><i>
Building a Secure Squid Web Proxy, Part I
</i></h3></div><div><div class="author"><h3 class="author">
Mick
 
Bauer
</h3></div><div class="issuemoyr">Issue #180, April 2009</div></div><div><p>
Nurture your inner control freak with Squid.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x27ad580.0x28a5460"></a></h2></div></div><p>

Consider the venerable Web proxy&mdash;back when the Internet was new to most
of us, setting up a Web proxy was a convenient way to grant users of an
otherwise non-Internet-connected network access to the World Wide Web. The
proxy also provided a convenient point to log outbound Web requests, to
maintain whitelists of allowed sites or blacklists of forbidden sites
and to enforce an extra layer of authentication in cases where some, but
not all, of your users had Internet privileges.
</p><p>
Nowadays, of course, Internet access is ubiquitous. The eclipsing of
proprietary LAN protocols by TCP/IP, combined with the technique of
Network Address Translation (NAT), has made it easy to grant direct
access from &ldquo;internal&rdquo; corporate and organizational networks to Internet
sites. So the whole idea of a Web proxy is sort of obsolete, right?
</p><p>
Actually, no.
</p><p>
After last month's editorial, we return to technical matters&mdash;specifically, to the venerable but assuredly not obsolete Web proxy. This
month, I describe, in depth, the security benefits of proxying your
outbound Web traffic, and some architectural and design considerations
involved with doing so. In subsequent columns, I'll show you how to
build a secure Web proxy using Squid, the most popular open-source Web
proxy package, plus a couple of adjunct programs that add key security
functionality to Squid.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x27ad580.0x28a5778"></a>
What Exactly Is a Web Proxy?</h2></div></div><p>
The last time I discussed proxies in this space was in my December 2002
article &ldquo;Configuring and Using an FTP Proxy&rdquo;. (Where does the time
go?) A quick definition, therefore, is in order.
</p><p>
The concept of a Web proxy is simple. Rather than allowing client
systems to interact directly with Web servers, a Web proxy impersonates
the server to the client, while simultaneously opening a
<span   class="emphasis"><em>second</em></span>
connection to the Web server on the client's behalf and impersonating
the client to that server.
This is illustrated in Figure 1.
</p><div       class="mediaobject"><a href="10407f1.large.jpg"><img src="10407f1.jpg"></a><div class="caption"><p>
Figure 1. How Web Proxies Work
</p></div></div><p>
Because Web proxies have been so common for so long, all major Web
browsers can be configured to communicate directly through Web proxies
in a &ldquo;proxy-aware&rdquo; fashion. Alternatively, many Web proxies support
&ldquo;transparent&rdquo; operation, in which Web clients are unaware of the proxy's
presence, but their traffic is diverted to the proxy via firewall rules
or router policies.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x27ad580.0x28a5c48"></a>
Why Proxy?</h2></div></div><p>
Just because nowadays it's <span   class="emphasis"><em>easy</em></span> to interconnect
TCP/IP networks directly doesn't mean you always
<span   class="emphasis"><em>should</em></span>. If a nasty worm
infects systems on your internal network, do you want to deal with
the ramifications of the infection spreading outward, for example, to
some critical business partner with whom your users communicate over
the Internet?
</p><p>
In many organizations, network engineers take it for granted that all
connected systems will use a &ldquo;default route&rdquo; that provides a path out
to the Internet. In other organizations, however, it's considered much
less risky to direct all Web traffic out through a controlled Web proxy
to which routes are internally published and to use <span   class="emphasis"><em>no default route
whatsoever</em></span> at the LAN level.
</p><p>
This has the effect of allowing users to reach the Internet via the Web
proxy&mdash;that is, to surf the Web&mdash;but not to use the Internet for non-Web
applications, such as IRC, on-line gaming and so forth. It follows that
what end users can't do, neither can whatever malware that manages to
infect their systems.
</p><p>
Obviously, this technique works only if you've got other types of
gateways for the non-Web traffic you need to route outward, or if the
only outbound Internet traffic you need to deal with is Web traffic. My
point is, a Web proxy can be a very useful tool in controlling outbound
Internet traffic.
</p><p>
What if your organization is in a regulated industry, in which it's
sometimes necessary to track some users' Web access? You can do that
on your firewall, of course, but generally speaking, it's a bad idea to
make a firewall log more than you have to for forensics purposes. This
is because logging is I/O-intensive, and too much of it can 
impact negatively the firewall's ability to fulfill its primary function, assessing
and dealing with network transactions. (Accordingly, it's common practice
mainly to log &ldquo;deny/reject&rdquo; actions on firewalls and not to
log &ldquo;allowed&rdquo;
traffic except when troubleshooting.)
</p><p>
A Web proxy, therefore, provides a better place to capture and record
logs of Web activity than on firewalls or network devices.
</p><p>
Another important security function of Web proxies is blacklisting. This
is an unpleasant topic&mdash;if I didn't believe in personal choice and
freedom, I wouldn't have been writing about open-source software since
2000&mdash;but the fact is that many organizations have legitimate, often
critical, reasons for restricting their users' Web access.
</p><p>
A blacklist is a list of forbidden URLs and name domains. A good
blacklist allows you to choose from different categories of URLs to block,
such as
social networking, sports, pornography, known spyware-propagators and so
on.
Note that not all blacklist categories necessarily involve restricting
personal freedom per se; some blacklists provide categories of &ldquo;known
evil&rdquo; sites that, regardless of whatever content they're actually
advertising, are known to try to infect users with spyware or adware,
or otherwise attack unsuspecting visitors.
</p><p>
And, I think a lot of Web site visitors do tend to be unsuspecting. The
classic malware vector is the e-mail attachment&mdash;an image or executable
binary that you trick the recipient into double-clicking on. But, what if
you could execute code on users' systems without having to trick them
into doing anything but visit a Web page?
</p><p>
In the post-Web 2.0 world, Web pages nearly always contain some sort of
executable code (Java, JavaScript, ActiveX, .NET, PHP and so on), and even
if your victim is running the best antivirus software with the latest
signatures, it won't examine any of that code, let alone identify evil
behavior in it. So, sure enough, the &ldquo;hostile Web site&rdquo; has become the
cutting edge in malware propagation and identity theft.
</p><p>
Phishing Web sites typically depend on DNS redirection (usually
through cache poisoning), which involves redirecting a
<span   class="emphasis"><em>legitimate</em></span>
URL to an attacker's IP address rather than that site's real IP, so
they're difficult to protect against in URL or domain blacklists. (At any
rate, none of the free blacklists I've looked at include a
phishing
category.) Spyware, however, is a common blacklist category, and a
good blacklist contains thousands of sites known to propagate client-side
code you almost certainly don't want executed on your users' systems.
</p><p>
Obviously, no URL blacklist ever can cover more than a tiny fraction of
the actual number of hostile Web sites active at any given moment. The
real solution to the problem of hostile Web sites is some combination
of client/endpoint security controls, better Web browser and operating
system design, and in advancing the antivirus software industry beyond
its reliance on virus signatures (hashes of known evil files),
which it's been stuck on for decades.
</p><p>
Nevertheless, at this very early stage in our awareness of and ability
to mitigate this type of risk, blacklists add <span   class="emphasis"><em>some</em></span> measure of
protection where presently there's very little else. So, regardless of
whether you need to restrict user activity per se (blocking access to
porn and so forth), a blacklist with a well-maintained spyware category
may be all the justification you need to add blacklisting capabilities
to your Web proxy. SquidGuard can be used to add blacklists to the Squid
Web proxy.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x27ad580.0x28a6590"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Just How Intelligent Is a Web Proxy?</b></p><p>
You should be aware of two important limitations in Web proxies. First, Web proxies generally aren't very smart about
detecting evil Web content. Pretty much anything in the payloads
of RFC-compliant HTTP and HTTP packets will be copied verbatim from
client-proxy transactions to proxy-server transactions, and vice versa.
</p><p>
Blacklists can somewhat reduce the chance of your users visiting
evil sites in the first place, and content filters can check
for <span   class="emphasis"><em>inappropriate</em></span> content and perhaps for viruses.
But,
hostile-Web-content attacks, such as invisible iframes that tell an
attacker's evil Web application which sites you've visited, typically
will not be detected or blocked by Squid or other mainstream Web proxies.
</p><p>
Note that enforcing RFC compliance is nothing to sneeze at. It constitutes
a type of input validation that could mitigate the risk of certain types
of buffer-overflow (and other unexpected server response) attacks. But
nonetheless, it's true that many, many types of server-side evil can be
perpetrated well within the bounds of RFC-compliant HTTP messages.
</p><p>
Second, encrypted HTTPS (SSL or TLS) sessions aren't truly proxied.
They're tunneled through the Web proxy. The contents of HTTPS sessions
are, in practical terms, completely opaque to the Web proxy.
</p></div><p>
If you're serious about blocking access to sites that are
inappropriate for your users, blacklisting is an admittedly primitive
approach. Therefore, in addition to blacklists, it makes sense
to do some sort of content filtering as well&mdash;that is, automated inspection of
actual Web content (in practice, mainly text) to determine its nature
and manage it accordingly. DansGuardian is an open-source Web content
filter that even has antivirus capabilities.
</p><p>
What if you need to limit use of your Web proxy, but for some reason,
can't use a simple source-IP-address-based Access Control List (ACL)? One
way to do this is by having your Web proxy authenticate users. Squid
supports authentication via a number of methods, including LDAP, SMB and
PAM. However, I'm probably not going to cover Web proxy authentication
here any time soon&mdash;802.1x is a better way to authenticate users and
devices at the network level.
</p><p>
Route-limiting, logging, blacklisting and authenticating are all security
functions of Web proxies. I'd be remiss, however, not to mention the
main reason many organizations deploy Web proxies, even though it isn't
directly security-related&mdash;performance. By caching commonly accessed files
and Web sites, a Web proxy can reduce an organization's
Internet bandwidth usage significantly, while simultaneously speeding up end-users'
sessions.
</p><p>
Fast and effective caching is, in fact, the primary design goal for Squid,
which is why some of the features I've discussed here require add-on
utilities for Squid (for example, blacklisting requires SquidGuard).
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x27ad580.0x28a6ab8"></a>
Web Proxy Architecture</h2></div></div><p>
Suppose you find all of this very convincing and want to use a Web
proxy to enforce blacklists and conserve Internet bandwidth. Where
in your network topology should the proxy go?
</p><p>
Unlike a firewall, a Web proxy doesn't need to be, nor should it be,
placed &ldquo;in-line&rdquo; as a choke point between your LAN and your
Internet's uplink, although it is a good idea to place it in a DMZ network. If you
have no default route, you can force all Web traffic to exit via
the proxy by a combination of firewall rules, router ACLs and end-user
Web browser configuration settings.
Consider the network shown in Figure 2.
</p><div       class="mediaobject"><img src="10407f2.jpg"><div class="caption"><p>
Figure 2. Web Proxy Architecture
</p></div></div><p>
In Figure 2, Firewall 1 allows all outbound traffic to reach TCP port
3128 on the proxy in the DMZ. It does <span   class="emphasis"><em>not</em></span> allow any outbound traffic
directly from the LAN to the Internet. It passes only packets explicitly
addressed to the proxy. Firewall 2 allows all outbound traffic on TCP 80
and 443 from the proxy (and only from the proxy) to the entire Internet.
</p><p>
Because the proxy is connected to a switch or router in the DMZ, if some
emergency occurs in which the proxy malfunctions but outbound Web traffic
must still be passed, a simple firewall rule change can accommodate this.
The proxy is only a logical control point, not a physical one.
</p><p>
Note also that this architecture could work with transparent
proxying as well, if Firewall 1 is configured to redirect all outbound Web
transactions to the Web proxy, and Firewall 2 is configured to redirect
all inbound replies to Web transactions to the proxy.
</p><p>
You may be wondering, why does the Web proxy need to reside in a
DMZ? Technically, it doesn't. You could put it on your LAN and have
essentially identical rules on Firewalls 1 and 2 that allow outbound
Web transactions only if they originate from the proxy.
</p><p>
But, what if some server-side attacker somehow manages to get at your
Web proxy via some sort of &ldquo;reverse-channel&rdquo; attack that, for example,
uses an unusually large HTTP response to execute a buffer-overflow attack
against Squid? If the Web proxy is in a DMZ, the attacker will 
be able to attack systems on your LAN only through
<span   class="emphasis"><em>additional</em></span> reverse-channel attacks that somehow exploit user-initiated outbound connections,
because Firewall 1 allows no DMZ-originated, inbound transactions. It
allows only LAN-originated, outbound transactions.
</p><p>
In contrast, if the Web proxy resides on your LAN, the attacker 
needs to get lucky with a reverse-channel attack only
<span   class="emphasis"><em>once</em></span> and can
scan for and execute more conventional attacks against your internal
systems. For this reason, I think Web proxies are ideally situated in DMZ
networks, although I acknowledge that the probability of a well-configured,
well-patched Squid server being compromised via firewall-restricted Web
transactions is probably low.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x27ad580.0x2c9f3e8"></a>
Yet to Come in This Series</h2></div></div><p>
I've explained (at a high level) how Web proxies work, described some
of their security benefits and shown how they might fit into one's
perimeter network architecture. What, exactly, will we be doing in
subsequent articles?
</p><p>
First, we'll obtain and install Squid and create a basic configuration
file. Next, we'll &ldquo;harden&rdquo; Squid so that only our intended users can
proxy connections through it.
</p><p>
Once all that is working, we'll add SquidGuard for blacklisting,
and DansGuardian for content filtering. I'll at least give pointers
on using other add-on tools for Squid administration, log analysis
and other useful functions.
</p><p>
Next month, therefore, we'll roll up our sleeves and plunge right in to
the guts of Squid configuration and administration. Until then, be safe!
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x27ad580.0x2c9f650"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>&ldquo;Configuring and Using an FTP Proxy&rdquo; by Mick Bauer,
<span   class="emphasis"><em>LJ</em></span>, December 2002: <a href="http://www.linuxjournal.com/article/6333" target="_self">www.linuxjournal.com/article/6333</a>
</p><p>
The Squid home page, where you can obtain the latest source code and
binaries for Squid:
<a href="http://www.squid-cache.org" target="_self">www.squid-cache.org</a>
</p><p>
The Squid User's Guide:
<a href="http://www.deckle.co.za/squid-users-guide/Main_Page" target="_self">www.deckle.co.za/squid-users-guide/Main_Page</a>
</p><p>
The SquidGuard home page&mdash;SquidGuard allows you to enforce blacklists
with Squid:
<a href="http://www.squidguard.org" target="_self">www.squidguard.org</a>
</p><p>
The DansGuardian home page, a free content-filtering engine that can
be used in conjunction with Squid:
<a href="http://dansguardian.org" target="_self">dansguardian.org</a>
</p></div></div></div>
<div class="authorblurb"><p>
Mick Bauer (<a href="mailto:darth.elmo@wiremonkeys.org">darth.elmo@wiremonkeys.org</a>) is Network
Security
Architect for one of the US's largest banks. He is the author of
the O'Reilly book <span   class="emphasis"><em>Linux Server Security</em></span>, 2nd edition
(formerly called
<span   class="emphasis"><em>Building Secure Servers With Linux</em></span>), an occasional
presenter at
information security conferences and composer of the &ldquo;Network
Engineering Polka&rdquo;.

</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../180/toc180.html">Issue Table of Contents</a>
    <a class="link3" href="../180/10407.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>