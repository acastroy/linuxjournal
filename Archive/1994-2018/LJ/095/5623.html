<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Client-Side Web Scripting</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Marco shows you how to read or download only the parts that&#10;    interest you from a web page.&#10;    "><meta name="keywords" content="web, scripting, Perl"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1bbf580.0x1cb6ab0"></a>Client-Side Web Scripting</h1></div><div><div class="author"><h3 class="author">Marco Fioretti</h3></div><div class="issuemoyr">Issue #95, March 2002</div></div><div><p>
    Marco shows you how to read or download only the parts that
    interest you from a web page.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb7298"></a></h2></div></div><p>There are many web browsers and FTP
clients for Linux, all rich in features and able to satisfy all
users, from command-line fanatics to 3-D multiscreen desktop
addicts. They all share one common defect, however: you have to be
at the keyboard to drive them. Of course, fine tools like wget can
mirror a whole site while you sleep, but you still have to find the
right URL first, and when it's finished you must read through every
bit that was downloaded anyway.
</p><p>With small, static sites, it's no big deal, but what if every
day you want to download a page that is given a random URL? Or what
if you don't want to read 100K of stuff just to scroll a few
headlines?</p><p>Enter client-side web scripting, i.e., all the techniques
that allow you to spend time only looking at web pages (or parts of
them) that interest you, and only after your computer found them
for you. With such scripts you could read only the traffic or
weather information related to your area, download only certain
pictures from a web page or automatically find the single link you
need.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb7450"></a>Mandatory Warning about Copyright and Bandwidth
Issues</h2></div></div><p>Besides saving time, client-side web scripting lets you learn
about some important issues and teaches you some self-discipline.
For one thing, doing indiscriminately what is explained here may be
considered copyright infringement in some cases or may consume so
much bandwidth as to cause the shutdown of your internet account or
worse. On the other hand, this freedom to surf is possible only as
long as web pages remain in nonproprietary languages (HTML/XML),
written in nonproprietary ASCII.</p><p>Finally, many fine sites can survive and remain available at
no cost only if they send out enough banners, so all this really
should be applied with moderation.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb75b0"></a>What Is Available</h2></div></div><p>As usual, before doing something from scratch, one should
check what has already been done and reuse it, right? A quick
search on <a href="http://Freshmeat.net" target="_self">Freshmeat.net</a>
for &ldquo;news ticker&rdquo; returns 18 projects, from Kticker to K.R.S.S to
GKrellM Newsticker.</p><p>These are all very valid tools, but they only fetch news, so
they won't work without changes in different cases. Furthermore,
they are almost all graphical tools, not something you can run as a
cron entry, maybe piping the output to some other program.</p><p>In this field, in order to scratch only your very own itch,
it is almost mandatory to write something for yourself. This is
also the reason why we don't present any complete solution here,
but rather discuss the general methodology.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb77c0"></a>What Is Needed</h2></div></div><p>The only prerequisites to take advantage of this article are
to know enough Perl to put together some regular expressions and
the following Perl modules: LWP::UserAgent, LWP::Simple,
HTML::Parse, HTML::Element, URI::URL and Image::Grab. You can fetch
these from CPAN
(<a href="http://www.cpan.org" target="_self">www.cpan.org</a>). Remember
that, even if you do not have the root password of your system
(typically on your office computer), you still can install them in
the directory of your choice, as explained in the Perl
documentation and the relevant README files.</p><p>Everything in this article has been tested under Red Hat
Linux 7.2, but after changing all absolute paths present in the
code, should work on every UNIX system supporting Perl and the
several external applications used.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb7978"></a>Collecting the Basic Information</h2></div></div><p>All the tasks described below, and web-client scripting in
general, require that you can download and store internally for
further analysis the whole content of some initial web page, its
last modification date, a list of all the URLs it contains or any
combination of the above. All this information can be collected
with a few lines of code at the beginning of each web-client
script, as shown in Listing 1.</p><p><a href="5623l1.html" target="_self">Listing 1. Collecting the Basic
Information</a></p><p>The code starts with the almost mandatory &ldquo;use strict&rdquo;
directive and then loads all the required Perl modules. Once that
is done, we proceed to save the whole content of the web page in
the $HTML_FILE variable via the get() method. With the instruction
that follows, we save each line of the HTTP header in one element
of the @HEADER array. Finally, we define an array (@ALL_URLS), and
with a for() cycle, we extract and save inside it all the links
contained in the original web page, making them absolute if
necessary (with the abs() method). At the end of the cycle, the
@ALL_URLS array will contain all the URLs found in the initial
document.</p><p>A complete description of the Perl methods used in this code,
and much more, can be found in the book <span   class="emphasis"><em>Web Client
Programming</em></span> (see Resources).</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb7c38"></a>Download Web Pages from the Command Line</h2></div></div><p>After having collected all this material, we can start to use
it. If you simply want to save the content of a web page on your
disk for later reading, you have to add a print instruction to the
original script:</p><pre     class="programlisting">
print $HTML_FILE;
</pre><p>And then run it from your shell prompt:
<pre     class="programlisting">
./webscript.pl http://www.fsf.org &gt; fsf.html
</pre>


This will allow you to save the whole page in the local file
fsf.html. Keep in mind, however, that if this is all you want, wget
is a better choice (see Resources, &ldquo;Downloading without a
Browser&rdquo;).
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb7e48"></a>Save the Images Contained in a Web Page to
Disk</h2></div></div><p>If all the absolute URLs are already inside the @ALL_URLS
array, we can download all the images with the following for()
cycle:</p><pre     class="programlisting">
foreach my $GRAPHIC_URL (grep /(gif|jpg|png)$/,
@ALL_URLS) {
  $GRAPHIC_URL =~ m/([^\/]+)$/;
  my $BASENAME = $1;
  print STDERR "SAVING  $GRAPHIC_URL
  in $BASENAME....\n";
  my $IMG = get ($GRAPHIC_URL);
  open (IMG_FILE, "&gt; $BASENAME") ||
  die "Failed opening $BASENAME\n";
  print IMG_FILE $IMG;
  close IMG;
  }
</pre><p>The loop operates on all the URLs contained in the document
ending with the .gif, .jpg or .png extension (extracted from the
original array with the grep instruction). First, the regular
expression finds the actual filename, defined as everything in the
URL from the rightmost slash sign to the end; this should be
generalized to deal with URLs hosted on those systems so twisted
that even the directory separator is backward.
</p><p>The result of the match is loaded in the $BASENAME variable,
and the image itself is saved with the already known get() method
inside $IMG. After that, we open a file with the proper name and
print the whole thing inside it.</p><p>Of course, many times you will not be interested in all the
images (especially because many of them usually will be advertising
banners, the site logo or other uninteresting stuff). In situations
like this, a simple look at the HTML source will help you figure
out what sets the image you need apart from the rest. For example,
you may find out that the interesting picture has a random name but
is always the third one in the list. If this is the case, modify
the previous loop as follows:</p><pre     class="programlisting">
my $IMG_COUNT  = 0;
my $WANTED_IMG = 3;
foreach my $GRAPHIC_URL (grep /(gif|jpg|png)$/,
@ALL_URLS) {
        $IMG_COUNT++;
        next unless ($IMG_COUNT == $WANTED_IMG);
        # rest of loop as before.....
        last if ($IMG_COUNT == $WANTED_IMG);
        }
print "FILE NOT FOUND TODAY\n" if
($IMG_COUNT != $WANTED_IMG);
</pre><p>The first instruction in the loop increments the image
counter; the second jumps to the next iteration until we reach the
third picture. The &ldquo;last&rdquo; instruction avoids unnecessary
iterations, and the one after the loop informs that the script
could not perform the copy because it found less than $WANTED_IMG
pictures in the source code.
</p><p>If the image name is not completely random, it's even easier
because you can filter directly on it in the grep instruction at
the beginning:</p><pre     class="programlisting">
foreach my $GRAPHIC_URL
(grep /(^daily(\d+).jpg)$/, @ALL_URLS) {
</pre><p>This will loop only on files whose names start with the
&ldquo;daily&rdquo; string, followed by any number of digits (\d+) and a .jpg
extension.
</p><p>The two techniques can be combined at will, and much more
sophisticated things are possible. If you know that the picture
name is equal to the page title plus the current date expressed in
the YYYYMMDD format, first extract the title:</p><pre     class="programlisting">
$HTML_FILE =~ m/&lt;TITLE&gt;([^&lt;]+)&lt;\/TITLE&gt;/;
my $TITLE = $1;
</pre><p>Then calculate the date:
<pre     class="programlisting">
my ($sec, $min, $hour, $day, $month, $year, @dummy)
= localtime(time);
$month++;      # months start at 0
$year += 1900; # Y2K-compliant, of course ;-)))
$TODAY = $year.$month.$day;
</pre>


And finally, filter on this:
<pre     class="programlisting">
foreach my $GRAPHIC_URL
(grep /(^$TITLE$TODAY.jpg)$/, &lt;@&gt;ALL_URLS) {
</pre>


</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb8420"></a>Extract and Display Only One Specific Section of
Text</h2></div></div><p>Now it starts to get really interesting. Customizing your
script to fetch only a certain section of the web page's text
usually requires more time and effort than any other operation
described here because it must be done almost from scratch on each
page and repeated if the page structure changes. If you have a slow
internet connection, or even a fast one but cannot slow down your
MP3 downloads or net games, you rapidly will recover the time spent
to prepare the script. You also will save quite a bit of money, if
you (like me) still pay per minute.</p><p>You have to open and study the HTML source of the original
web page to figure out which Perl regular expression filters out
all and only the text you need. The Perl LWP library already
provides methods to extract all the text out of the HTML code. If
you only want a plain ASCII version of the whole content, go for
them.</p><p>You may be tempted to let the LWP library extract the whole
text from the source, and then work on it, even when you only want
to extract some lines from the web page. I have found this method
to be much more difficult to manage in real cases, however. Of
course, the ASCII formatting makes the text immediately readable to
a human, but it also throws out all the HTML markup that is so
useful to tell the script which parts you want to save. The easiest
example of this false start is if you want to save or display all
and only the news titles, and they are marked in the source with
the &lt;H1&gt;&lt;/H1&gt; tags. Those markers are trivial to use in
a Perl regular expression, but once they are gone, it becomes much
harder to make the script recognize headlines.</p><p>To demonstrate the method on a real web page, let's try to
print inside our terminal all the press-release titles from the FSF
page at
<a href="http://www.fsf.org/press/press.html" target="_self">www.fsf.org/press/press.html</a>.
Pointing our script at this URL will save all its content inside
the $HTML_FILE variable. Now, let's apply to it the following
sequence of regular expressions (I suggest that you also look at
that page and at its source code with your browser to understand
everything going on):</p><pre     class="programlisting">
$HTML_FILE =~ s/.*&gt;Press Releases&lt;//gsmi;
$HTML_FILE =~ s/.*&lt;DL&gt;//gsmi;
$HTML_FILE =~ s/&lt;\/DL&gt;.*$//gsmi;
$HTML_FILE =~ s/&lt;dt&gt;([^&lt;]*)&lt;\/dt&gt;/-&gt; $1: /gi;
$HTML_FILE =~ s/&lt;dd&gt;&lt;a href=[^&gt;]*&gt;([^&lt;]*)&lt;\/a&gt;/
$1 /gsmi;
$HTML_FILE =~
s/\.\s+\([^\)]*\.\)&lt;\/dd&gt;/&lt;DD&gt;/gsmi;
$HTML_FILE =~ s/\s+/ /gsmi;
$HTML_FILE =~ s/&lt;DD&gt;/\n/gsmi;
</pre><p>The first three lines cut off everything before and after the
actual press-release list. The fourth one finds the date and strips
the HTML tags out of it. Regexes number five and six do the same
thing to the press-release subject. The last two eliminate
redundant white spaces and put new lines where needed. As of
December 14, 2001, the output at the shell prompt looks like this
(titles have been manually cut by me for better formatting):
<pre     class="programlisting">
-&gt; 3 December 2001: Stallman Receives Prestigious...
-&gt; 22 October 2001: FSF Announces Version 21 of the...
-&gt; 12 October 2001: Free Software Foundation
   Announces...
-&gt; 24 September 2001: Richard Stallman and
   Eben Moglen...
-&gt; 18 September 2001: FSF and FSMLabs come
   to agreement...
</pre>


The set of regular expressions above is not complete; for one
thing, it doesn't manage news with update sections. One also should
make it as independent as possible from extra spaces inside HTML
tags or changes in the color or size of some fonts. This regular
expression strips out all the font markup:
<pre     class="programlisting">
$HTML_FILES =~ s/&lt;font face="Verdana" size="3"&gt;
([^&lt;]+)&lt;\/font&gt;/$1/g;
</pre>


This performs the same task but works on any font type and
(positive) font size:
<pre     class="programlisting">
$HTML_FILES =~ s/&lt;font face="[^"]+"
size="\d+"&gt;
([^&lt;]+)&lt;\/font&gt;/$1/g;
</pre>


The example shown here, however, still is detailed enough to show
the principle, and again the one-time effort to write a custom set
for any given page really can save a lot of time.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb8840"></a>Make News Appear on Your Screen</h2></div></div><p>Once you have managed to extract the text you want and to
format it to your taste, there is no reason to limit yourself to a
manual use of the script, or to use it only at the console for that
matter. If you want to do something else and be informed by the
computer only when a new headline about Stallman appears, only
three more steps are needed.</p><p>First, put the script among your cron entries (<b  >man
cron</b> will tell you everything about this). After that,
add the following check to your Perl script:</p><pre     class="programlisting">
if ($HTML_FILE =~ m/Stallman/) {
       # INFORM ME!!!
}
</pre><p>This will do what you want only if the remaining text does
contain the Stallman string (or whatever else you want to know
about, of course).
</p><p>Next, fill the block with something like this:</p><pre     class="programlisting">
open (XMSG, "|/usr/bin/X11/xmessage -title \"NEWS!\"
-file -") || die;
print XMSG $HTML_FILE;
close XMSG;
</pre><p>This will open a UNIX pipe to the xmessage program, which
pops up a window with the title given with the corresponding switch
and containing the text of the file following the -file option. In
our case, &ldquo;-&rdquo; tells xmessage to get the text from the standard
input. As it is, the Perl script will wait to exit, so that you
close the xmessage window. This may or may not be what you want. In
the case of a cron script, it's much better to let it start
xmessage in the background on a temporary file and exit, like this:
<pre     class="programlisting">
open (XMSG, "&gt; /tmp/gee") || die;
print XMSG $HTML_FILE;
close XMSG;
exec "/usr/bin/X11/xmessage -title \"NEWS!\"
-file /tmp/gee&amp;";
</pre>


</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x1cb8c08"></a>Check to See If a Page Was Changed after a
Particular Date</h2></div></div><p>If you want to process the page only if the content was
changed since the last visit, or in the last two hours, you need
the Last-Modified HTTP header. It is already available, expressed
in seconds since January 1, 1970, in the third element of our
@HEADER array. Hence, if you want to do something only on pages
modified in the last two hours, start calculating what the time was
in that moment (always in the &ldquo;seconds since...&rdquo; unit):</p><pre     class="programlisting">
$NOW = time;
$TWO_HOURS_AGO = $NOW - (3600*2);
</pre><p>Then compare that time with the modification date of the web
page:
<pre     class="programlisting">
if ($HEADER[2] &gt; $TWO_HOURS_AGO) {
       # do whatever is needed
}
</pre>


</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x20b1068"></a>Add Dynamic Bookmarks to Your Window Manager
Menu</h2></div></div><p>This is one of the rare exceptions to the do-it-yourself rule
stated at the beginning: download WMHeadlines (see Resources),
install it, and then configure and modify to suit your taste. Out
of the box, it can fetch headlines from more than 120 sites and
place them in the root menu of Blackbox, WindowMaker, Enlightenment
and GNOME in such a way that you start your browser on the dynamic
menu voice you click on.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x20b1170"></a>Driving Your Browser from within a
Script</h2></div></div><p>Netscape can be given several commands from the prompt or any
script. Such commands will cause Netscape to start if it wasn't
already running or will load the requested URL in the current
window, or even in a new one. However, the commands to run change
depending on whether Netscape is already running. Look at the
nslaunch.pl script in the WMHeadlines distribution to figure out
how to check if Netscape is already running.</p><p>You also can drive Netscape to perform other actions from a
script: to print a page just as Netscape would do if driven
manually, make it load the page first:</p><pre     class="programlisting">
exec($NETSCAPE, '-noraise', '-remote',
"openURL($URL,new-window)");
</pre><p>Then save it as PostScript:
<pre     class="programlisting">
exec($NETSCAPE, '-noraise', '-remote',
"saveAs(/tmp/netscape.ps,
PostScript)");
</pre>


And finally, print it:
<pre     class="programlisting">
exec("mpage -PYOURPRINTER -1 /tmp/netscape.ps");
</pre>


Or, even add it to the bookmarks:
<pre     class="programlisting">
exec($NETSCAPE, '-noraise', '-remote',
"addBookmark($SOME_URL, $ITS_TITLE)");
</pre>


Konqueror, the KDE web browser, can be started simply by invoking
it in this way:
<pre     class="programlisting">
system("/usr/bin/konqueror $URL");
</pre>


Konqueror can be driven by scripts for many nonweb-related tasks,
such as copying files, starting applications and mounting devices.
Type <b  >kfmclient --commands</b> for more details.
</p><p>Galeon can be started in an almost equal way:</p><pre     class="programlisting">
system("/usr/bin/galeon $URL");
</pre><p>As explained in <span   class="emphasis"><em>A User's Guide to Galeon</em></span>
(see Resources), you also can decide whether Galeon (if already
running) should open the URL in a new tab:
<pre     class="programlisting">
system("/usr/bin/galeon -n $URL");
</pre>


in a new window:
<pre     class="programlisting">
system("/usr/bin/galeon -w $URL");
</pre>


or temporarily bookmark the $URL:
<pre     class="programlisting">
system("/usr/bin/galeon -t $URL");
</pre>


</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x20b17a0"></a>Smart Browsing</h2></div></div><p>The opposite approach, i.e., starting a generic mirroring or
image-fetching script from your browser, is possible in Konqueror
(or even KMail) during normal browsing. If you right click on a
link and select the &ldquo;Open with..&rdquo; option, it will let you enter
the path of the script to be used and add it to the choices next
time. This means you can prepare a mirror or fetch_images script
following the instructions given here and start it in the
background on any URL you wish with a couple of clicks.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x20b18a8"></a>Smart Mirroring and FTP</h2></div></div><p>The URL list contained in the @ALL_URLS array also can be
used to start mirroring or (parallel) FTP sessions. This can be
done entirely in Perl, using the many FTP and mirroring modules
available, or simply by collecting the URLs to be mirrored or
fetched by FTP, and leaving the actual work to wget or curl, as
explained in A. J. Chung's article, &ldquo;Downloading without a
Browser&rdquo; (see Resources).</p><p>If your favorite web portal chooses a different cool site
every day, and you want your PC to mirror it for you, just fetch
the URL as you would do for images, and then say in your
script:</p><pre     class="programlisting">
exec "wget -m -L -t 5 $COMPLETE_URL";
</pre><p>All the commands for parallel FTP and mirroring explained in
Chung's article can be started in this way from a Perl script,
having as arguments the URLs found by this one.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x20b1ab8"></a>Build Your Custom Web Portal</h2></div></div><p>Many of us have more than one favorite site and would like to
have them all in the same window. A general solution for this is to
extract the complete HTML body of each page in this way:</p><pre     class="programlisting">
$HTML_FILE = s/^.*&lt;body[^&gt;]*&gt;//i; # strips everything
                                  # before
$HTML_FILE = s/&lt;\/body[^&gt;]*&gt;.*$//i; # strips everything
                                    # after
</pre><p>and then print out an HTML table with each original page in
each box:
<pre     class="programlisting">
print&lt;&lt;END_TABLE;
....All HTML &lt;HEAD&gt; and &lt;BODY&gt; stuff here
&lt;TABLE&gt;
&lt;TR&gt;&lt;TD&gt;$HTML_FILE_1&lt;/TD&gt;&lt;/TR&gt;
&lt;TR&gt;&lt;TD&gt;$HTML_FILE_2&lt;/TD&gt;&lt;/TR&gt;
.........
&lt;/TABLE&gt;&lt;/BODY&gt;&lt;/HTML&gt;
END_TABLE
</pre>


Save the script output in $HOME/.myportal.html, set that file as
your starting page in your browser and enjoy! The complete script
will probably require quite some tweaking to clean up different
CSSes, fonts and so on, but you know how to do it by now, right?
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1bbf580.0x20b1cc8"></a>Conclusion</h2></div></div><p>We have barely scratched the surface of client-side web
scripting. Much more sophisticated tasks are possible, such as
dealing with cookies and password-protected sites, automatic form
submission, web searches with all the criteria you can think about,
scanning a whole web site and displaying the ten most-pointed-to
URLs in a histogram, and web-mail checking.</p><p>You only need some patience, Perl practice and a good
knowledge of the relevant modules to succeed. Good browsing!</p><p><a href="5623s1.html" target="_self">Resources</a></p></div></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../095/toc095.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>