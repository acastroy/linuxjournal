<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
At the Forge
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;How can you categorize documents using machine learning? It's simpler than&#10;you might think.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x241e580.0x2515ac0"></a>
At the Forge
</h1></div><div><h3 class="subtitle"><i>
Classifying Text
</i></h3></div><div><div class="author"><h3 class="author">
Reuven
 M. 
Lerner
</h3></div><div class="issuemoyr">Issue #276, April 2017</div></div><div><p>
How can you categorize documents using machine learning? It's simpler than
you might think.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x241e580.0x2516408"></a></h2></div></div><p>
In my last few articles, I've looked
at several ways one
can apply machine learning, both supervised and
unsupervised. This time, I want to bring your attention to a
surprisingly simple&mdash;but powerful and widespread&mdash;use of machine
learning, namely document classification.
</p><p>
You almost certainly have seen this technique used in day-to-day
life. Actually, you might not have seen it in action, but you 
certainly have benefited from it, in the form of an email spam filter.
You might remember that back in the earliest days of spam filters, you
needed to &ldquo;train&rdquo; your email program, so that it would know what your
real email looked like. Well, that was a machine-learning model in
action, being told what &ldquo;good&rdquo; documents looked like, as opposed to
&ldquo;bad&rdquo; documents. Of course, spam filters are far more sophisticated
than that nowadays, but as you'll see over the course of this
article, there are logical reasons why spammers include
innocent-seeming (and irrelevant to their business) words in the text
of their spam.
</p><p>
Text classification is a problem many businesses and
organizations have to deal with. Whether it's classifying legal
documents, medical records or tweets, machine learning can help you
look through lots of text, separating it into different groups.
</p><p>
Now, text classification requires a bit more sophistication than
working with purely numeric data. In particular, it requires that you
spend some time collecting and organizing data into a format that
a model can handle. Fortunately, Python's scikit-learn comes with a
number of tools that can get you there fairly easily.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x241e580.0x2516828"></a>
Organizing the Data</h2></div></div><p>
Many cases of text classification are supervised learning
problems&mdash;that is, you'll train the model, give it inputs (for example,
text documents) and the &ldquo;right&rdquo; output for each input (for
example, categories). In scikit-learn, the general template for supervised
learning is:

<pre     class="programlisting">
model = CLASS()
model.fit(X, y)
model.predict(new_data_X)
</pre>
</p><p>
<tt  >CLASS</tt> is one of the 30 or so Python classes that come with
scikit-learn, each of which implements a different type of
&ldquo;estimator&rdquo;&mdash;a machine-learning algorithm. Some estimators work best with
supervised classification problems, some work with supervised
regression problems, and still others work with clustering (that is,
unsupervised classification) problems. You often will be able to
choose from among several different estimators, but the general format
remains the same.
</p><p>
Once you have created an instance of your estimator, you then have to
train it. That's done using the &ldquo;fit&rdquo; method, to which you give X (the
inputs, as a two-dimensional NumPy array or a Pandas data frame) and y
(a one-dimensional NumPy array or a Pandas series). Once the model is
trained, you then can invoke its &ldquo;predict&rdquo; method, passing it
<tt  >new_data_X</tt>, another two-dimensional NumPy array or Pandas data frame.
The result is a NumPy array, listing the (numeric) categories into
which the inputs should be classified.
</p><p>
One of my favorite parts of using scikit-learn is the fact that so
much of it uses the same API. You almost always will be using some
combination of &ldquo;fit&rdquo; and &ldquo;predict&rdquo; on your model, no matter what kind
of model you're using.
</p><p>
As a general rule, machine-learning models require that inputs be
numeric. So, you turn category names into numbers, country names into
numbers, color names into numbers&mdash;basically, everything has to be a
number.
</p><p>
How, then, can you deal with textual data? It's true that bytes are
numbers, but that won't really help here; you want to deal with words and
sentences, not with individual characters.
</p><p>
The answer is to turn documents into a DTM&mdash;a &ldquo;document term
matrix&rdquo; in which the columns are the words that were used across the
documents, and the rows indicate whether (and how many times) that word
existed in the document.
</p><p>
For example, take the following three sentences:
</p><div class="itemizedlist"><ul type="disc"><li><p>
I'm hungry, and need to eat lunch.
</p></li><li><p>
Call me, and we'll go eat.
</p></li><li><p>
Do you need to eat?
</p></li></ul></div><p>
Let's turn the above into a DTM:

<pre     class="programlisting">
i'm hungry and need to eat lunch call me we'll go do you
1     1     1    1  1    1   1   0    0     0  0   0  0
0     0     1    0  0    1   0   1    1     1  1   0  0
0     0     0    1  1    1   0   0    0     0  0   1  1
</pre>
</p><p>
Now, this DTM certainly does a good job of summarizing which words
appeared in which documents. But with just three short sentences that
I constructed to have overlapping vocabulary, it's already starting
to get fairly wide. Imagine what would happen if you were to categorize
a large number of documents; the DTM would be massive! Moreover, the
DTM would mostly consist of zeros.
</p><p>
For this reason, a DTM usually is implemented as as &ldquo;sparse
matrix&rdquo;,
listing the coordinates of where the value is non-zero. That tends to
crunch down its size and, thus, processing time, quite a lot.
</p><p>
It's this DTM that you'll feed into your model. Because it's numeric,
the model can handle it&mdash;and, thus, can make predictions. Note that
you'll actually need to make two different DTMs: one for training the
model and another for handing it the text you want to categorize.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x241e580.0x25173d8"></a>
Creating a DTM</h2></div></div><p>
I decided to do a short experiment to see if I could create a
machine-learning model that knows how to differentiate between Python
and Ruby code. Not only do I have a fair amount of such code on my
computer, but the languages have similar vocabularies, and I was
wondering how accurately a model could actually do some
categorization.
</p><p>
So, the first task was to create a Python list of text, with a
parallel list of numeric categories. I did this using some list
comprehensions, as follows:

<pre     class="programlisting">
from glob import glob

# read Ruby files
ruby_files = [open(filename).read()
              for filename in glob("Programs/*.rb")]

# read Python files
python_files = [open(filename).read()
                for filename in glob("Programs/*.py")]

# all input files
input_text = ruby_files + python_files

# set up categories
input_text_categories = [0] * len(ruby_files) + [1] 
 &#8618;* len(python_files)
</pre>
</p><p>
After this code is run, I have a list (<tt  >input_text</tt>) of strings and
another list (<tt  >input_text_categories</tt>) of integers representing the two
categories into which these strings should be classified.
</p><p>
Now I have to turn this list of strings into a DTM. Fortunately,
scikit-learn comes with a number of &ldquo;feature extraction&rdquo; tools to make
this easy:

<pre     class="programlisting">
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
cv_dtm = cv.fit_transform(input_text)
</pre>
</p><p>
<tt  >CountVectorizer</tt> isn't the only way to create a DTM. Indeed, there are
different strategies you can use. Among other things, the granularity
of one word, rather than multiple words, might not be appropriate for
your text.
</p><p>
Notice that I use <tt  >cv.fit_transform</tt>. This both teaches
the vectorizer
the vocabulary (&ldquo;fit&rdquo;) and produces a DTM. I can create new DTMs
with this same vocabulary using just &ldquo;transform&rdquo;&mdash;and I will indeed
do this in a little bit, when I want to make a prediction or two.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x241e580.0x25179b0"></a>
Creating a Model</h2></div></div><p>
Now I have my inputs in a format that can be used to create a model!
You potentially can use a number of algorithms, but one
of the most common (and surprisingly accurate) is Naive Bayes.
Scikit-learn actually comes with several different versions of Naive
Bayes. The one that I use here is called MultinomialNB; it works
well with this sort of textual data. (But, of course, it's generally a
good idea to test your models and even tweak the inputs and
parameters to squeeze better results out of them.) Here's how I
create and then train my model:

<pre     class="programlisting">
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(input_text_dtm, input_text_categories)
</pre>
</p><p>
Notice that I've used &ldquo;fit&rdquo; twice now: once (on 
<tt  >CountVectorizer</tt>) to train and create a DTM from the input text and
then (on <tt  >MultinomialNB</tt>) to train the model based on that DTM.
</p><p>
The model is now all set! Now I can make some predictions. I'll
create some new documents:

<pre     class="programlisting">
docs_new = ['class Foo(object):\nprint "Hello, {}".format(self.name)\n',
            'x = [10, 20, 30]\n',
           '10.times do {|i| puts i}']
</pre>
</p><p>
The <tt  >docs_new</tt> variable contains three strings: the first is in Python,
the second could be either Ruby or Python, and the third is in Ruby.
</p><p>
To see how the model categorizes them, I'll first need to create a
DTM from these documents. Note that I'm going to reuse
<tt  >cv</tt>, the
<tt  >CountVectorizer</tt> object. However, I'm not going to
use the &ldquo;fit&rdquo;
method to train it with a new vocabulary. Rather, I'm going to use
&ldquo;transform&rdquo; to use the existing vocabulary with the new documents.
This will allow the model to compare the documents with the previous ones:

<pre     class="programlisting">
docs_new_dtm = cv.transform(docs_new)
</pre>
</p><p>
Now to make a prediction:

<pre     class="programlisting">
nb.predict(docs_new_dtm)
</pre>
</p><p>
The output is:

<pre     class="programlisting">
array([1, 1, 0])
</pre>
</p><p>
In other words, the first two documents are seen as Python, and the
third is seen as Ruby&mdash;not bad, for such a small training set. As you
can imagine, the more documents with which you train, the more
accurate your categorization is likely to be.
</p><p>
I tried a slight variation on the above code with the &ldquo;20
newsgroups&rdquo;
data set, using 20,000 postings from 20 different Usenet forum
postings. After using <tt  >CountVectorizer</tt> and
<tt  >MultinomialNB</tt> just as I did
here, the model was able to predict, with a surprisingly high degree
of accuracy, the most appropriate newsgroup for a variety of sentences
and paragraphs.
</p><p>
Of course, as with everything statistical&mdash;including machine
learning&mdash;the success rate never will be 100%. And indeed, you can
(and probably will want to) update the model, tuning the inputs and
the model's hyperparameters to try to improve it even more.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x241e580.0x250ec38"></a>
Summary</h2></div></div><p>
Document categorization is a practical application of machine learning
that a large number of organizations use&mdash;not just in spam filters, but
also for sorting through large volumes of text. As you can see,
setting up such a model isn't especially difficult, and scikit-learn
provides a large number of vectorizers, feature extraction tools and
estimators that you can use to create them.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x241e580.0x250ed40"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
I used Python (<a href="http://python.org" target="_self">python.org</a>) and the many parts of the SciPy
stack (NumPy, SciPy, Pandas, Matplotlib and scikit-learn) in this
article. All are available from PyPI (<a href="http://PyPI.python.org" target="_self">PyPI.python.org</a>) or from
ScyPy.org (<a href="http://scipy.org" target="_self">scipy.org</a>).
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x241e580.0x250f058"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Reuven M. Lerner, a longtime Web developer, offers training and consulting
services in Python, Git, PostgreSQL and data science. He has written
two programming ebooks (<span   class="emphasis"><em>Practice Makes Python</em></span> and
<span   class="emphasis"><em>Practice Makes
Regexp</em></span>) and publishes a free weekly newsletter for programmers,
at
<a href="http://lerner.co.il/newsletter" target="_self">lerner.co.il/newsletter</a>. Reuven tweets at
@reuvenmlerner and
lives in Modi'in, Israel, with his wife and three children.

</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../276/toc276.html">Issue Table of Contents</a>
    <a class="link3" href="../276/12166.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>