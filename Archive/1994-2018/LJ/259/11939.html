<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
At the Forge
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;A look at tools that push your server to its limits, testing&#10;loads before your users do.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x26eb580.0x27e2ac0"></a>
At the Forge
</h1></div><div><h3 class="subtitle"><i>
Performance Testing
</i></h3></div><div><div class="author"><h3 class="author">
Reuven
 M. 
Lerner
</h3></div><div class="issuemoyr">Issue #259, November 2015</div></div><div><p>
A look at tools that push your server to its limits, testing
loads before your users do.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x26eb580.0x27e33b0"></a></h2></div></div><p>
In my last few articles, I've considered Web application
performance in a number of different ways. What are the different
parts of a Web application? How might each be slow? What are the
different types of slowness for which you can (and should) check? How
much load can a given server (or collection of servers) handle?
</p><p>
So in this article, I survey several open-source tools you can use to
better identify how slow your Web applications might be running, in a
number of different ways. I should add that as the Web has grown in
size and scope, the number and types of ways you can check your
apps' speed also have become highly diverse, such that talking about
&ldquo;load testing&rdquo; or &ldquo;performance testing&rdquo; should beg the question,
&ldquo;Which kind of testing are you talking about?&rdquo;
</p><p>
I also should note that although I have tried to cover a number of the most
popular and best-known tools, there are dozens (and perhaps hundreds)
of additional tools that undoubtedly are useful. If I've neglected an
excellent tool that you think will help others, please feel free to
send me an e-mail or a Tweet; if readers suggest enough such tools, I'll
be happy to follow up with an additional column on the subject.
</p><p>
In my next article, I'll conclude this series by looking at tools and
techniques you can use to identify and solve client-side problems.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x26eb580.0x27e3778"></a>
Logfiles</h2></div></div><p>
One of the problems with load testing is that it often fails to catch
the problems you experience in the wild. For this reason, some of
the best tools that you have at your disposal are the logfiles on your
Web server and in your database. I'm a bit crazy about logfiles, in
that I enjoy having more information than I'll really need written in
there, just in case. Does that tend to make my applications perform a
bit worse and use up more disk space? Absolutely&mdash;but I've often
found that when users have problems, I'm able to understand
what happened better, and why it happened, thanks to the logfiles.
</p><p>
This is true in the case of application performance, as well. 
Regarding Ruby on Rails, for example, the logfile will tell you how long
each HTTP request took to be served, breaking that down further into
how much time was spent in the database and creating the HTML output
(&ldquo;view&rdquo;). This doesn't mean you can avoid digging deeper in many
cases, but it does allow you to look through the logfile and get a
basic sense of how long different queries are taking and understand
where you should focus your efforts.
</p><p>
In the case of databases, logfiles are also worth a huge amount. In
particular, you'll want to turn on your database's system that logs
queries that take longer than a certain threshold. MySQL has the
&ldquo;slow query log&rdquo;, and PostgreSQL has the
<tt  >log_min_duration_statement</tt>
configuration option. In the case of PostgreSQL, you can set
<tt  >log_min_duration_statement</tt> to be any number of ms you like,
enabling you to see, in the database's log, any query that takes
longer than (for example) 500 ms. I often set this number to be 200 or
300 ms when I first work on an application, and then reduce it as I
optimize the database, allowing me to find only those that are truly
taking a long time.
</p><p>
It's true that logfiles aren't quite part of load testing, but they
are an invaluable part of any analysis you might perform, in
production or even in your load tests. Indeed, when you run the load
tests, you'll need to understand and determine where the problems and
bottlenecks are. Being able to look at (and understand) the logs will
give you an edge in such analysis.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x26eb580.0x27e3b40"></a>
Apachebench</h2></div></div><p>
Once you've set up your logfiles, you are ready to begin some basic
load testing. Apachebench (ab) is one of the oldest load-testing
programs, coming with the source code for Apache httpd. It's not the
smartest or the most flexible, but ab is so easy to use that it's
almost certainly worth trying it for some basic tests.
</p><p>
ab takes a number of different options, but the most useful ones are
as follows:
</p><div class="itemizedlist"><ul type="disc"><li><p>
n: the total number of requests to send.
</p></li><li><p>
c: the number of requests to make concurrently.
</p></li><li><p>
i: use a <tt  >HEAD</tt> request instead of
<tt  >GET</tt>.
</p></li></ul></div><p>
Thus, if I want to start testing the load on a system, I can say:

<pre     class="programlisting">
ab -n 10000 -c 100 -i http://myserver.example.com/
</pre>
</p><p>
Note that if you're requesting the home page from an HTTP server, you
need to have the trailing slash, or ab will pretend it didn't see
a URL. As it runs, ab will produce output as it passes every 10%
milestone.
</p><p>
ab produces a table full of useful information when you run it. Here
are some parts that I got from running it against an admittedly small,
slow box:

<pre     class="programlisting">
Concurrency Level:      100
Time taken for tests:   36.938 seconds
Complete requests:      1000
Failed requests:        0
Total transferred:      1118000 bytes
HTML transferred:       0 bytes
Requests per second:    27.07 [#/sec] (mean)
Time per request:       3693.795 [ms] (mean)
Time per request:       36.938 [ms] (mean, across all concurrent
                         &#8618;requests)
Transfer rate:          29.56 [Kbytes/sec] received
</pre>
</p><p>
In other words, my piddling Web server was able to handle all 1,000
requests. But it was able to handle only 27 simultaneous requests,
meaning that about 75% of the concurrent requests sent to my box were
being ignored. It took 3.6 seconds, on average, to respond to each
request, which was also pretty sad and slow.
</p><p>
Just from these results, you can imagine that this box needs to be
running more copies of Apache (more processes or threads, depending on
the configuration), just to handle a larger number of incoming
requests. You also can imagine that I need to check it to see why
going to the home page of this site takes so long. Perhaps the
database hasn't been configured or optimized, or perhaps the home page
contains a huge amount of server-side code that could be optimized
away.
</p><p>
Now, it's tempting to raise the concurrency level
(<tt  >-c</tt> option) to
something really large, but if you're running a standard Linux box,
you'll find that your system quickly runs out of file
descriptors. In such cases, you either can reconfigure your system
or you can use Bees with Machine Guns, described below.
</p><p>
So, what's wrong with ab? Nothing in particular, other than the fact
that you're dealing with a simple HTTP request. True, using ab's
various options, you can pass an HTTP authentication string (user name
and password), set cookies (names and values), and even send
<tt  >POST</tt> and
<tt  >PUT</tt> requests whose inputs come from specified files. But if you're
looking to check the timing and performance of a set of user
actions, rather than a single URL request, ab isn't going to be
enough for you.
</p><p>
That said, given that the Web is stateless, and that you're likely to
be focusing on a few particular URLs that might be causing problems,
ab still might be sufficient for your needs, assuming that you can set
the authentication and cookies appropriately.
</p><p>
The above also fails to take into account how users perceive the speed
of your Web site. ab measured only the time it took to do all of the
server-side processing. Assuming that network latency is zero and that
JavaScript executes infinitely fast, you don't need to worry about
such things. But of course, this is the real world, which means
that client-side operations are no less important, as you'll see 
in my next article.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x26eb580.0x27e44e0"></a>
Bees with Machine Guns (BWMG)</h2></div></div><p>
If there's an award for best open-source project name, I think that it
must go to Bees with Machine Guns. Just saying this project's name
is almost guaranteed to get me to laugh out loud. And yet, it
does something very serious, in a very clever way. It allows you to
orchestrate a distributed denial-of-service (DDOS) attack against your
own servers.
</p><p>
The documentation for BWMG states this, but I'll add to the
warnings. This tool has the potential to be used for evil, in that
you can very easily set up a DDOS attack against any site you wish on
the Internet. I have to imagine that you'll get caught pretty quickly
if you do so, given that BWMG uses Amazon's EC2 cloud servers, which
ties the servers you use to your name and credit card. But even if you
won't get caught, you really shouldn't do this to a site that's not
your own.
</p><p>
In any event, Bees assumes that you have an account with Amazon. It's
written in Python, and as such, it can be installed with the
<tt  >pip</tt>
command:

<pre     class="programlisting">
pip install beeswithmachineguns
</pre>
</p><p>
The basic idea of Bees is that it fires up a (user-configurable)
number of EC2 machines. It then makes a number of HTTP requests,
similar to ab, from each of those machines. You then power down the
EC2 machines and get your results.
</p><p>
In order for this to work, you'll need at least one AWS keypair
(.pem file), which Bees will look for (by default) in your personal
~/.ssh directory. You can, of course, put it elsewhere. Bees relies
on Boto, a Python package that allows for automated work with AWS, so
you'll also need to define a ~/.boto file containing your AWS key and
secret (that is, user name and password).
</p><p>
Once you have the keypair and .boto files in place, you then can set
up your Bees test. I strongly suggest that you put this in a shell
script, thus ensuring that everything runs. You really don't want to
fire up a bunch of EC2 machines with the <tt  >bees up</tt> command, only to
discover the following month that you forgot to turn it off.
</p><p>
Bees uses the <tt  >bees</tt> command for everything, so every line of
your script will start with the word <tt  >bees</tt>. Some of the commands you
can issue include the following:
</p><div class="itemizedlist"><ul type="disc"><li><p>
<tt  >bees up</tt>: start up one or more EC2 servers. You can
specify the <tt  >-s</tt>
option to indicate the number of servers, the <tt  >-g</tt> option to indicate
the security group, and <tt  >-k</tt> to tell Bees where to look for your EC2
keypair file.
</p></li><li><p>
<tt  >bees attack</tt>: much like ab, you'll use the
<tt  >-n</tt> option to indicate the
number of requests you want to make and the <tt  >-c</tt> option to indicate
the level of concurrency.
</p></li><li><p>
<tt  >bees down</tt>: shut down all of the EC2 servers you started in this
session.
</p></li></ul></div><p>
So, if you want to do the same thing as before (that is, 1,000 requests),
but now divided across ten different servers, you would say:

<pre     class="programlisting">
bees up -s 10 -g beesgroup -k beespair
bees attack -n 100 -c 10 -u http://myserver.example.com/
bees down
</pre>
</p><p>
When you run Bees, the fun really begins. You get a verbose printout
indicating that bees are joining the swarm, that they're attacking
(bang bang!) and that they're done (&ldquo;offensive complete&rdquo;).
</p><p>
The report at the conclusion of this attack, similar to ab, will
indicate whether all of the HTTP requests were completed successfully,
how many requests the server could handle per second, and how long it
took to respond to various proportions of bees attacking.
</p><p>
Bees is a fantastic tool and can be used in at least two different
ways. First, you can use it to double-check that your server will
handle a particular load. For example, if you know that you're likely
to get 100,000 concurrent requests across your server farm, you
can use Bees to load that up on 1,000 different EC2 machines.
</p><p>
But another way to use Bees, or any load-testing tool, is to probe the
limits of your system&mdash;that is, to overwhelm your
server intentionally, to find out how many simultaneous requests it can take before
failing over. This simply might be to understand the limits of the
application's current architecture and implementation, or it might
provide you with insights into which parts of the application will
fail first, so that you can address those issues. Regardless,
in this scenario, you run your load-testing tool at repeatedly higher
levels of concurrency until the system breaks&mdash;at which point you try to
identify what broke, improve it and then overwhelm your server once again.
</p><p>
A possible alternative to Bees with Machine Guns, which I have played
with but never used in production, is Locust. Locust can run on a
single machine (like ab) or on multiple machines, in a distributed
fashion (like Bees). It's configured using Python and provides a
Web-based monitoring interface that allows you to see the current
progress and state of the requests. Locust uses Python objects, and
it allows you to write Python functions that execute HTTP requests and
then chain them together for complex interactions with a site.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x26eb580.0x2bdd498"></a>
Conclusion</h2></div></div><p>
If you're interested in testing your servers, there are several
high-quality, open-source tools at your disposal. Here, I
looked at several systems for exploring your server's limits, and also
how you can configure your database to log when it has problems. You're
likely going to want to use multiple tools to test your system, since
each exposes a different set of potential problems.
</p><p>
In my next article, I'll look at a variety of tools that let you identify
problems and slowness within the client side of your Web application.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x26eb580.0x2bdd5f8"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
Apachebench is part of the HTTP server project at the Apache Software
Foundation. That server is hosted at <a href="https://httpd.apache.org" target="_self">https://httpd.apache.org</a>. ab is
part of the source code package for Apache httpd.
</p><p>
Bees with Machine Guns is hosted on GitHub at
<a href="https://github.com/newsapps/beeswithmachineguns" target="_self">https://github.com/newsapps/beeswithmachineguns</a>. That page contains a
README with basic information about how to use the program. It
assumes familiarity with Amazon's EC2 service and a working set of keys.
</p><p>
Locust is hosted at <a href="http://locust.io" target="_self">locust.io</a>, where there also is extensive
documentation and examples. You will need to know Python, including
the creation of functions and classes, in order to use Locust.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x26eb580.0x2bdd9c0"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Reuven M. Lerner trains companies around the world in Python,
PostgreSQL, Git and Ruby. His ebook, &ldquo;Practice Makes Python&rdquo;,
contains
50 of his favorite exercises to sharpen your Python skills. Reuven
blogs regularly at <a href="http://blog.lerner.co.il" target="_self">blog.lerner.co.il</a> and tweets as
@reuvenmlerner. Reuven has a PhD in Learning Sciences from
Northwestern University, and he lives in Modi'in, Israel, with his wife
and three children.

</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../259/toc259.html">Issue Table of Contents</a>
    <a class="link3" href="../259/11939.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>