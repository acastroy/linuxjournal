<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Linux for Suits</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;How about a SETI project to build knowledge about how the Net is&#10;actually working?&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1efb580.0x1ff2ab0"></a>Linux for Suits</h1></div><div><h3 class="subtitle"><i>
The Search for Terrestrial Stupidity</i></h3></div><div><div class="author"><h3 class="author">
Doc
 
Searls
</h3></div><div class="issuemoyr">Issue #151, November 2006</div></div><div><p>
How about a SETI project to build knowledge about how the Net is
actually working?
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1efb580.0x1ff32f0"></a></h2></div></div><p>
If Net Neutrality is a good thing, shouldn't we be able to test for it?
Shouldn't everybody on the Net be in a position to see how things are
going for them? And, wouldn't it be useful to slice and dice data coming
in from all those nodes looking at Net performance from the edges?
</p><p>
Those were some of the questions raised by Tom Evslin
with &ldquo;Net Neutrality at Home: Distributed
Citizen Journalism against Net Discrimination&rdquo;&mdash;a recent luncheon talk at Harvard's Berkman Center for Internet and
Society (see the on-line Resources).  &ldquo;The goal of what I'm proposing
is to preserve in the United States an Internet that is equally open to
all applications regardless of who owns the network and regardless of
who the application owner is.&rdquo; He adds, &ldquo;I'm being US-centric here
because we suck as far as the rest of the world is concerned....The
problem is here.&rdquo; He also adds, &ldquo;Note that this doesn't mean that all
applications will function equally well on every network.&rdquo;
</p><p>
As an example, he gives his own Internet connection from rural Vermont,
which bounces off a satellite 25,000 miles over the equator and involves
unearthly latencies that make it nearly unsuitable for VoIP.
(Ironically, Tom is a VoIP pioneer.  At the time he sold his wholesale
VoIP company several years ago, it was the #7 carrier of voice data
traffic minutes in the world.) A neutral network would make a best
effort to deliver packets without discrimination in favor or against
its source, destination or content.  As Tom puts it, &ldquo;What we want to
see is each network equally open to applications, and not be more open
to the application of the network owner, particularly if the network
owner happens to be a monopoly.&rdquo;
</p><p>
This is where the line between technology and politics blurs.  Carriers
and other neutrality opponents say the Net itself has never been neutral
and has always allowed many kinds of discrimination.  They argue that
some applications&mdash;live teleconferencing, VoIP, streaming audio and
video, fault-tolerant grid computing and live remote surgery, for
example&mdash;would all benefit from QoS (Quality of Service) efforts that
are anything but &ldquo;neutral&rdquo;.  And they point out that discrimination of
all sorts&mdash;in provisioning asymmetries, multiple service levels,
selective port blockages and specific usage restrictions, to name a
few&mdash;have been common practices for nearly as long as ISPs have been in
business.  They'd like to retain the right to discriminate, or to improve
service any way they please, and to charge customers willing to pay for
the benefits.  They say they'd like to do that without government
interference (even though carriers inhabit what they call &ldquo;the
regulatory environment&rdquo;).
</p><p>
Meanwhile, neutrality advocates, such as Web
inventor Tim Berners-Lee, want laws to preserve the neutrality they say
has always been there and is threatened by carriers who loathe the
concept.  Plus, it's obvious (except to those employed by the
carriers&mdash;a population that sadly includes many lawmakers) that the carriers have
little if any interest in building open infrastructure that enlarges
business opportunity for everybody who builds on it.  They would, in
every case, rather capture markets than liberate them&mdash;even if they
would clearly have privileged first-mover and incumbent positions in
those liberated markets.  To them, &ldquo;free market&rdquo; means
&ldquo;your choice of silo&rdquo;.
</p><p>
Tom Evslin wants us to step back from the Net Neutrality fray and
resolve the issues through widespread knowledge that currently does not
exist.  Specifically, he'd like as many users as possible to test their
network connections for upload and download speeds, DNS speed, latency, jitter, blocking,
consistency and uptime, to name a few of many possible variables.
</p><p>
Yes, techies can run some of these tests at the command line (with ping,
traceroute and so on).  And today, any user can visit a site such as
Speakeasy.net or BroadbandReports.com to test upload and download speeds
in a browser.  (BroadbandReports even lets users compare results with
those of other customers of the same provider.) But Tom wants to go much
further than that.  He wants everybody to know what they're getting and
to pool data that will paint clear pictures of how individual networks
and network connections are performing over time.  He believes this will
not only provide useful information to both sides of the current debate,
but will allow everybody to observe and speak about the Internet with
far more understanding than individuals have today.
</p><p>
&ldquo;We don't want to look just for discrimination&rdquo;, Tom says.
&ldquo;We want the
result of running the tools to be sort of a consumers' report map of
Internet quality in general....The tools can measure both quality,
and then discrimination as an aspect of quality&mdash;if the discrimination
exists.  But even if there's no discrimination, we'll get useful data
over what kind of quality to expect where.&rdquo; He sees much to gain and
little to lose for everybody.  That is, if everybody&mdash;or at least a
very large number of users&mdash;participates.
</p><p>
A number of questions then follow:
</p><div class="orderedlist"><ol type="1"><li><p>
Exactly what kind of tests are we talking about?
</p></li><li><p>
How do we get users to participate on a large or massive scale?
</p></li><li><p>
If millions of users are running millions of tests or probes, how
do we prevent what we might call an &ldquo;insistence on service
attack&rdquo;?
</p></li><li><p>
How do we compile, edit and publish results?
</p></li></ol></div><p>
One answer to the first question came from a report about Dan Kaminsky
releasing details about a traceroute-like TCP-based fault probe, at the
Black Hat security conference in August 2006.  The report says:
</p><div class="blockquote"><blockquote class="blockquote"><p>
But unlike Traceroute, Kaminsky's software will be able to make
traffic appear as if it is coming from a particular carrier or is
being used for a certain type of application, like VoIP.  It will
also be able to identify where the traffic is being dropped and
could ultimately be used to finger service providers that are
treating some network traffic as second-class.
</p></blockquote></div><p>
Look for this capability amidst a free suite of tools called Paketto
Kieretsu Version 3.  Now, what else?
</p><p>
For guidance, Tom says the tools must be:
</p><div class="itemizedlist"><ul type="disc"><li><p>
Verified and calibrated.
</p></li><li><p>
Open source.
</p></li><li><p>
Perceived as safe.
</p></li><li><p>
Do non-destructive testing.
</p></li><li><p>
Return value to each user.
</p></li></ul></div><p>
One model he brought up was SETI@home.  Here thousands of individuals
contribute otherwise idle compute cycles to the Search for
Extraterrestrial Intelligence (SETI) Project.  That's a familiar model to
many of us, but not likely to attract users who aren't turned on by the
challenge of helping find ET.  So Tom is looking for something that is
SETI-like in distribution, but pays off with practical information for
the users.  The following are some questions from my notes at the luncheon:
</p><div class="itemizedlist"><ul type="disc"><li><p>
What if users actually knew how well the Net and its providers
worked for them, on both absolute and relative scales?
</p></li><li><p>
What if users could look at their connection speeds the same way
they look at speedometers in their cars? (Speakeasy.net does
something like this with its speed tests, but how about making the
test independent of any company?)
</p></li><li><p>
What if users could monitor packet loss or link quality with the
same ease as they check signal strength on a cell phone?
</p></li><li><p>
What if users could see by a simple indicator that the Wi-Fi
connection at the conference they're attending won't allow
outbound e-mail? (How about a list of port blockages and what they
mean?)
</p></li></ul></div><p>
The program would have to be widely distributed.  Tom says:
</p><div class="blockquote"><blockquote class="blockquote"><p>
We want
volunteers to run servers, to make sure various ports are open and to
test the geography&mdash;like for DNS propagation.  We need people who are
willing to have their servers be the proxy for testing the intentional
degrading of file sharing, SIP, P2P protocols and geography.  Because
geography is an issue.  Countries now have firewalls.  There might be
legitimate peering problems, or routing issues.  But we need to know when
actual blocking is going on.
</p></blockquote></div><p>
Where would these tools come from? The obvious answer is the Free Software
and Open Source communities.  &ldquo;It is absolutely essential that the tools
we get be open source&rdquo;, Tom says.  &ldquo;The tools themselves might be
prejudiced.  So you need to be able to see inside them to know that
they're not.  Second, we want to be able to bring to bear as much of
the technical community as cares to participate in the development and
elaboration of these tools.&rdquo; Tom thinks the application vendors should
contribute to the effort as well, because they could only benefit from
knowledge about the network.  Same goes for the carriers, who would
presumably like to gain bragging rights about how well they perform.
</p><p>
There needs to be organizations, perhaps on the SETI model, &ldquo;so the task
of information collection and analysis is distributed, as well as just
the initial probing&rdquo;, Tom says.  Also:
</p><div class="blockquote"><blockquote class="blockquote"><p>
We need people responsible for
verification....I'm very sensitive to that, because I've been
wondering whether my satellite ISP is blocking Skype.  I go on Skype and
the BlueSky forums and see one person saying, &ldquo;I ran this test that shows
absolutely that there's been blocking&rdquo;, and another person saying,
&ldquo;The
application is working for me but the test is failing&rdquo;....So it's not a
simple thing to know a test is actually working.  One
particular problem with Skype, and why Skype might not benefit from this
as well as other providers, is that Skype uses a proprietary
protocol....It's hard to imagine Skype contributing the particular
open-source tool that is necessary to debug the things that might happen to
the protocol that they're keeping secret.
</p></blockquote></div><p>
Then again, having these
kinds of tools looking at the network would help expose to users the
deficiencies, in an open world, of closed protocols, codecs and other
techniques for maintaining silos and keeping customers captive.
</p><p>
David Isenberg points out, &ldquo;Unless you have tools for each application,
that are app spec, you always run the risk that the test works fine in a
generic sense and then they've got this deep packet inspection that
finds the signature of the given application and blocks it.&rdquo; Tom
answers, &ldquo;So you'd like to have a tool where you could feed in the
signature of the application and test that generically, and at the same
time you'd like to test the protocols that they use.  SIP makes sense as an
example.&rdquo;
</p><p>
There is an editorial function too.  News needs to go out through
traditional media, as well as bloggers and other Net-based writers.  The
end result, in addition to keeping the carriers honest, is a far more
well-informed public.  Right now, most users know far less about how they
travel the Net than they do about how they travel the road system.
&ldquo;Latency&rdquo;, &ldquo;jitter&rdquo;, &ldquo;packet loss&rdquo;
and &ldquo;port blocking&rdquo; are no more
technical than &ldquo;speed&rdquo;, &ldquo;acceleration&rdquo;,
&ldquo;stopping distance&rdquo; or &ldquo;falling
rock zone&rdquo;.  Network performance knowledge should be common, not
professionally specialized.
</p><p>
The US has been falling behind the rest of the civilized world in
broadband speed and penetration.  Japan and Korea are committed to making
fiber-grade service available to their entire populations, and other
countries are similarly motivated to do what the US still cannot,
because most of its Internet service is provided by a duopoly that cares
far less about providing Net infrastructure than about delivering
high-definition TV.  Clearly, no help will come from lawmakers who still
think a highly regulated phone/cable duopoly is actually a &ldquo;free
market&rdquo;
for anything, much less the Internet.
</p><p>
David Isenberg wrote his landmark paper, &ldquo;The Rise of the Stupid
Network&rdquo; (see Resources) when he was still working for
(the original) AT&amp;T.  The paper observed that most of a network's value
is on its edges, rather than in its middle.  At the time AT&amp;T was busy
engineering intelligence into its switches and other mediating
technologies.  Meanwhile, Dr Isenberg said that the network should be
stupid (say, in the same way that the core and mantle of the Earth is
stupid).  It should be there to support the intelligence that resides on
it and takes advantage of it, but is not reducible to it.  In 1998, when
he wrote that essay, the Net was already well-established.  Yet the
thinking of the carriers was still deeply mired in the past.  Here's the
gist of the piece:
</p><div class="blockquote"><blockquote class="blockquote"><p>
A new network &ldquo;philosophy and architecture&rdquo;, is replacing the vision
of an Intelligent Network.  The vision is one in which the public
communications network would be engineered for &ldquo;always-on&rdquo; use, not
intermittence and scarcity.  It would be engineered for intelligence
at the end user's device, not in the network.  And the network would
be engineered simply to &ldquo;Deliver the Bits, Stupid&rdquo;, not for fancy
network routing or &ldquo;smart&rdquo; number translation.
</p><p>
Fundamentally, it would be a Stupid Network.
</p><p>
In the Stupid Network, the data would tell the network where it
needs to go.  (In contrast, in a circuit network, the network tells
the data where to go.) In a Stupid Network, the data on it would be
the boss.
</p></blockquote></div><p>
According to Craig Burton, the best geometric expression of the Net's
&ldquo;end-to-end&rdquo; design is a hollow sphere: a big three-dimensional zero.
Across it, every device is zero distance from every other device.  Yes,
there are real-world latency issues.  No path across the void is perfect.
But the ideal is clear: the connection between any two computers should
be as fast and straightforward as the connection between your keyboard
and your screen.  Value comes from getting stuff out of the way, not from
putting stuff in the way&mdash;especially if that stuff is designed to
improve performance selectively.  The middle is ideally a vacuum.  You can
improve on it only by making it more of a vacuum, not less.  And, like
gravity, it should work the same for everybody.
</p><p>
So I see the challenge here as a Search for Terrestrial Stupidity.  And I
think it's a challenge that goes directly to <i  >Linux Journal</i> readers and
their friends.  We are the kinds of people (and perhaps some of the
actual people) who imagined and built the open Internet that the whole
world is coming to enjoy.  And we're the ones who are in the best
position to save it from those who want to make it gravy for television.
</p><p>
In other words, we need smart people to save the Stupid Network.  I look
forward to seeing how we do it.
</p><p><span   class="bold"><b>Resources for this article:</b></span>
<a href="../151/9261.html" target="_self">/article/9261</a>.
</p></div></div>
<div class="authorblurb"><p>
Doc Searls is Senior Editor of <i  >Linux Journal</i>.  He is also a Visiting
Scholar at the University of California at Santa Barbara and a Fellow
with the Berkman Center for Internet and Society at Harvard University.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../151/toc151.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>