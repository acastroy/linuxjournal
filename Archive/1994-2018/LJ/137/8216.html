<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
A System Monitoring Dashboard</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;This simple set of shell scripts keeps&#10;you informed about disks that are filling up,&#10;CPU-hog processes and problems with the Web and&#10;mail servers.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1a9d580.0x1b94ab0"></a>
A System Monitoring Dashboard</h1></div><div><div class="author"><h3 class="author">
John
 
Ouellette
</h3></div><div class="issuemoyr">Issue #137, September 2005</div></div><div><p>
This simple set of shell scripts keeps
you informed about disks that are filling up,
CPU-hog processes and problems with the Web and
mail servers.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a9d580.0x1b952f0"></a></h2></div></div><p>
For about a year, my company had been struggling to roll out a monitoring
solution. False positives and
inaccurate after-hours pages were affecting morale and
wasting system administrators' time. After speaking to
some colleagues about what we really need to monitor,
it came down to a few things:
</p><div class="itemizedlist"><ul type="disc"><li><p>
Web servers&mdash;by way of HTTP, not only physical servers.
</p></li><li><p>
Disk space.
</p></li><li><p>
SMTP servers' availability&mdash;by way of SMTP, not only physical
servers.
</p></li><li><p>
A history of these events to diagnose and pinpoint problems.
</p></li></ul></div><p>
This article explains the process I developed and how I set up disk, Web and SMTP
monitoring both quickly and simply. Keeping the monitoring process
simple meant that all the tools used should be available on a recent Linux
distribution and should not use advanced protocols, such as SNMP or
database technology. As a result, all of my scripts use the Bash shell, basic
HTML, some modest Perl and the wget utility. All of these monitoring
scripts share the same general skeleton and installation steps, and they are available
from the <i  >Linux Journal</i> FTP site (see the on-line Resources).
</p><p>
Installing the scripts involves several steps. Start by copying the
script to a Web server and making it world-executable with
<tt  >chmod</tt>. Then, create a directory under the root of
your Web server where the script can write its logs and history. I used webmon for
monitor_web.sh. The other scripts are similar: I used smtpmon for
monitor_smtp.sh and stats for monitor_stats.pl. monitor_disk.sh is
different from the others because it is the only one installed locally
on each server you want to monitor. </p><p>
Next, schedule the scripts in cron. You can run each script with any
user capable of running wget, df -k and top. The user also needs to have the
ability to write to the script's home. I suggest creating a local user
called monitor and scheduling these through that user's crontab.
Finally, install wget if it is not already present on your Linux distribution.
</p><p>
My first challenge was to monitor the Web servers by way of HTTP, so I
chose wget as the engine and scripted around it. The resulting script
is monitor_web.sh. For those unfamiliar with wget, its author describes
it as &ldquo;a free software package for retrieving files using HTTP, HTTPS
and FTP, the most widely used Internet protocols&rdquo; (see Resources).
</p><p>
After installation, monitor_web.sh requires only two choices for the
user, e-mail recipient and URLs to monitor, which are labeled clearly. The
URLs must conform to HTTP standards and return a valid <tt  >http
200 OK</tt> string to work. They can be HTTP or HTTPS, as wget and monitor_web.sh
support both. Once installed and run the first time, the user is able to
get to http://localhost/webmon/webmon.html and view the URLs, the last
result and the history in a Web browser, as they all are links.
</p><p>
Now, let's break down the script; see monitor_web.sh, available on the
<i  >LJ</i> FTP site. First, I set all the
variables for system utilities and the wget program. These may change on
your system. Next, we make sure we are on the network. This ensures that if
the server monitoring the URLs goes off-line, a massive number of alerts
are not queued up by Sendmail until the server is back on-line.
</p><p>
As I loop through all the URLs, I have wget connect two times with a
timeout of five seconds. I do this twice to reduce false positives. If
the Web site is down, the script generates an e-mail message for the recipient and
updates the Web page. Mail also is sent when the site is back up. The
script sends only one message, so we don't overwhelm the recipient. This
is achieved with the following code:

<pre     class="programlisting">

wget $URL  2&gt;&gt;   $WLOG

if (( $? != 0 ));then

  echo \&lt;A HREF\="$URL"\&gt;$URL\&lt;\/A\&gt; is down\

          $RTAG $EF.\
          $LINK Last Result $LTAG  &gt;&gt; $WPAGE

  if [[ ! -a down.$ENV ]];then

        touch /tmp/down.$ENV
        mail_alert down

  else
         echo Alert already sent for $ENV \
         - waiting | tee -a $WLOG
  fi

fi

</pre>
</p><p>
I have included the HTML for green and red text
in the script, if you choose not to use graphics.
Again, the full script is available from the <i  >Linux Journal</i> FTP site.
</p><div       class="mediaobject"><a href="8216f1.large.jpg"><img src="8216f1.jpg"></a><div class="caption"><p>
Figure 1. monitor_web.sh in action. Run the script from
cron to regenerate this page as often as needed.
</p></div></div><p>
With the Web servers taken care of, it was time to tackle disk
monitoring. True to our keep-it-simple philosophy, I chose to
create a script that would run from cron and alert my team based on the
output of <tt  >df -k</tt>. The result was monitor_disk.sh. The first real block
of code in the script sets up the filesystems list:

<pre     class="programlisting">

FILESYSTEMS=$(mount | grep -iv proc |\
grep -iv cdrom | awk'{print $3}')

</pre>
</p><p>
I ignore proc and am careful not to report on the CD-ROM, should my
teammates put a disk in the drive. The script then compares the value of
Use% to two values, THRESHOLD_MAX and THRESHOLD_WARN. If Use% exceeds
either one, the script generates an e-mail to the appropriate recipient,
RECIPIENT_MAX or RECIPIENT_WARN. Notice that I made sure the Use% value for each
filesystem is interpreted as an integer with this line:

<pre     class="programlisting">

typeset -i  UTILIZED=$(df -k $FS | tail -1 | \
awk '{print $5}' | cut -d"%" -f1)

</pre>
</p><p>
A mailing list was set up with my team members' e-mail addresses and the
e-mail address of the on-call person to receive the critical e-mails and
pages. You may need to do the same with your mail
server software, or you simply can use your group or pager
as both addresses.
</p><p>
Because our filesystems tend to be large, about 72GB&ndash;140GB, I have set
critical alerts to 95%, so we still have some time to address issues
when alerted. You can set your own threshold with the THRESHOLD_MAX
and THRESHOLD_WARN variables. Also, our database servers run some
disk-intensive jobs and can generate large amounts of archive log files,
so I figured every 15 minutes is a good frequency at which to monitor. For
servers with less active filesystems, once an hour is enough.
</p><p>
Our third script, monitor_smtp.sh, monitors our SMTP servers'
ability to send mail. It is similar to the first two scripts
and simply was a matter of finding a way to connect directly to a user-defined
SMTP server so I could loop through a server list and send a
piece of mail. This is where smtp.pl comes in. It is a Perl script
(Listing 1) that uses the NET::SMTP module to send mail
to an SMTP address. Most recent distributions have this module
installed already (see the Do I Have That Perl Module Installed sidebar). Monitor_smtp.sh updates the defined
Web page based on the success of
the transmission carried out by smtp.pl. No attempt is made to alert our
group, as this is a trouble-shooting tool and ironically cannot rely on
SMTP to send mail if a server is down. Future versions of monitor_smtp.sh
may include a round-robin feature and be able to send an alert through
a known working SMTP server.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a9d580.0x1b960b0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 1. smtp.pl tests outgoing mail through the
SMTP server.</b></p><pre     class="programlisting">

#!/usr/bin/perl -w
#
# Title  :   smtp.pl
# Author :   John_Ouellette@yahoo.com
# Files  :   smtp.pl
# Pupose :   Send email through SMTP server
#            Called from monitor_smtp.sh
#
# Submit as

use Net::SMTP;

my $rcpt   = $ARGV[2] || 'mygroup@somewhere';
my $sender = $ARGV[1] || 'root@host01';
my $host   = $ARGV[0];


#Start Script

my $smtp =Net::SMTP-&gt;new($host, Debug =&gt; 1);
my $input="test msg for server $host";

$smtp-&gt;mail("$sender");
$smtp-&gt;to("$rcpt");
$smtp-&gt;data();
$smtp-&gt;datasend("To: $rcpt\n") ;
$smtp-&gt;datasend("From: $sender\n") ;
$smtp-&gt;datasend("Subject: $host test\n") ;
$smtp-&gt;datasend("$input");
$smtp-&gt;dataend();
$smtp-&gt;quit;

</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a9d580.0x1b96268"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Do I Have That Perl Module Installed?
</b></p><p>
An easy way to check whether you have any Perl module
installed is by issuing this from the command line:

<pre     class="programlisting">
$ perl -e "use Net::SMTP";
</pre>
</p><p>
If nothing prints, you have that module installed. If you're missing
the module, you get an error that looks like this:

<pre     class="programlisting">

$perl -e "use Net::OTHER";
Can't locate Net/OTHER.pm in @INC (@INC contains:
/usr/lib/perl5/5.8.3/i386-linux-thread-multi /usr/lib/perl5/5.8.3
/usr/lib/perl5/vendor_perl/5.8.0 /usr/lib/perl5/vendor_perl .) at -e line
1.
BEGIN failed--compilation aborted at -e line 1.

</pre>
</p><p>
This error indicates a lack of the module in question.
</p></div><p>
Finally, we come to our stats script,
monitor_stats.pl. This script logs in to each host and
runs the commands:

<pre     class="programlisting">

df -k
swapon -s
top -n 1 | head -n 20
hostname
uptime

</pre>
</p><p>
It then displays the results in a
browser (Figure 2) and saves the result in a log,
again sorted by date on the filesystem. It serves
as a simple dashboard to give quick stats on
each server.
</p><p>
The benefit of this monitoring design is threefold:
</p><div class="orderedlist"><ol type="1"><li><p>
We have a history of CPU, disk and swap usage, and we easily can pinpoint
where problems may have occurred.
</p></li><li><p>
Tedious typing to extract this information for each
server is reduced. This comes in handy before leaving work to resolve potential
problems before getting paged at night.
</p></li><li><p>
Management quickly can see how well we're doing.
</p></li></ol></div><p>
We are using the insecure rsh protocol in this script to show
you how to get this set up quickly, but we recommend that you use SSH
with properly distributed keys to gain security.
</p><div       class="mediaobject"><a href="8216f2.large.jpg"><img src="8216f2.jpg"></a><div class="caption"><p>
Figure 2. monitor_stats.pl in Action
</p></div></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a9d580.0x1b96b58"></a>
Conclusion</h2></div></div><p>
With the use of this new system monitoring dashboard, my team's
productivity has increased and and its confidence
in monitoring has soared, because we no longer are
wasting time chasing down false positives. A history
of system performance has been a real time saver
in diagnosing problems. Finally, easy installation
allows users with basic skills to conquer a complex
system administration problem in one business day.
</p><p><span   class="bold"><b>Resources for this article:</b></span> <a href="../137/8269.html" target="_self">/article/8269</a>.
</p></div></div>
<div class="authorblurb"><p>
John Ouellette is a system administrator with nine years of experience in
Microsoft Windows NT
and UNIX. He believes the command line is king and loves chicken
parmigiana. He can be reached at <a href="mailto:john_ouellette@yahoo.com">john_ouellette@yahoo.com</a>.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../137/toc137.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>