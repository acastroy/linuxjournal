<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>A Web Crawler in Perl</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Here's how spiders search the Web collecting information for you.&#10;    "><meta name="keywords" content="Perl, WWW, search"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1aff580.0x1bf6ab0"></a>A Web Crawler in Perl</h1></div><div><div class="author"><h3 class="author">Mike Thomas</h3></div><div class="issuemoyr">Issue #40, August 1997</div></div><div><p>
    Here's how spiders search the Web collecting information for you.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1aff580.0x1bf74a8"></a></h2></div></div><p>Web-crawling robots, or spiders, have a
certain mystique among Internet users. We all use search engines
like Lycos and Infoseek to find resources on the Internet, and
these engines use spiders to gather the information they present to
us. Very few of us, however, actually use a spider program
directly.
</p><p>Spiders are network applications which traverse the Web,
accumulating statistics about the content found. So how does a web
spider work? The algorithm is straightforward:</p><div class="orderedlist"><ol type="1"><li><p>Create a queue of URLs to be searched beginning
with one or more known URLs.</p></li><li><p>Pull a URL out of the queue and fetch the Hypertext
Markup Language (HTML) page which can be found at that
location.</p></li><li><p>Scan the HTML page looking for new-found
hyperlinks. Add the URLs for any hyperlinks found to the URL
queue.</p></li><li><p>If there are URLs left in the queue, go to step
2.</p></li></ol></div><p><a href="2200l1.html" target="_self">Listing 1</a> is a program,
spider.pl, which implements the above algorithm in Perl. This
program should run on any Linux system with Perl version 4 or
higher installed. Note that all code mentioned in this article
assumes Perl is installed in /usr/bin/Perl. These scripts are
available for download on my web page at
http://www.javanet.com/~thomas/.</p><p>To run the spider at the shell prompt use the command:</p><pre     class="programlisting">
spider.pl &lt;starting-URL&lt;search-phrase&gt;
</pre><p>The spider will commence the search. The starting URL must be
fully specified, or it may not parse correctly. The spider searches
the initial page and all its descendant pages for the given search
phrase. The URL of any page with a match is printed. To print a
list of URLs from the SSC site containing the phrase
&ldquo;<i  >Linux Journal</i>&rdquo;, type:
<pre     class="programlisting">
spider.pl http://www.ssc.com/ "<i  >Linux Journal</i>"
</pre>


The Perl variable <b  >$DEBUG</b>, defined in the first
few lines of spider.pl, is used to control the amount of output the
spider produces. <b  >$DEBUG</b> can range from 0
(matching URLs are printed) to 2 (status of the program and dumps
of internal data structures are output).
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1aff580.0x1bf7c90"></a>Interaction with the Internet</h2></div></div><p>The most interesting thing about the spider program is the
fact that it is a network program. The subroutine
<b  >get_http()</b> encapsulates all the network
programming required to implement a spider; it does the &ldquo;fetch&rdquo;
alluded to in step 2 of the above algorithm. This subroutine opens
a socket to a server and uses the HTTP protocol to retrieve a page.
If the server has a port number appended to it, this port is used
to establish the connection; otherwise, the well-known port 80 is
used.</p><p>Once a connection to the remote machine has been established,
<b  >get_http()</b> sends a string such as:</p><pre     class="programlisting">
GET /index.html HTTP/1.0
</pre><p>This string is followed by two newline characters. This is a
snippet of the Hypertext Transport Protocol (HTTP), the protocol on
which the Web is based. This request asks the web server to which
we are connected to send the contents of the file /index.html to
us. <b  >get_http()</b> then reads the socket until an
end of file is encountered. Since HTTP is a connectionless
protocol, this is the extent of the conversation. We submit a
request, the web server sends a response and the connection is
terminated.
</p><p>The response from the web server consists of a header, as
specified by the HTTP standard, and the HTML-tagged text making up
the page. These two parts of the response are separated by a blank
line. Running the spider at debug level 2 will display the HTTP
headers for you as a page is fetched. The following is a typical
response from a web server.</p><pre     class="programlisting">
HTTP/1.0 200 OK
Date: Tue, 11 Feb 1997 21:54:05 GMT
Server: Apache/1.0.5
Content-type: text/html
Content-length: 79
Last-modified: Fri, 22 Nov 1996 10:11:48 GMT
&lt;HTML&gt;&lt;TITLE&gt;My Web Page&lt;/TITLE&gt;
&lt;BODY&gt;
This is my web page.
&lt;/BODY&gt;
&lt;/HTML&gt;
</pre><p>The spider program checks the <b  >Content-type</b>
field in the HTTP header as it arrives. If the content is of any
MIME type other than text/html or text/plain, the download is
aborted. This avoids the time-consuming download of things like .Z
and .tar.gz files, which we don't wish to search. While most sites
use the FTP protocol to transfer this type of file, more and more
sites are using HTTP.
</p><p>There is a hardware dependency in
<b  >get_http()</b> that you should be aware of if you
are running Linux on a SPARC or Alpha. When building the network
addresses for the socket, the Perl <b  >pack()</b>
routine is used to encode integer data. The line:</p><pre     class="programlisting">
$sockaddr="S n a4 x8";
</pre><p>is suitable only for 32-bit CPUs. To get around this, see
Mike Mull's article &ldquo;Perl and Sockets&rdquo; in <i  >LJ</i>
Issue 35.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1aff580.0x1bf8318"></a>The URL Queue</h2></div></div><p>Once the spider has downloaded the HTML source for a web
page, we can scan it for text matching the search phrase and notify
the user if we find a match.</p><p>We can also find any hypertext links embedded in the page and
use them as a starting point for a further search. This is exactly
what the spider program does; it scans the HTML content for anchor
tags of the form <b  >&lt;A HREF="url"&gt;</b> and adds
any links it finds to its queue of URLs.</p><p>A hyperlink in an HTML page can be in one of several forms.
Some of these must be combined with the URL of the page in which
they're embedded to get a complete URL. This is done by the
<b  >fqURL()</b> function. It combines the URL of the
current page and the URL of a hyperlink found in that page to
produce a complete URL for the hyperlink.</p><p>For example, here are some links which might be found in a
fictitious web page at http://www.ddd.com/clients/index.html,
together with the resulting URL produced by
<b  >fqURL()</b>.</p><div class="table"><a name="N0x1aff580.0x1bf8630"></a><p class="title"><b></b></p><table     summary="" border="1"><colgroup><col><col></colgroup><tbody><tr><td><p><span   class="bold"><b>URL in Anchor
Tag</b></span></p></td><td><p><span   class="bold"><b>Resulting URL</b></span></p></td></tr><tr><td><p>http://www.eee.org/index.html</p></td><td><p>http://www.eee.org/index.html</p></td></tr><tr><td><p>att.html</p></td><td><p>http://www.ddd.com/clients/att.html</p></td></tr><tr><td><p>/att.html</p></td><td><p>http://www.ddd.com/att.html</p></td></tr></tbody></table></div><p>As these examples show, the spider can handle both a
fully-specified URL and a URL with only a document name. When only
a document name is given, it can be either a fully qualified path
or a relative path. In addition, the spider can handle URLs with
port numbers embedded, e.g.,
http://www.ddd.com:1234/index.html.</p><p>One function not implemented in <b  >fqURL()</b> is
the stripping of back-references (<b  >../</b>) from a
URL. Ideally, the URL /test/.../index.html is translated to
/index.html, and we know that both point to the same
document.</p><p>Once we have a fully-specified URL for a hyperlink, we can
add it to our queue of URLs to be scanned. One concern that crops
up is how to limit our search to a given subset of the Internet. An
unrestricted search would end up downloading a good portion of the
world-wide Internet content&mdash;not something we want to do to our
compadres with whom we share network bandwidth. The approach
spider.pl takes is to discard any URL that does not have the same
host name as the beginning URL; thus, the spider is limited to a
single host. We could also extend the program to specify a set of
legal hosts, allowing a small group of servers to be searched for
content.</p><p>Another issue that arises when handling the links we've found
is how to prevent the spider from going in circles. Circular
hyperlinks are very common on the Web. For example, page A has a
link to page B, and page B has a link back to page A. If we point
our spider at page A, it finds the link to B and checks it out. On
B it finds a link to A and checks it out. This loop continues
indefinitely. The easiest way to avoid getting trapped in a loop is
to keep track of where the spider has been and ensure that it
doesn't return. Step 2 in the algorithm shown at the beginning of
this article suggests that we &ldquo;pull a URL out of our queue&rdquo; and
visit it. The spider program doesn't remove the URL from the queue.
Instead, it marks that URL as having been scanned. If the spider
later finds a hyperlink to this URL, it can ignore it, knowing it
has already visited the page. Our URL queue holds both visited and
unvisited URLs.</p><p>The set of pages the spider has visited will grow steadily,
and the set of pages it has yet to visit can grow and and shrink
quickly, depending on the number of hyperlinks found in each page.
If a large site is to be traversed you may need to store the URL
queue in a database, rather than in memory as we've done here. The
associative array that holds the URL queue,
<b  >%URLqueue</b>, could easily be linked to a GDBM
database with the Perl 4 functions <b  >dbmopen()</b> and
<b  >dbmclose()</b> or Perl 5 functions
<b  >tie()</b> and <b  >untie()</b>.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1aff580.0x1ff1590"></a>Responsible Use</h2></div></div><p>Note that you should not unleash this beast on the Internet
at large, not only because of the bandwidth it consumes, but also
because of Internet conventions. The document request the spider
sends is a one line <b  >GET</b> request. To strictly
follow the HTTP protocol, it should also include
<b  >User-Agent</b> and <b  >From</b> fields,
giving the remote server the opportunity to deny our request and/or
collect statistics.</p><p>This program also ignores the &ldquo;robots.txt&rdquo; convention that
is used by administrators to deny access to robots. The file
/robots.txt should be checked before any further scanning of a
host. This file indicates if scanning from a robot is welcome and
declares any subdirectories that are off-limits. A robots.txt file
that excludes scanning of only 2 directories looks like
this:</p><pre     class="programlisting">
Useuagent: *
Disallow: /tmp/
Disallow: /cgi-bin/
</pre><p>A file that prohibits all scanning on a particular web server
looks like this:
<pre     class="programlisting">
User-agent: *
Disallow: /
</pre>


Robots like our spider can place a heavy load on a web server, and
we don't wish to use it on servers that have been declared
off-limits to robots by their administrators
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1aff580.0x1ff1900"></a>Application of the spider.pl Script</h2></div></div><p>How might we use the spider program, other than as a
curiosity? One use for the program would be as a replacement for
one of the web site index and query programs like Harvest
(http://harvest.cs.colorado.edu/Harvest/) or Excite for Web Servers
(http://www.excite.com/navigate/prodinfo.html). These programs are
large and complicated. They often provide the functionality of the
Perl spider program, a means of archiving the text retrieved and a
CGI query engine to run against the resulting database. Ongoing
maintenance is required, since the query engine runs against the
database rather than against the actual site content; therefore,
the database must be regenerated whenever a change is made to the
content of the site.</p><p>Some search engines, such as Excite for Web Servers, cannot
index the content at a remote site. These engines build their
database from the files which make up the web site, rather than
from data retrieved across a network. If you had two web sites
whose content was to appear in a single search application, these
tools would not be appropriate. Furthermore, the Linux version of
Excite for Web Servers is still in the &ldquo;coming soon&rdquo;
stage.</p><p><a href="2200l2.html" target="_self">Listing 2</a> and
<a href="2200l3.html" target="_self">Listing 3</a> show a simple CGI search
engine that is implemented using the spider.pl program. Listing 2
is an HTML form which calls spiderfind.cgi to process its input.
Listing 3 is spiderfind.cgi. It first uses Brigitte Jellinek's
library to move the data entered in the form into an associative
array. It then calls the spider.pl program using the Perl
<b  >system()</b> function and passes the form data as
parameters. Finally, it converts the output from spider.pl into a
series of HTML links. The user's browser will display a list of
hyperlinked URLs in which the search text was found. Note that the
name of the host to search is specified by a hidden field in the
HTML document. There are better and more security-conscious ways
for two Perl programs to interact than through a Perl
<b  >system()</b> call, but I wanted to use an unmodified
copy of spider.pl for this demonstration.</p><p>This script doesn't provide the complete functionality of the
packages mentioned above, and it won't perform as well. Since we're
doing the search against web server documents across the Net, we
don't have the advantage of index files; therefore, the search will
be slower and more processor-intensive. However, this script is
easy to install and easier to maintain than those engines.</p><p>Another application that could be built using the spider.pl
program is a broken link scanner for the Web. The HTTP response we
showed previously began with the line &ldquo;<b  >HTTP/1.0 200
OK</b>&rdquo;, indicating the request could be fulfilled. If we
tried to hit a URL with a non-existent document, we would get the
line &ldquo;<b  >HTTP/1.0 404 Not found</b>&rdquo; instead. We
could use this as an indication that the document does not exist
and print the URL which referenced this page.</p><p>The modifications to the spider program needed to accomplish
this are minor. Every time a hyperlink's URL is added to the URL
queue, we also record the URL of the document in which we found the
hyperlink. Then, when the spider checks out the hyperlink and
receives a &ldquo;<b  >404 Not found</b>&rdquo; response, it
outputs the URL of the referring page.</p></div></div>
<div class="authorblurb"><p>
        <div       class="mediaobject"><img src="2200aa.jpg"></div>

      <span   class="bold"><b>Mike Thomas</b></span>
      is an Internet application developer
      working for a consulting firm in Saskatchewan, Canada. Mike lives
      in Massachusetts and uses two Linux systems to telecommute 2000
      miles to his job and to Graduate School at the University of
      Regina. He can be reached by e-mail at thomas@javanet.com.</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../040/toc040.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>