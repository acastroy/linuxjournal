<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Stuttgart Neural Network Simulator</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Exploring connectionism and machine learning with SNNS.&#10;    "><meta name="keywords" content="science, engineering, applications"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1f4a580.0x2041ab0"></a>Stuttgart Neural Network Simulator</h1></div><div><div class="author"><h3 class="author">Ed Petron</h3></div><div class="issuemoyr">Issue #63, July 1999</div></div><div><p>
    Exploring connectionism and machine learning with SNNS.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f4a580.0x2042500"></a></h2></div></div><p>Conventional algorithmic solution methods
require the application of unambiguous definitions and procedures.
This requirement makes them impractical or unsuitable for
applications such as image or sound recognition where logical rules
do not exist or are difficult to determine. These methods are also
unsuitable when the input data may be incomplete or distorted.
Neural networks provide an alternative to algorithmic methods.
Their design and operation is loosely modeled after the networks of
neurons connected by synapses found in the human brain and other
biological systems. One can also find neural networks referred to
as artificial neural networks or artificial neural systems. Another
designation that is used is &ldquo;connectionism&rdquo;, since it deals with
information processing carried out by interconnected networks of
primitive computational cells. The purpose of this article is to
introduce the reader to neural networks in general and to the use
of the Stuttgart Neural Network Simulator (SNNS).
</p><p>In order to understand the significance of the ability of a
neural network to handle data which is less than perfect, we will
preview at this time a simple character-recognition application and
demonstrate it later. We will develop a neural network that can
classify a 7x5 rectangular matrix representation of alphabetic
characters.</p><p><a href="3142f1.jpg" target="_self"><span   class="bold"><b>Figure 1. Ideal
Letter &ldquo;A&rdquo;</b></span></a></p><p>In addition to being able to classify the representation in
Figure 1 as the letter &ldquo;A&rdquo;, we would like to be able to do the
same with Figure 2 even though it has an extra pixel filled in. As
a typical programmer can see, conventional algorithmic solution
methods would not be easy to apply to this situation.</p><p><a href="3142f2.jpg" target="_self"><span   class="bold"><b>Figure 2.
Blurred &ldquo;A&rdquo;</b></span></a></p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f4a580.0x20428c8"></a>Neural Network Principles of Operation</h2></div></div><p>A neural network consists of an interconnected network of
simple processing elements (PEs). Each PE is one of three
types:</p><div class="itemizedlist"><ul type="disc"><li><p>Input: these receive input data to be
processed.</p></li><li><p>Output: these output the processed data.</p></li><li><p>Hidden: these PEs, if used in the given
application, provide intermediate processing support.</p></li></ul></div><p>Connections exist between selected pairs of the PEs. These
connections carry the output in the form of a real number from one
element to all of the elements to which it is connected. Each
connection is also assigned a numeric weight.
</p><p>PEs operate in discrete time steps <b  >t</b>. The
operation of a PE is best thought of as a two-stage function. The
first stage calculates what is called the net input, which is the
weighted sum of its input elements and the weights assigned to the
corresponding input connections. For the <b  >j</b>th PE,
the value at time <b  >t</b> is calculated as
follows:</p><div       class="mediaobject"><a href="3142e1.large.jpg"><img src="3142e1.jpg"></a></div><p>where <b  >j</b> identifies the PE in question,
<b  >xi(t)</b> is the input at time <b  >t</b>
from the PE identified by <b  >i</b>, and
<b  >wi,j</b> are the weights assigned to the connections
from <b  >i</b> to <b  >j</b>.</p><p>The second stage is the application of some output function,
called the activation, to the weighted sum. This function can be
one of any number of functions, and the choice of which one to use
is dependent on the application. A commonly used one is known as
the logistic function:</p><div       class="mediaobject"><img src="3142e2.jpg"></div><p>which always takes on values between 0 and 1. Generally, the
activation <b  >Aj</b> for the <b  >j</b>th PE
at time <b  >t+1</b> is dependent on the value for the
weighted sum <b  >netj</b> for time
<b  >t</b>:</p><div       class="mediaobject"><img src="3142e3.jpg"></div><p>In some applications, the activation for step
<b  >t+1</b> may also be dependent on the activation from
the previous step <b  >t</b>. In this case, the
activation would be specified as follows:</p><div       class="mediaobject"><img src="3142e4.jpg"></div><p>In order to help the reader make sense out of the above
discussion, the illustration in Figure 3 shows an example
network.</p><p><a href="3142f3.jpg" target="_self"><span   class="bold"><b>Figure 3.
Sample Neural Network</b></span></a></p><p>This network has input PEs (numbered 1, 2 and 3), output PEs
(numbered 8 and 9) and hidden PEs (numbered 4, 5, 6 and 7). Looking
at PE number 4, you can see it has input from PEs 1, 2 and 3. The
activation for PE number 4 then becomes:</p><div       class="mediaobject"><a href="3142e5.large.jpg"><img src="3142e5.jpg"></a></div><p>If the activation function is the logistic function as
described above, the activation for PE number 4 then becomes</p><div       class="mediaobject"><a href="3142e6.large.jpg"><img src="3142e6.jpg"></a></div><p>A typical application of this type of network would involve
recognizing an input pattern as being an element of a finite set.
For example, in a character-classification application, we would
want to recognize each input pattern as one of the characters A
through Z. In this case, our network would have one output PE for
each of the letters A through Z. Patterns to be classified would be
input through the input PEs and, ideally, only one of the output
units would be activated with a 1. The other output PEs would
activate with 0. In the case of distorted input data, we should
pick the output with the largest activation as the network's best
guess.</p><p>The computing system just described obviously differs
dramatically from a conventional one in that it lacks an array of
memory cells containing instructions and data. Instead, its
calculating abilities are contained in the relative magnitudes of
the weights between the connections. The method by which these
weights are derived is the subject of the next section.</p><p>In addition to being able to handle incomplete or distorted
data, a neural network is inherently parallel. As such, a neural
network can easily be made to take advantage of parallel hardware
platforms such as Linux Beowulf clusters or other types of parallel
processing hardware. Another important characteristic is fault
tolerance. Because of its distributed structure, some of the
processing elements in a neural network can fail without making the
entire application fail.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f4a580.0x203a5f8"></a>Training</h2></div></div><p>In contrast to conventional computer systems which are
programmed to perform a specific function, a neural network is
trained. Training involves presenting the network with a series of
inputs and the outputs expected in each case. The errors between
the expected and actual outputs are used to adjust the weights so
that the error is reduced. This process is typically repeated until
the error is zero or very small.</p><p>Training methods vary greatly from one application to the
next, and there is no single universal solution. As an example, we
turn to a general discussion of what is known as gradient descent
or steepest descent. Here, the error for each training pattern is
quantified as:</p><div       class="mediaobject"><img src="3142e7.jpg"></div><p>where <b  >Ep</b> is the error for pattern
<b  >p</b> and
<div       class="mediaobject"><img src="3142d1.jpg"></div>
 <b  >pk</b> is the difference between the expected and
actual outputs for pattern p and output PE <b  >k</b>.
The error <b  >Ep</b> can also be thought of as a scalar
valued function of the set of connection weights in the
network:</p><div       class="mediaobject"><img src="3142e9.jpg"></div><p>With this in mind, minimizing <b  >Ep</b> involves
moving the weights in the direction of the negative gradient
<b  >-</b>
<div       class="mediaobject"><img src="3142d2.jpg"></div>
<b  >Ep</b>. The weight change for a selected weight
<b  >wi</b> can be calculated as:</p><div       class="mediaobject"><img src="3142e8.jpg"></div><p>where <b  >nu</b> is some constant between 0 and 1.
The function F is typically chaotic and highly nonlinear. That
being the case, the actual gradient component may be a very large
value that may cause us to overshoot the solution. The constant
<b  >nu</b> can be used to suppress this.</p><p>I have included the above discussion mostly for the benefit
of readers with some basic knowledge of multi-variable calculus.
For others, it is really only important to know that, through an
iterative process, a neural network is adapted to fit its problem
domain. For this reason, neural networks are considered to be part
of a larger class of computing systems known as adaptive
systems.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f4a580.0x203b1a8"></a>Prototyping with SNNS</h2></div></div><p>Although the actual implementation and operation of a neural
network can be accomplished on a variety of platforms ranging from
dedicated special-purpose analog circuits to massively parallel
computers, the most practical is a conventional workstation.
Simulation programs could be written from scratch, but the designer
can save much time by using one of several available neural network
prototyping tools. One such tool is the Stuttgart Neural Network
Simulator (SNNS).</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f4a580.0x203b2b0"></a>A Demonstration of SNNS</h2></div></div><p>The easiest way to get started with SNNS is to experiment
with one of the example networks that comes with the distribution.
One of these is a solution to the character recognition problem
discussed at the beginning of this article.</p><p>Upon invoking SNNS, the manager (Figure 4) and banner (Figure
5) windows appear.</p><p><a href="3142f4.jpg" target="_self"><span   class="bold"><b>Figure 4. SNNS
Manager</b></span></a></p><p><a href="3142f5.jpg" target="_self"><span   class="bold"><b>Figure 5. SNNS
Banner</b></span></a></p><p>Selecting the file option from the manager window presents
the user with a file selection box (Figure 6).</p><p><a href="3142f6.jpg" target="_self"><span   class="bold"><b>Figure 6. SNNS
File Selection</b></span></a></p><p>The file selector uses extensions of .net, .pat, etc. to
filter the file names in the selected directory. We will load the
letters_untrained.net file, since we want to see the training
process in action. We will also load the letters.cfg configuration
file and the letters.pat file which contains training
patterns.</p><p>After the files are loaded, selecting the display option in
the manager window will present the user with a graphical display
of the untrained network (Figure 7).</p><p><a href="3142f7.jpg" target="_self"><span   class="bold"><b>Figure 7. SNNS
Untrained Network Display</b></span></a></p><p>This window shows the input units on the left, a layer of
hidden units in the center and the output units on the right. The
output units are labeled with the letters A through Z to indicate
the classification made by the network. Note that in the Figure 7
display, no connections are showing yet because the network is
untrained at this point.</p><p><a href="3142f8.jpg" target="_self"><span   class="bold"><b>Figure 8. SNNS
Control Window</b></span></a></p><p>Selecting the control option from the manager window presents
the user with the control window (Figure 8). Training and testing
are directed from the control window. Training basically involves
the iterative process of inputting a training vector, measuring the
error between the expected output and the actual output, and
adjusting the weights to reduce the error. This is done with each
training pattern, and the entire process is repeated until the
error is reduced to an acceptable level. The button marked ALL
repeats the weight adjustment process for the entire training data
set for the number of times entered in the CYCLES window. Progress
of the training can be monitored using the graph window (Figure
9).</p><p><a href="3142f9.jpg" target="_self"><span   class="bold"><b>Figure 9. SNNS
Graph Window</b></span></a></p><p>In the graph window, the horizontal axis shows the number of
training cycles and the vertical axis displays the error.</p><p><a href="3142f10.jpg" target="_self"><span   class="bold"><b>Figure 10.
Partially Trained Network</b></span></a></p><p>Figure 10 shows a partially trained network. In contrast to
the untrained network in Figure 7, this picture shows some
connections forming. This picture was taken at the same time as
Figure 9. After enough training repetitions, we get a network
similar to that shown in Figure 11. Notice that the trained
network, when the letter A is input on the left, the corresponding
output unit on the right is activated with a 1.</p><p><a href="3142f11.jpg" target="_self"><span   class="bold"><b>Figure 11.
Trained Network</b></span></a></p><p>As a quick check to see if the network can generalize, a
modified version of the training data set is tested with one of the
dots in the A matrix set to zero instead of one, while an erroneous
dot is set to one instead of zero. Figure 12 demonstrates that the
distorted version of the letter A is still recognized.</p><p><a href="3142f12.jpg" target="_self"><span   class="bold"><b>Figure 12.
Test of Distorted A</b></span></a></p><p>As pointed out in the section on training, many different
methods can be used to adjust the connection weights as part of the
training process. The proper one to use depends on the application
and is often determined experimentally. SNNS can apply one of many
possible training algorithms automatically. The training algorithm
can be selected from the drop-down menu connected to the control
window.</p><p>SNNS reads network definition and configuration data from
ASCII text files, which can be created and edited with any text
editor. They can also be created by invoking the
<span   class="bold"><b>bignet</b></span> option from the manager
window. <span   class="bold"><b>bignet</b></span> enables the
creation of a network by filling in general characteristics on a
form. Refinements can be made by manually editing the data files
with a text editor or by using other options within SNNS. Training
and test data files are also plain ASCII text files.</p><p>Other notable features of SNNS include:</p><div class="itemizedlist"><ul type="disc"><li><p>Remote Procedure Call (RPC)-based facility for use
with workstation clusters</p></li><li><p>A tool called
<span   class="bold"><b>snns2c</b></span> for converting a network
definition into a C subroutine</p></li><li><p>Tools for both 2-D and 3-D visualization of
networks</p></li></ul></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f4a580.0x203c490"></a>Getting and Installing SNNS</h2></div></div><p>Complete instructions for getting and installing SNNS can be
found at
<a href="http://www.informatik.uni-stuttgart.de/ipvr/bv/projekte/snns/obtain.html" target="_self">http://www.informatik.uni-stuttgart.de/ipvr/bv/projekte/snns/obtain.html.</a>
Prepackaged binaries are also available as part of the Debian
distribution. Check
<a href="http://www.debian.org" target="_self">http://www.debian.org/</a>
for the Debian FTP site nearest you.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f4a580.0x203c648"></a>Summary</h2></div></div><p>Neural networks enable the solution of problems for which
there is no known algorithm or defining set of logical rules. They
are based loosely on neurobiological processes and are thus capable
of decisions which are intuitively obvious to humans but extremely
difficult to solve using conventional computer processes. They are
also less brittle and prone to failure than conventional systems
due to their distributed nature. Their inherent parallelism
provides opportunities for highly optimized performance using
parallel hardware. The Stuttgart Neural Network Simulator (SNNS) is
a powerful tool for prototyping computer systems based on neural
network models.</p><p><a href="3142s1.html" target="_self">Resources</a>

</p></div></div>
<div class="authorblurb"><p>
        <div       class="mediaobject"><img src="3142aa.jpg"></div>

      <span   class="bold"><b>Ed Petron</b></span>
      is a computer consultant interested in
      heterogeneous computing. He holds a Bachelor of Music from Indiana
      University and a Bachelor of Science in computer science from
      Chapman College. His home page, The Technical and Network Computing
      Home Page at
      <a href="http://www.leba.net/~epetron" target="_self">www.leba.net/~epetron</a>,
      is dedicated to Linux, the X Window System, heterogeneous computing
      and free software. Ed can be reached via e-mail at
      epetron@leba.net.</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../063/toc063.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>