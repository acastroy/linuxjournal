<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Natural Selection in a Linux Universe</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Astronomers at the University of Texas-Austin are using the&#10;    ideas of Charles Darwin to learn about the interior of white&#10;    dwarf stars&mdash;using a minimal parallel Linux cluster tailored&#10;    specifically to their application.&#10;    "><meta name="keywords" content="network, programming"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1033580.0x112aab0"></a>Natural Selection in a Linux Universe</h1></div><div><div class="authorgroup"><div class="author"><h3 class="author">Travis Metcalfe</h3></div><div class="author"><h3 class="author">Ed Nather</h3></div><div class="issuemoyr">Issue #65, September 1999</div></div></div><div><p>
    Astronomers at the University of Texas-Austin are using the
    ideas of Charles Darwin to learn about the interior of white
    dwarf stars&mdash;using a minimal parallel Linux cluster tailored
    specifically to their application.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1033580.0x112b870"></a></h2></div></div><p>Astronomers worry about how stars work.
Our current models describe stars as huge, hot gasballs, bloated
and made luminous by a fusion furnace deep inside that burns
hydrogen into helium and releases energy in the process. A kind of
internal thermostat keeps them stable, so our planet enjoys a
comfortable environment in its orbit around our star, the sun. In
about 6,000 million years or so, all available fuel will be burned
up, and as the fuel gets low, the sun will bloat, then shrink until
it is 100 times smaller than it is now, becoming a white dwarf
star. Written inside, in the ashes of the furnace, will be its
nuclear history.
</p><p>We have pieced together this story by looking at many
different stars, which last much longer than we do, but we cannot
see inside any of them. Stars are very luminous yet thoroughly
opaque. Geologists have built up a detailed picture of the earth's
interior, even though it is opaque too; they do this by watching as
compression waves from earthquakes rattle around inside and make
their way back to the surface: seismology. By a very fortunate
circumstance, we have found that some white dwarf stars vibrate
internally with something akin to earthquakes, all the time. Their
rapid changes in brightness tell us what is going on inside:
asteroseismology.</p><p>To take advantage of this cosmic bonanza, we build computer
models of the stars, with adjustable parameters that reflect,
one-to-one, the physics going on inside. We must &ldquo;vibrate&rdquo; our
model and tweak its parameters until the model behaves like a real
star. We then believe that the parameters in our model tell us
about the physics inside the white dwarf star. We can then start to
read the history written there.</p><div       class="mediaobject"><img src="3255f1.jpg"><div class="caption"><p>
Figure 1. Evolving Penguin
</p></div></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1033580.0x112bbe0"></a>Evolving Darwin</h2></div></div><p>The basic idea is nifty, but the practice is a bit
complicated. The models have many parameters, not all independent
of one another, and we are not completely sure we have all the
physics right. To make sure the set of model parameters we use is
the best fit to the observed behavior and the only reasonable one,
we have to explore a very large, multi-dimensional parameter
space&mdash;far too large and complex to examine in exhaustive detail.
No existing computer could handle it. There is a way though: we
populate our huge parameter space at random with models whose
parameters cover the whole shebang. Then we breed them together,
preferentially allowing those which fit the observations fairly
well to survive into later generations. This survival of the
fittest is done with a &ldquo;genetic algorithm&rdquo; that mimics, in a
crude but effective way, the process of natural selection proposed
by Charles Darwin.</p><p><a href="3255s1.html" target="_self">Genetic Algorithms</a></p><p>Even using this trickery, a <span   class="emphasis"><em>lot</em></span> of
computing is required, so we built a massive parallel system to cut
the runtime to hours instead of weeks. Most of the model
calculations are done in floating-point arithmetic, so we measure
performance in flops, the number of floating-point operations per
second. Our assembled system, called a metacomputer, is capable of
more than two gigaflops&mdash;2,000 million floating-point operations
per second&mdash;not bad for an assembly of Linux boxes.</p><p>Our strategy in designing this system was minimalist; keep
each computer node as cheap and simple as possible, consistent with
doing our job and getting the maximum amount of computing for the
buck. Our budget is fairly limited. CPU cost is not a linear
function of speed, so you pay a great deal more per megaflop for
the fastest CPU on the market. Older CPUs are cheaper, but require
more boxes and supporting electronics to house them for the same
final performance. We watched the price drops with avid interest
and jumped just after the 300MHz Intel P-II dropped below $300. We
could afford a good master control computer and 32 computing nodes
with our $22,000 budget.</p><div       class="mediaobject"><img src="3255f2.jpg"><div class="caption"><p>
Figure 2. Computer Lab
</p></div></div><p>Some time after we settled on the design, we became aware of
the existence of Beowulf machines through an article in
<i  >Linux Journal</i> (see Resources)&mdash;also parallel
systems running Linux, but with faster Ethernet connections and
more storage than our problem requires. They are much more general
purpose than the system we built, so they can handle many problems
ours cannot. They cost more too.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1033580.0x112c108"></a>Cheap Hardware, Free Software</h2></div></div><p>Our master computer is a Pentium-II 333 MHz system with 128MB
SDRAM and two 8.4GB hard disks. It has two NE-2000 compatible
network cards, each connected to 16 nodes using a simple 10base-2
coaxial network. We assembled the nodes from components obtained at
a local discount computer outlet. Each has a Pentium-II 300 MHz
processor housed in an ATX tower case with 32MB SDRAM and an
NE-2000-compatible network card. We used inexpensive 32KB EPROMs,
programmed with a BP Microsystems EP-1 using a ROM image from Gero
Kuhlmann's Netboot package, allowing each node to boot from the
network.</p><p><a href="3255t1.html" target="_self">Table 1.</a></p><p>Configuring the software was not much more complicated than
setting up a diskless Linux box (see Robert Nemkin's Diskless Linux
Mini-HOWTO). The main difference was that we minimized network
traffic by giving each node an identical, independent file system
rather than mounting a shared network file system. Since the nodes
had no hard disks, we needed to create a self-contained file system
that could be mounted in a modest fraction of the 32MB RAM.</p><p>To create this root file system, we used Tom Fawcett's YARD
package
(<a href="http://www.croftj.net/~fawcett/yard" target="_self">http://www.croftj.net/~fawcett/yard/</a>).
Although Yard was designed to make rescue disks, it was also
well-suited for our needs. We included in the file system a
trimmed-down, execute-only distribution of the PVM (parallel
virtual machine) software developed at Oak Ridge National
Laboratory
(<a href="http://www.epm.ornl.gov/pvm" target="_self">http://www.epm.ornl.gov/pvm/</a>).
PVM allows code to be run on the system in parallel by starting a
daemon on each node and using a library of message-passing routines
to coordinate the tasks from the master computer.</p><p>We configured the master computer to be a BOOTP/TFTP server,
allowing each node to download the boot image&mdash;essentially a
concatenation of a kernel image and a compressed root file system.
We used the Netboot package
(<a href="http://www.han.de/~gero/netboot" target="_self">http://www.han.de/~gero/netboot/</a>)
to create this boot image using the root file system created by
YARD and a small kernel image custom-compiled for the nodes.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1033580.0x112c4d0"></a>How It Works</h2></div></div><p>With the master computer up and running, we turned on each
node one at a time. By default, the BIOS in each node tries to boot
from the network first. It finds the boot ROM on the Ethernet card,
and the ROM image broadcasts a BOOTP request over the network. When
the server receives the request, it identifies the associated
hardware address, assigns a corresponding IP address, and allows
the requesting node to download the boot image. The node loads the
kernel image into memory, creates an 8MB initial RAM disk, mounts
the root file system, and executes an
<span   class="bold"><b>rc</b></span> script which starts essential
services and daemons.</p><p>Once all nodes are up, we log in to the server and start the
PVM daemon. An rhosts file in the home directory on each of the
nodes allows the server to start up the daemons. We can then run in
parallel any executable file that uses the PVM library routines and
is included in the root file system.</p><p>For our problem, the executable residing on the nodes
involves building and vibrating a white dwarf model and comparing
the resulting theoretical frequencies to those observed in a real
white dwarf. A genetic algorithm running on the master computer is
concerned with sending sets of model parameters to each node and
modifying the parameter sets based on the results. We tested the
performance of the finished metacomputer with the same genetic
algorithm master program as our white dwarf project, but with a
less computationally intensive node program. The code ran 29.5
times faster using all 32 nodes than it did using a single node.
Our tests also indicate that node programs with a higher
computation to communication ratio yield an even better efficiency.
We expect the white dwarf code to be approximately ten times more
computationally intensive than our test problem.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1033580.0x112c6e0"></a>Stumbling Blocks</h2></div></div><p>After more than three months without incident, one of the
nodes abruptly died. As it turned out, the power supply had gone
bad, frying the motherboard and the CPU fan in the process. The
processor overheated, shut itself off, and triggered an alarm. We
now keep a few spare CPU fans and power supplies on hand. This is
the only real problem we have had with the system, and it was
easily diagnosed and fixed.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1033580.0x112c7e8"></a>Conclusions</h2></div></div><p>The availability of open-source software like Linux, PVM,
Netboot and YARD made this project possible. We would never have
considered doing it this way if we'd had to use a substantial
fraction of our limited budget to buy software as well as hardware
and if we'd been unable to modify it to suit our needs once we had
it. This is an aspect of the Open Source movement we have not seen
discussed before&mdash;the ability to try something new and show it can
work, before investing a lot of money in the fond hope that
everything will turn out fine.</p><p><a href="3255s2.html" target="_self">Resources</a></p></div></div>
<div class="authorblurb"><p>
          <div       class="mediaobject"><img src="3255aa.jpg"></div>
          <span   class="bold"><b>Travis Metcalfe</b></span> (<a href="mailto:travis@astro.as.utexas.edu">travis@astro.as.utexas.edu</a>) is a
          doctoral student in astronomy at the University of Texas-Austin.
          When not sitting in front of a
          computer, he can usually be found tilting at windmills. His use of
          Linux since 1994 has reportedly made him more unruly.
        </p><p>
          <div       class="mediaobject"><img src="3255aa2.jpg"></div>
          <span   class="bold"><b>Ed Nather</b></span> (<a href="mailto:nather@astro.as.utexas.edu">nather@astro.as.utexas.edu</a>) is a
          professor of astronomy who publishes science fiction in several
          astronomical journals, under an alias (R. E. Nather). In his spare
          time, he installs and re-installs the newest Linux distributions,
          hoping to find the perfect one. He also believes in the tooth fairy.
        </p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../065/toc065.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>