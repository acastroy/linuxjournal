<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Automating Remote Backups
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Most home users don't think about backups till their disks crash.&#10;With a little bit of upfront work, you can automate your home backups&#10;and sleep easy at night.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x18ec580.0x19e3ac0"></a>
Automating Remote Backups
</h1></div><div><div class="author"><h3 class="author">
Michael
 J. 
Hammel
</h3></div><div class="issuemoyr">Issue #194, June 2010</div></div><div><p>
Most home users don't think about backups till their disks crash.
With a little bit of upfront work, you can automate your home backups
and sleep easy at night.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e42a8"></a></h2></div></div><p>
Linux users are a diverse group because of the wide swath of choices they
have at their fingertips. But, whether they choose Ubuntu, Fedora or
Debian, or KDE, GNOME or Xfce, they all have one thing in common: a lot of
data. Losing data through hard disk failure or simply by overwriting is
something all users must face at some point. Yet, these are not the only
reasons to do backups. With a little planning, backups are not nearly as
hard as they might seem.
</p><p>
Hard disk prices have dropped to the point where USB storage easily
replaces the need for off-line tape storage for the average user. Pushing
your data nightly to external USBs, either local or remote, is a fairly
inexpensive and simple process that should be part of every user's personal
system administration.
</p><p>
In this article, I describe a process for selecting files to back up,
introduce the tools you'll need to perform your backups and provide simple
scripts for customizing and automating the process. I have used these
processes and scripts both at home and at work for a number of years. No
special administrative skills are required, although knowledge of
SSH will be useful.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e4568"></a>
Why We Backup</h2></div></div><p>
Before proceeding, you should ask yourself the purpose of the backup.
There are two reasons to perform a backup. The first is to recover a
recent copy of a file due to some catastrophic event. This type of
recovery makes use of <span   class="emphasis"><em>full backups</em></span>, where only a single
copy of each file is maintained in the backup archive. Each file that is
copied to the archive replaces the previous version in the archive.
</p><p>
This form of backup is especially useful if you partition your system with
a root partition for the distribution of choice (Fedora, Ubuntu and so
forth) and a user partition for user data (/home). With this
configuration, distribution updates are done with re-installs instead of
upgrades. Installing major distributions has become fairly easy
and nearly unattended. Re-installing using a separate root partition allows
you to wipe clean the old installation without touching user data. All
that is required is to merge your administrative file backups&mdash;a process
made easier with tools like meld (a visual diff tool).
</p><p>
The second reason to perform a backup is to recover a previous version of a
file. This type of recovery requires the backup archive to maintain an
initial full backup and subsequent incremental changes. Recovery of a
particular version of a file requires knowing the time between when the
full backup was performed and the date of the version of the file that is
desired in order to rebuild the file at that point.
Figure 1 shows the full/incremental backup concepts graphically.
</p><div       class="mediaobject"><a href="10679f1.large.jpg"><img src="10679f1.jpg"></a><div class="caption"><p>
Figure 1. Full backups replace archive contents. Incremental backups
extend archives with time-based file changes.
</p></div></div><p>
Incremental backups will use up disk space on the archive faster than full
backups. Most home users will be more concerned with dealing with
catastrophic failure than retrieving previous versions of a file. Because
of this, home users will prefer full backups without incremental updates,
so this article focuses on handling only full backups. Fortunately,
adding support for incremental backups to the provided scripts is not
difficult using advanced features of the tools described here.
</p><p>
In either case, commercial environments often keep backups in three
locations: locally and two remote sites separated by great distance.
This practice avoids the possibility of complete loss of data should
catastrophe be widespread. Home users might not go to such lengths, but
keeping backups on separate systems, even within your home, is highly
recommended.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e49e0"></a>
Tool Primer</h2></div></div><p>
The primary tool for performing backups on Linux systems is
rsync. This tool is designed specifically for handling
copying of large numbers of files between two systems. It originally
was designed as a replacement for rcp and scp, the latter being the file copy
tool provided with OpenSSH for doing secure file transfers.
</p><p>
As a replacement for scp, rsync is able to utilize the features provided by
OpenSSH to provide secure file transfers. This means a properly
installed SSH configuration can be utilized when using rsync. In fact, SSH
transfers are used by default using standard URI formats for source or
destination files (such as user@host:/path). Alternatively, rsync provides
a standalone server that rsync clients can connect to for file transfers.
To use the rsync server, use a double colon in the URI instead of a single
colon.
</p><p>
SSH (secure shell), is a client/server system for performing operations
across a network using encrypted data. This means what you're transferring
can't be identified easily. SSH is used to log in securely to remote Linux
systems, for example. It also can be used to open a secure channel, called
a tunnel, through which remote desktop applications can be run and
displayed on the local system.
</p><p>
SSH configuration can be fairly complex, but fortunately, it doesn't have to
be. For use with rsync, configure the local and remote machines for the
local machine to log in to the remote machine without a password. To do
this, on the local machine, change to $HOME/.ssh and generate a public key
file:

<pre     class="programlisting">
$ cd $HOME/.ssh
$ ssh-keygen -t dsa
</pre>
</p><p>
ssh-keygen will prompt you for various information. For simplicity's sake,
press Enter to take the default for each prompt. For higher security, 
read the ssh-keygen and ssh man pages to learn what those prompts
represent.
</p><p>
ssh-keygen generates two files, id_dsa and id_dsa.pub. The latter file
must be copied to the remote system under $HOME/.ssh and appended to the
file $HOME/.ssh/authorized_keys. In this code, <span   class="emphasis"><em>remoteHost</em></span> is the
name of the remote computer and <span   class="emphasis"><em>localHost</em></span> is the name of
the local computer:

<pre     class="programlisting">
$ scp id_dsa.pub \
      remoteHost:$HOME/.ssh/id_dsa.pub.localHost
$ ssh remoteHost
$ cd $HOME/.ssh
$ cat id_dsa.pub.localHost &gt;&gt; authorized_keys
</pre>
</p><p>
In this article, I assume a proper SSH configuration with no password
required in order to perform the rsync-based backups. These automated backup
scripts are intended to be run from cron and require a
proper SSH configuration.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e4e58"></a>
Backup UI: Grsync</h2></div></div><p>
For users who prefer to use a desktop tool instead of scripts for setting
up and performing backups, there is the Grsync tool. This is a GTK+-based tool
that provides a nearly complete front end to rsync. It can be used to select
a single source and destination and is likely available from Linux
distribution repositories.
</p><div       class="mediaobject"><img src="10679f2.jpg"><div class="caption"><p>
Figure 2. Grsync is a desktop tool for scheduling backups. Although generally
useful, it lacks include/exclude options and direct cron management.
</p></div></div><p>
Although previous versions appear to have had an integrated cron configuration,
the current version available with Fedora does not. Also, Grsync does not
allow selection of multiple source files or directories nor does it allow
setting exclusion lists. Both of these are supported by the rsync command
line. Grsync can create a session file that can be called from cron, but it 
does not include information on how to notify the user of the results of
the backup.
</p><p>
Due to the lack of cron integration, missing include and exclude options
and no integration of user notification, Grsync is not an ideal backup
solution. The scripts described here, along with the addition of ssmtp for
simplified SMTP-based notification, are a better solution.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e51c8"></a>
File Selection</h2></div></div><p>
With SSH set up and the choice to script backups instead of using a desktop
application out of the way, it is time to consider what files to back up.
Four sets of files should be considered: system
configuration files, database files, users' home directories and Web files.
</p><p>
System configuration files include files such as the password and group
files, hosts, exports and resolver files, MySQL and PHP configurations, SSH
server configuration and so forth. Backup of various system configuration
files is important even if it's not desirable to reuse them directly
during a system re-install. The password and group files, for example, shouldn't
be copied verbatim to /etc/passwd and /etc/group but rather used as
reference to re-create user logins matched to their home directories and
existing groups. The entire /etc directory can be backed up, although in
practice, only a few of these files need to be re-installed or merged after a
distribution re-installation.
</p><p>
Some applications built from source, such as ssmtp, which will be used for
notification in the backup scripts, may install to /usr/local or /opt.
Those directories can be backed up too, or the applications can be rebuilt
after a distribution upgrade.
</p><p>
MySQL database files can be backed up verbatim, but it may be easier to dump
the databases to a text file and then reload them after an upgrade. This
method should allow for the database to handle version changes cleanly.
</p><p>
User home directories typically contain all user data. Generally, all
files under /home except the /home/lost+found directory should be backed
up. This assumes that all user logins are kept on /home. Check your
distribution documentation to verify the location of user home directories.
</p><p>
Home users may not use Web servers internally, but there is no reason they
shouldn't be. Wikis, blogs, media archives and the like are easy to set up
and offer a family a variety of easy-to-use communication systems within the home.
Setting up document root directories (using Apache configuration files)
under /home makes backing up these files identical to any other user files.
</p><p>
There are also files and directories to avoid when performing
backups. The lost+found directory always should be excluded, as should
$HOME/.gvfs, which is created for GNOME users after they log in.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e5538"></a>
Scripting and Notification</h2></div></div><p>
All of the backups can be handled by a single script, but because backup
needs change often, I find it easier to keep with UNIX tradition and
created a set of four small scripts for managing different backup
requirements.
</p><p>
The first script is used to run the other scripts and send e-mail
notifications of the reports on the backup process. This script is run by
root via cron each night:


<pre     class="programlisting">

#!/bin/bash

HOST=`hostname`
date=`date`
mailfile="/tmp/$$.bulog"

# Mail Header
echo "To: userid@yourdomain.org"          &gt; $mailfile
echo "From: userid@yourdomain.org"       &gt;&gt; $mailfile
echo "Subject: $HOST: Report for $date"  &gt;&gt; $mailfile
echo " "                                 &gt;&gt; $mailfile
echo "$HOST backup report:"              &gt;&gt; $mailfile
echo "-------------------------------"   &gt;&gt; $mailfile

# Run the backup.
$1 &gt;&gt; $mailfile 2&gt;&amp;1

# Send the report.
cat $mailfile | \
    /usr/local/ssmtp/sbin/ssmtp -t \
    -auuserid@yourdomain.org -apyourpassword \
    -amCRAM-MD5
rm $mailfile

</pre>
</p><p>
The first argument to the script is the backup script to run. An enhanced
version would verify the command-line option before attempting to run it.
</p><p>
This script uses an external program (ssmtp) for sending backup reports.
If you have an alternative tool for sending e-mail from the command
line, you can replace ssmtp usage with that tool. Alternatively, you can
skip using this front end completely and run the backup scripts directly
from cron and/or the command line.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e57f8"></a>
ssmtp</h2></div></div><p>
ssmtp is a replacement for Sendmail that is considerably less complex to
configure and use. It is not intended to retrieve mail, however. It is
intended only for outbound e-mail. It has a small and simple configuration
file, and when used as a replacement for Sendmail, it will be used by
command-line programs like mail for sending e-mail.
</p><p>
ssmtp is not typically provided by Linux distributions, but the source can
be found with a Google search on the Internet. Follow the package
directions to build and install under /usr/local. Then, replace sendmail
with ssmtp by setting a symbolic link from /usr/sbin/sendmail to the
installation location of ssmtp.

<pre     class="programlisting">
$ mv /usr/sbin/sendmail /usr/sbin/sendmail.orig
$ ln -s /usr/local/sbin/ssmtp /usr/sbin/sendmail
</pre>
</p><p>
If your distribution supports the alternatives tool, you may
prefer to use it instead of the symbolic link to let the system use ssmtp
instead of Sendmail. Note that, as a bonus, when the author replaced
Sendmail with ssmtp, LogWatch suddenly began sending nightly reports via
e-mail, allowing me a view on system activity I never had seen before and
which many Linux users probably never have seen before either.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e5a08"></a>
System Configuration File Backups</h2></div></div><p>
Backing up system configuration files is handled by a Perl script that
verbosely lists the files to be copied to a location on the /home partition.
The script is run by root via cron every night to copy the configuration
files to a directory in user data space (under /home):

<pre     class="programlisting">

#!/usr/bin/perl
$filelist = &lt;&lt;EOF;
/etc/passwd
/etc/group
...  # other config files to backup
EOF

@configfiles = split('\n', $filelist);
for (@configfiles)
{
    if (-e $_) { $files = join(" ", $files, $_); }
    elsif (index($_, "*") &gt;= 0) {
        $files = join(" ", $files, $_);
    }
}
print "Creating archive...\n";
`tar Pczf $ARGV[0]/systemfiles.tar.gz $files`;

</pre>
</p><p>
This brute-force method contains a list of the files to back up, joins
them into a single tar command and builds a tar archive of those files on
the local system. The script is maintained easily by modifying the list of
files and directories. Because the configuration files are copied
locally to user data space, and user data space is backed up separately,
there is no need for rsync commands here. Instead, the system configuration
tar archive is kept with user data and easily referenced when doing
restores or system upgrades. The backup script functions as a full backup,
replacing the tar archive with each execution unless a different
destination is specified as a command-line argument.
</p><p>
What this script lacks in Perl excellence it makes up for in simplicity of
maintenance. Note that the &ldquo;retail&rdquo; version of this script ought to include
additional error checking for the command-line argument required to specify
the location to save the archive file.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19e5c70"></a>
Database Backups</h2></div></div><p>
Like system configuration files, databases are backed up to user data
directories to be included in the user data backups. Databases are of
slightly higher importance in day-to-day use, so this script uses a seven-day rotating cycle for database file dumps. This allows restoring backups
from up to a week ago without overuse of disk space for the backups. This
method is not incremental, however. It is a set of seven full backups of
each database.
</p><p>
Like the system configuration file backup script, this script lists the
items to back up. The mysqldump command assumes no password for
the root user to access the databases. This is highly insecure, but for
users behind a firewall, it is likely the easiest way to handle database
management:

<pre     class="programlisting">

#!/usr/bin/perl -w
use File::Path qw(make_path remove_tree);
my $BUDIR1="/home/httpd/db";
my ($sec,$min,$hour,$mday,$mon,$year,
    $wday,$yday,$isdst) = localtime time;
$year += 1900;
$mon += 1;
if ($mon &lt; 10 )  { $mon  = "0".$mon; }
if ($mday &lt; 10 ) { $mday = "0".$mday; }
$TODAY = $wday;

@dbname = (
    "mysql",
    "wordpress",
);

make_path ("$BUDIR1/$year");
foreach $db (@dbname) {
    $cmd = "mysqldump -B -u root $db " .
           "-r $BUDIR1/$year/$TODAY-$db.sql";
    system("$cmd");
}

print ("Database Backups for " .
       $year . "/" . $mon . "/" .
       $mday . "\n");
print ("-------------------------------\n");
open(PD, "ls -l $BUDIR1/$year/$TODAY-*.sql |" );
@lines = &lt;PD&gt;;
close(PD);
$output = join("\n", @lines);
print ($output);

</pre>
</p><p>
Unlike the configuration file backup script, this script prints out the
list of files that have been created. This provides a quick, visual
feedback in the e-mailed report that the backups produced something
meaningful.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19dc710"></a>
User Data Backups</h2></div></div><p>
The system configuration backup script and the database backup script are
run first to generate backups to user data space. Once complete, all
data is ready to be backed up to the remote system with an rsync-based
script:

<pre     class="programlisting">

#!/bin/bash
function checkRC
{
    rc=$1
    name=$2
    if [ $rc != 0 ]
    then
        echo "== $name failed with rsync rc=$rc =="
    fi
}
LOGIN=root@feynman
BRAHE=$LOGIN:/media/BackupDrive/feynman
if [ "$1" != "" ]
then
    BRAHE=$1
fi

</pre>
</p><p>
The script includes a shell function to test rsync's return code and
print an error message on failure. The front-end script redirects output
from this script to a file, so error messages show up in the e-mailed
backup report.
</p><p>
The default destination for the backup is configured at the start of the script.
The first command-line argument can be used to override the default:

<pre     class="programlisting">
DIR1="/home/httpd"
DIR2="/home/mjhammel"
EXCL2=--exclude-from=/home/mjhammel/.rsync/local
</pre>
</p><p>
The user data backup script is focused on directories. Unlike the other
backup scripts, the list of items to back up are hard-coded in separate
variables. Again, this is a brute-force method used for simplicity, because
each directory to back up may have one or more sets of include and
exclude arguments. Associative arrays could be used instead of the set of
variables in a more generalized version of this script.
</p><p>
Notice that this configuration calls out individual directories under /home
instead of backing up all of /home. The script from which this was pulled
is used on a machine with development directories under /home that do not
need to be backed up. Specifying /home and using an exclusion file is an
alternative way of doing the same thing:

<pre     class="programlisting">
DATE=`date`
echo "== Backing up `uname -n` to $BRAHE."
echo "== Started @ $DATE "
echo "== Directory: $DIR1"
rsync -aq --safe-links $DIR1 $BRAHE
checkRC $? "$DIR1"
</pre>
</p><p>
The first directory is backed up to the remote system. The -a option tells
rsync to operate in archive mode, where rsync will do the following:
</p><div class="itemizedlist"><ul type="disc"><li><p>
Recursively traverse the specified directory tree.
</p></li><li><p>
Copy symlinks as symlinks and not the files they point to.
</p></li><li><p>
Preserve owner, groups, permissions and modification times.
</p></li><li><p>
Preserve special files, such as device files.
</p></li></ul></div><p>
The safe-links option tells rsync to ignore symbolic links that point to files
outside the current directory tree. This way, restoration from the archive
won't include symbolic links that may point to locations that no longer exist.
The -q option tells rsync to run with as few non-error messages as
possible:

<pre     class="programlisting">
echo "== Directory: $DIR2"
rsync -aq --safe-links $EXCL2 $DIR2 $BRAHE
checkRC $? "$DIR2"
DATE=`date`
echo "Backups complete @ $DATE"
</pre>
</p><p>
The second directory tree is backed up using an exclusion list. This list
is a file that specifies the files and directories within the current
directory tree to be ignored by rsync. Entries in this file prefixed with
a dash are excluded from the set of files and directories rsync will
process. The three asterisks match anything below the specified
directories:

<pre     class="programlisting">
- /mjhammel/.gvfs/***
- /mjhammel/Videos/***
- /mjhammel/brahe/***
- /mjhammel/iso/***
</pre>
</p><p>
This example shows that no files under the Videos and iso directories will
be included in the backup. It would be a poor use of disk space to back up
files that exist in your home directory but that also can be retrieved from
the Internet.
</p><p>
The brahe reference is a mountpoint for the home directory
of an identical user ID on a remote system. This allows access to files
under a login on another system simply by changing into the remote system's
local mountpoint. But, there is no reason to back up those remote files on the
local system, as that remote system has its own backup scripts
configured.
</p><p>
The full version of this script includes an SSH-based verification that the
remote system has the required external USB drive mounted and it is
available for use. This allows the script to recognize that the remote
system is misbehaving before wasting time trying to run a backup that would
fail anyway.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19dd058"></a>
Automation via Cron</h2></div></div><p>
The order in which these scripts is run is
important. The system configuration
file backup script and the database backup script can run in parallel but
must complete before the user data backup script is run:

<pre     class="programlisting">

30 0 * * * /path/to/backup-db.pl
30 1 * * * /path/to/backup-configfiles.sh \
           /path/to/save/dir 2&gt;&amp;1 &gt; /dev/null
30 2 * * * /path/to/backup-frontend.sh \
           /path/to/backup-data.sh

</pre>
</p><p>
To pass arguments to backup-data.sh, enclose the entire command in double
quotes:

<pre     class="programlisting">
30 2 * * * /path/to/backup-frontend.sh \
          "/path/to/backup-data.sh root@copernicus:/backups"
</pre>
</p><p>
Each morning, the backup report is available for each machine that runs
these scripts and can be reviewed to make sure the backups completed
successfully. In practice, the most common problems encountered are related to
unmounted or non-functioning drives, or to network outages that occur before or
during the backup process.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19dd318"></a>
Summary</h2></div></div><p>
In preparing a personal backup strategy, it is important to identify the
purpose of the backup, establish a set of processes that prepares files for
backup and performs backups to remote systems. It is also important that
automation of these processes provide feedback, so users can have at least a
starting point of understanding why backups are failing and when that may
have occurred.
</p><p>
The methods shown here are somewhat simple and certainly not ideal for
every user. The scripts probably are not bug-free and also have room for
improvement. They are intended only as a starting point for
building personal backup plans. I welcome feedback on any improvements you
make to these scripts.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18ec580.0x19dd478"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
Backup Scripts for This Article: <a href="../listings/194/10679.tgz" target="_self">ftp.linuxjournal.com/pub/lj/listings/issue194/10679.tgz</a>
</p><p>
ssmtp: <a href="http://www.graphics-muse.org/source/ssmtp_2.61.orig.tar.gz" target="_self">www.graphics-muse.org/source/ssmtp_2.61.orig.tar.gz</a>
</p><p>
rsync: <a href="http://samba.anu.edu.au/rsync" target="_self">samba.anu.edu.au/rsync</a>
</p><p>
OpenSSH: <a href="http://www.openssh.com" target="_self">www.openssh.com</a>
</p><p>
meld: <a href="http://meld.sourceforge.net" target="_self">meld.sourceforge.net</a>
</p></div></div></div>
<div class="authorblurb"><p>
Michael J. Hammel is a Principal Software Engineer for Colorado
Engineering, Inc. (CEI), in Colorado Springs, Colorado, with more than 20
years
of
software development and management experience. He has written more than
100
articles for numerous on-line and print magazines and is the author of
three
books on The GIMP, the premier open-source graphics editing package.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../194/toc194.html">Issue Table of Contents</a>
    <a class="link3" href="../194/10679.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>