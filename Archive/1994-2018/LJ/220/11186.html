<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
A Guide to Using LZO Compression in Hadoop
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Deploy and implement MapReduce programs that take advantage of the LZO&#10;compression techniques supported by Hadoop.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x227a580.0x2371ac0"></a>
A Guide to Using LZO Compression in Hadoop
</h1></div><div><div class="author"><h3 class="author">
Arun
 
Viswanathan
</h3></div><div class="issuemoyr">Issue #220, August 2012</div></div><div><p>
Deploy and implement MapReduce programs that take advantage of the LZO
compression techniques supported by Hadoop.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x23721f8"></a></h2></div></div><p>
Compression is the process of reducing the size of actual data by
using an algorithm to encode the information. Compression provides
the following benefits:
</p><div class="itemizedlist"><ul type="disc"><li><p>
Reduces the hard disk space occupied by the data.
</p></li><li><p>
Uses lower transmission bandwidth.
</p></li><li><p>
Reduces the time taken to copy or transfer the data from one location
to another.
</p></li></ul></div><p>
However, it also comes with a problem. Before putting the compressed
data to some use, it first must be decompressed. After processing,
the data must be compressed again. This increases the time it takes an
application to process it before it can use this data.
</p><p>
As Hadoop adoption grows in corporate communities, you see data
in terms of TeraBytes and PetaBytes that is stored in HDFS by large
enterprises. Because Hadoop works well on commodity hardware, high-end
servers are not required for storing and processing this kind of
data, and it would be beneficial for enterprises to reduce the space used
to store the data. The Hadoop framework supports a number of mechanisms,
such as gzip, bzip and lzo to compress the data that is stored in HDFS.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x2372720"></a>
LZO Compression</h2></div></div><p>
Lempel-Ziv-Oberhumer (or LZO) is a lossless algorithm that
compresses data to ensure high decompression speed. It has the following
characteristics:
</p><div class="itemizedlist"><ul type="disc"><li><p>
Data compression is similar to other popular compression techniques,
such as gzip and bzip.
</p></li><li><p>
It enables very fast decompression.
</p></li><li><p>
It supports overlapping compression and in-place decompression.
</p></li><li><p>
Compression and decompression happen on a block of data.
</p></li><li><p>
It requires no additional memory for decompression except for source
buffers and destination buffers.
</p></li></ul></div><p>
In Hadoop, using LZO helps reduce data size and causes shorter disk
read times. Furthermore, the block structure of LZO allows it to be split
for parallel processing in MapReduce programs. These characteristics
make LZO suitable for use in Hadoop.
</p><p>
In this article, I look at the procedure for enabling LZO in Hadoop-based
frameworks and look at a few examples of LZO's usage.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x2372cf8"></a>
Prerequisites</h2></div></div><p>
The following describes the software that was set up in CentOS 5.5-based
machines.
</p><p>
Set up and configure the Cloudera Distribution of Hadoop (CDH3) or
Apache Hadoop 0.20.x in a cluster of two or more machines. Refer to
the Cloudera or Apache Hadoop Web sites for more information on setting
up Hadoop. Alternatively, you also could use the Cloudera demo VM as a
single-node cluster for testing.
</p><p>
Next, install the LZO package in the system. Download and install the
package from its Linux distribution repository. For this
article, I installed this RPM: lzo-2.04-1.el5.rf.i386.rpm.
</p><p>
There are two ways to install the LZO-specific jars that can be used
by Hadoop:
</p><div class="itemizedlist"><ul type="disc"><li><p>
Download and build the hadoop-lzo project from Twitter that will provide
the necessary jars (see Resources). 
</p></li><li><p>
Download the prebuilt jars in RPM or Debian packages from the
hadoop-gpl-packing project. For this article, I used this RPM: 
hadoop-gpl-packaging-0.2.0-1.i386.rpm. 
</p></li></ul></div><p>
The following binaries will be installed on the machine:

<pre     class="programlisting">
$HADOOP_GPL_HOME/lib/*.jar
$HADOOP_GPL_HOME/native
</pre>
</p><p>
HADOOP_GPL_HOME is the directory where the hadoop-lzo project will
store the built binaries.
</p><p>
Using the prebuilt RPMs, the binaries will be installed in the 
/opt/hadoopgpl folder.
</p><p>
Note: if you are using a cluster of more than one machine, the
above three steps need to be done for all the machines in the cluster.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x2373278"></a>
Deploy LZO in the Hadoop Ecosystem</h2></div></div><p>
First, install LZO for Hadoop.
Then, add the Hadoop GPL-related jars to the Hadoop path:

<pre     class="programlisting">
$ cp $HADOOP_GP_HOME/lib/*.jar $HADOOP_HOME/lib/
</pre>
</p><p>
Next, run the following command, depending on the platform you're using:

<pre     class="programlisting">
$ tar -cBf - -C $HADOOP_GPL_HOME/native/ * | 
 &#8618;tar -xBvf - -C $HADOOP_HOME/lib/native/
</pre>
</p><p>
Then, update the Hadoop configuration files to register external codecs
in the codec factory.
Refer to Listing 1 to add the lines to the $HADOOP_HOME/conf/core-site.xml
file.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x2373538"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 1. Adding LZO Codecs to Hadoop core-site.xml</b></p><pre     class="programlisting">
&lt;!-- Add LZO Compression Codecs --&gt;
&lt;property&gt;
   &lt;name&gt;io.compression.codecs&lt;/name&gt;
   &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop
&#8618;.io.compress.DefaultCodec,com.hadoop.compression.lzo.
&#8618;LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.
&#8618;hadoop.io.compress.BZip2Codec&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
   &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;
   &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;
&lt;/property&gt;
</pre></div><p>
Refer to Listing 2 to add the lines to the $HADOOP_HOME/conf/mapred-site.xml file.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x23737a0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 2. Adding LZO Codecs to Hadoop mapred-site.xml</b></p><pre     class="programlisting">

&lt;!-- Add LZO Codecs details --&gt;
&lt;property&gt;
   &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt;
   &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
   &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt;
   &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;
&lt;/property&gt;

</pre></div><p>
The LZO files also need to be added in the Hadoop classpath. In the
beginning of the $HADOOP_HOME/conf/hadoop-env.sh file, add the entries as
shown in
Listing 3.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x2373a60"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 3. Adding LZO-Related Libraries to hadoop-env.sh</b></p><pre     class="programlisting">
export
HADOOP_CLASSPATH="$HADOOP_HOME/lib/hadoop-lzo.jar:
&#8618;$HADOOP_CLASSPATH:$CLASS_FILES"

# For 32-bit machines
export
JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native/Linux-i386-32:
&#8618;$HADOOP_HOME/lib/native
# For 64-bit machines
export
JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native/Linux-amd64-64:
&#8618;$HADOOP_HOME/lib/native
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x2373c70"></a>
Install LZO for Pig</h2></div></div><p>
Add the Hadoop GPL-related jars to the Pig path:

<pre     class="programlisting">
$ cp $HADOOP_GPL_HOME/lib/*.jar $PIG_HOME/lib/
</pre>
</p><p>
Next, run the following command, depending on the platform you're using:

<pre     class="programlisting">
$ tar -cBf - -C $HADOOP_GPL_HOME/native/ * | 
 &#8618;tar -xBvf - -C $PIG_HOME/lib/native/
</pre>
</p><p>
Additionally, you'll need to make changes to the Pig Script and
configuration files to register the external codecs in the codec factory.
Refer to Listing 4 to add the lines to the $PIG_HOME/conf/pig.properties file,
and refer to Listing 5 to add the lines to the $PIG_HOME/bin/pig
script file.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236a768"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 4. Adding LZO Codecs to the Pig Properties File</b></p><pre     class="programlisting">
mapred.output.compression.codec=
&#8618;com.hadoop.compression.lzo.LzoCodec
mapred.map.output.compression.codec=
&#8618;com.hadoop.compression.lzo.LzoCodec

# For 32-bit machines
mapred.child.java.opts=-Djava.library.path=
&#8618;/opt/hadoopgpl/native/Linux-i386-32
# For 64-bit machines
#mapred.child.java.opts=-Djava.library.path=
&#8618;/opt/hadoopgpl/native/Linux-amd64-64
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236a978"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 5. Adding LZO-Related Libraries to the Pig Script</b></p><pre     class="programlisting">
# For 32-bit machines
PIG_OPTS="$PIG_OPTS -Djava.library.path=
&#8618;/opt/hadoopgpl/native/Linux-i386-32"
# For 64-bit machines
#PIG_OPTS="$PIG_OPTS -Djava.library.path=
&#8618;/opt/hadoopgpl/native/Linux-amd64-64"

# Add hadoop lzo to CLASSPATH
for f in $PIG_HOME/lib/hadoop*lzo*.jar; do
  CLASSPATH=${CLASSPATH}:$f;
Done
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236ab88"></a>
Install LZO for HBase</h2></div></div><p>
Copy the Hadoop GPL jars to the HBase lib directory:

<pre     class="programlisting">
$ cp $HADOOP_GPL_HOME/lib/*.jar $HBASE_HOME/lib/
</pre>
</p><p>
Run either of the following commands, depending on the platform you're
using:

<pre     class="programlisting">
$ cp $HADOOP_GPL_HOME/native/Linux-i386-32/lib/* 
 &#8618;$HBASE_HOME/lib/native/Linux-i386-32/
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236ad98"></a></h2></div></div><p>
or:

<pre     class="programlisting">
$ cp $HADOOP_GPL_HOME/native/Linux-amd64-64/lib/* 
 &#8618;$HBASE_HOME/lib/native/Linux- amd64-64/
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236aef8"></a>
Using LZO Compression</h2></div></div><p>
Let's look at a sample program for testing LZO in Hadoop.
The code in Listing 6 shows a sample MapReduce program that reads
an input file in LZO-compressed format. To generate compressed data for
use with this word counter, run the lzop program on a regular data
file.
Similar sample code is provided with the Elephant-Bird Project.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236b0b0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 6. Sample MapReduce Program to Test LZO in a Hadoop Cluster</b></p><pre     class="programlisting">
/**
 * MapReduce Word count Sample
 * Input File ~V LZO compressed file
 * Run com.hadoop.compression.lzo.LZOIndexer / 
 * com.hadoop.compression.lzo.
 * DistributedLZOIndexer to create .lzo.index file to further
 * improve the read speed of LZO compressed files.
 * If the input lzo files are indexed, the input format will take
 * advantage of it. The input file/directory is taken as the first 
 * argument. The output directory is taken as the second argument.
 * Uses NullWritable for efficiency.
 *
 * Usage: hadoop jar path/to/this.jar &lt;input-dir&gt; &lt;output-dir&gt;
 */
public class SimpleLZOWC extends Configured implements Tool {

  private SimpleLZOWC () {}

  public static class LzoWordCountMapper extends Mapper&lt;LongWritable, 
 &#8618;Text, Text, LongWritable&gt; {
    private final LongWritable one = new LongWritable(1L);
    private final Text word = new Text();

    @Override
    protected void map(LongWritable key, Text value, Context context) 
 &#8618;throws IOException, InterruptedException {
      String line = value.toString();
      StringTokenizer tokenizer = new StringTokenizer(line);
      while (tokenizer.hasMoreTokens()) {
        word.set(tokenizer.nextToken());
        context.write(word, one);
      }
    }
  }

  public int run(String[] args) throws Exception {
    Job job = new Job(getConf());
    job.setJobName("Simple LZO Word Count");

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(LongWritable.class);

    job.setJarByClass(getClass());
    job.setMapperClass(LzoWordCountMapper.class);
    job.setCombinerClass(LongSumReducer.class);
    job.setReducerClass(LongSumReducer.class);

    // Use the custom LzoTextInputFormat class.
    job.setInputFormatClass(LzoTextInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);

    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    return job.waitForCompletion(true) ? 0 : 1;
  }

  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new LzoWordCount(), args);
    System.exit(exitCode);
  }
}
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236b2c0"></a>
Sample Program for Testing LZO in Pig</h2></div></div><p>
The PigLzoTest program shown in Listing 7 achieves the same result as
the MapReduce program described previously with the only difference being it
is written using Pig.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236b420"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 7. Sample Program to Test LZO in Pig</b></p><pre     class="programlisting">
/**
 * Pig Word count Sample
 * Input File ~V LZO compressed file
 * Run com.hadoop.compression.lzo.LZOIndexer / 
 * com.hadoop.compression.lzo.
 * DistributedLZOIndexer to create .lzo.index file to further
 * improve the read speed of LZO compressed files.
 * Output - output directory is taken as the second argument.
 *
 * To generate data for use with this word counter, run lzop 
 * on a data file
 * Usage: PigLzoTest &lt;input-file&gt; &lt;output-folder&gt;
 */
public class PigLzoTest {

    /**
     * @param args
     */
    public static void main(String[] args) {
            try {
                     PigServer pigServer = new PigServer("mapreduce");
                     pigServer.registerJar("lib/elephant-bird-2.0.jar");
                     pigServer.registerJar("lzotest.jar");

                     runWordCountQuery(pigServer, args[0], args[1]);
          } catch (IOException e) {
                     e.printStackTrace();
          }
    }

    /**
     * Pig Script for Word Count
     * @param pigServer
     * @throws IOException
     */
    public static void runWordCountQuery(PigServer pigServer, 
 &#8618;String inputFile, String outputFile) throws IOException {
        pigServer.registerQuery("A = load '" + inputFile + "';");
        pigServer.registerQuery("B = foreach A generate
 &#8618;flatten(TOKENIZE((chararray)$0)) as word;");
        pigServer.registerQuery("C = filter B by word matches '\\w+';");
        pigServer.registerQuery("D = group C by word;");
        pigServer.registerQuery("E = foreach D generate group as word,
 &#8618;COUNT(C) as count;");
        pigServer.registerQuery("F = order E by count desc;");

        pigServer.registerQuery("store F into '" + outputFile + "' 
 &#8618;using com.hadoop.compression.lzo.LzoTextStorer();");
    }
}
</pre></div><p>
The last line in Listing 7 calls a user-defined function (UDF) to
write the output in LZO format. The code snippet in Listing 8 shows the contents
of this class. The LZOTextStorer class shown in Listing 8 extends the
<tt  >com.twitter.elephantbird.pig.store.LzoBaseStoreFunc</tt> class provided by
the Elephant-Bird Project for writing the output in the LZO format.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236b738"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 8. Sample Pig UDF to Write the Output in LZO Format</b></p><pre     class="programlisting">
/**
 * Write the LZO file line by line, passing each 
 * line as a single-field Tuple to Pig.
 */
public class LzoTextStorer extends LzoBaseStoreFunc {
  private static final TupleFactory tupleFactory_ =
TupleFactory.getInstance();

  protected enum LzoTextLoaderCounters { LinesRead }

  public LzoTextStorer() {}

    @Override
    public OutputFormat getOutputFormat() throws IOException {
            return new TextOutputFormat&lt;WritableComparable, Text&gt;();
    }

    @Override
    public void putNext(Tuple tuple) throws IOException {
            if (writer == null)
                    System.out.println("Writer is null");

            int numElts = tuple.size();
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i &lt; numElts; i++) {
          String field;
          try {
            field = String.valueOf(tuple.get(i));
          } catch (ExecException ee) {
            throw ee;
          }
          sb.append(field);

          if (i == numElts - 1) {
            // Last field in tuple.
            sb.append('\n');
          } else {
            sb.append('\t');
          }
        }

        Text text = new Text(sb.toString());
            try {
            writer.write(NullWritable.get(), text);
        } catch (InterruptedException e) {
            throw new IOException(e);
        }
    }
}
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236b948"></a>
Sample Program for Testing LZO in HBase</h2></div></div><p>
To use LZO in HBase, specify a per-column family compression flag while
creating the table:

<pre     class="programlisting">
create 'test', {NAME=&gt;'colfam:', COMPRESSION=&gt;'lzo'}
</pre>
</p><p>
Any data that is inserted into this table now will be stored in LZO format.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236bb00"></a>
Conclusion</h2></div></div><p>
In this article, I looked at the process for building and setting up
LZO in Hadoop. I also looked at the sample implementation processes
across MapReduce, Pig and HBase frameworks. LZO compression helps in
reducing the space used by data that is stored in the HDFS. It also
provides an added performance benefit due to the splittable block
architecture that it follows. Faster read times of LZO compressed
data with reduced decompression time makes it ideal as a compression
algorithm for storing data in the HDFS. It is already a popular technique
that is used by a number of social Web companies, such as Twitter,
Facebook and so on, internally to store data. Twitter also has provided the
open-source Elephant-Bird Project that provides the basic classes for
using LZO. 
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x227a580.0x236bc08"></a></h2></div></div><div class="sidebar"><p class="title"><b>Resources</b></p><p>
Lempel-Ziv-Oberhumer Algorithm:
<a href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer" target="_self">en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer</a>
</p><p>
Using LZO Compression in Hadoop:
<a href="http://wiki.apache.org/hadoop/UsingLzoCompression" target="_self">wiki.apache.org/hadoop/UsingLzoCompression</a>
</p><p>
Cloudera Demo VM:
<a href="https://ccp.cloudera.com/display/SUPPORT/Cloudera's+Hadoop+Demo+VM" target="_self">https://ccp.cloudera.com/display/SUPPORT/Cloudera's+Hadoop+Demo+VM</a>
</p><p>
LZO Package for Red Hat-Based Distributions:
<a href="http://rpmforge.sw.be/redhat/el5/en/i386/rpmforge/RPMS/lzo-2.04-1.el5.rf.i386.rpm" target="_self">rpmforge.sw.be/redhat/el5/en/i386/rpmforge/RPMS/lzo-2.04-1.el5.rf.i386.rpm</a>
</p><p>
Twitter Hadoop-lzo Project: <a href="http://github.com/kevinweil/hadoop-lzo" target="_self">github.com/kevinweil/hadoop-lzo</a>
</p><p>
RPM for LZO for Linux: <a href="http://code.google.com/p/hadoop-gpl-packing" target="_self">code.google.com/p/hadoop-gpl-packing</a>
</p><p>
Hadoop GPL Compression FAQ:
<a href="http://code.google.com/a/apache-extras.org/p/hadoop-gpl-compression/wiki/FAQ" target="_self">code.google.com/a/apache-extras.org/p/hadoop-gpl-compression/wiki/FAQ</a>
</p><p>
LZOP binaries for Windows and Linux: <a href="http://www.lzop.org" target="_self">www.lzop.org</a>
</p><p>
Hadoop at Twitter:
<a href="http://www.cloudera.com/blog/2009/11/hadoop-at-twitter-part-1-splittable-lzo-compression" target="_self">www.cloudera.com/blog/2009/11/hadoop-at-twitter-part-1-splittable-lzo-compression</a>
</p><p>
Twitter Elephant-Bird Project:
<a href="https://github.com/kevinweil/elephant-bird" target="_self">https://github.com/kevinweil/elephant-bird</a>
</p></div></div></div>
<div class="authorblurb"><p>
Arun Viswanathan is a Technology Architect with the Cloud Labs at Infosys
Ltd, a global leader in IT and business consulting services. Arun has
about nine years of experience with expertise in Java; Java EE application
development; and defining, architecting and implementing large-scale,
mission-critical, IT solutions across a range of industries. He is
currently involved in design, development and consulting for big-data
solutions using Apache Hadoop.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../220/toc220.html">Issue Table of Contents</a>
    <a class="link3" href="../220/11186.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>