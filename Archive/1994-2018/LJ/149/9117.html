<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
PathScale InfiniPath Interconnect</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;InfiniBand and AMD HyperTransport are made for each other just like soup&#10;and...something that goes with soup.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x282a580.0x2921ab0"></a>
PathScale InfiniPath Interconnect</h1></div><div><div class="author"><h3 class="author">
Logan
 G. 
Harbaugh
</h3></div><div class="issuemoyr">Issue #149, September 2006</div></div><div><p>
InfiniBand and AMD HyperTransport are made for each other just like soup
and...something that goes with soup.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x282a580.0x29222f0"></a></h2></div></div><p>
As the use of large clusters gains ground in academia and moves from the
scientific world to the business world, many administrators are looking
for ways to increase performance without significantly increasing the
cost per node.  Some may focus on CPU power/speed or the amount of RAM
per node, relatively expensive components, to increase their horsepower.
PathScale (recently acquired by QLogic) is taking a different approach,
instead focusing on unleashing the computational power already contained
in the cluster as a whole by allowing the &ldquo;thoroughbred&rdquo; processors
built by Intel and AMD to move all the messages they are capable of
generating.
</p><p>
By focusing on dramatically increasing the message traffic between nodes
and by reducing the latency of those messages, applications running on
clusters are able to run faster and scale higher than previously
possible.  And, the increased performance is achieved with the combination
of inexpensive x86 servers with standard InfiniBand adapters and switches.
</p><p>
The InfiniPath InfiniBand cluster interconnect is available in two
flavors: PCI Express for ubiquitous deployments with any motherboard
and any processor, and directly connected to the HyperTransport bus for
the absolute lowest latency.  This article deals with the
InfiniPath HyperTransport (or HTX) product line.  Servers with
motherboards that support InfiniPath HTX are available from more than 25
different system vendors, including Linux Networx, Angstrom, Microway,
Verari and Western Scientific.  In the near future, servers with HTX
slots could be available from the larger tier-one computer system
suppliers.  Motherboards with HTX slots are currently shipping from Iwill
(the DK8-HTX) and Supermicro (H8QC8-HTe), with additional offerings from
Arima, ASUS, MSI and others coming soon.  InfiniPath adapters, which can
be used with just about any flavor of Linux, can be connected to any
InfiniBand switch from any vendor.  Also, for mixed deployments with
InfiniBand adapters from other vendors, InfiniPath supports the
OpenFabrics (formerly OpenIB) software stack (downloadable from the
PathScale Web site).
</p><p>
What the InfiniPath HTX adapter does better than any other cluster
interconnect is accept the millions of messages generated every second
by fast, multicore processors and gets them to the receiving processor.
Part of the secret is removing all the delays associated with bridge
chips and the PCI bus, because traffic is routed over the much faster
HyperTransport bus.  In real-world testing, this produces a two- to
three-times improvement in latency, and in real-world clustered
applications, an increase in messages per second of ten times or more.
</p><p>
Message transmission rate is the unsung hero in the interconnect world,
and by completely re-architecting its adapter, InfiniPath beats the
next-best by more than ten times.  Where the rest of the industry builds
off-load engines, miniature versions of host servers with an embedded
processor and separate memory, InfiniPath is based on a very simple,
elegant design that does not duplicate the efforts of the host
processor.  Embedded processors on interconnect adapter cards are only
about one-tenth the speed of host processors so they can't keep up with
the number of messages those processors generate.  By keeping things
simple, InfiniPath avoids wasting CPU cycles on pinning cache and other
housekeeping chores, required with off-load engines, and instead does
real work for the end user.  The beauty of this approach is that it not
only results in lower CPU utilization per MB transferred, but it also
has a lower memory footprint on host systems.
</p><p>
The reason a two- or three-times improvement in latency has such a large
effect on the message rate (messages per second) is that low latency
reduces the time that nodes spend waiting for the next communication at
both ends, so all the processors substantially reduce wasted cycles
spent waiting on adapters jammed with message traffic.
</p><p>
What does this mean for real-world applications? It will depend on the
way the application uses messages, the sizes of those messages and how
well optimized it is for parallel processing.  In my testing, using a
17-node (16 compute nodes and one master node) cluster, I got a result
of 5,149.154 MB/sec using the b_eff benchmark.  This compares with
results of 1,553&ndash;1,660 MB/sec for other InfiniBand clusters tested by
the Daresbury Lab in 2005, and with a maximum of 2,413 MB/sec for any
other cluster tested.  The clusters tested all had 16 CPUs.
</p><p>
See Listing 1 for the results of the b_eff benchmark.  The results of the
Daresbury Lab study are available at
<a href="http://www.cse.clrc.ac.uk/disco/Benchmarks/commodity.2005/mfg_commodity.2005.pdf" target="_self">www.cse.clrc.ac.uk/disco/Benchmarks/commodity.2005/mfg_commodity.2005.pdf</a>,
page 21.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x282a580.0x2922710"></a></h2></div></div><div class="sidebar"><p class="title"><b>Listing 1. b_eff output</b></p><pre     class="programlisting">
The effective bandwidth is b_eff = 5149.154 MByte/s on 16 processes

( = 321.822 MByte/s * 16 processes)

Ping-pong latency: 1.352 microsec

Ping-pong bandwidth: 923.862 MByte/s at Lmax= 1.000 MByte

(MByte/s=1e6 Byte/s) (MByte=2**20 Byte)

system parameters : 16 nodes, 128 MB/node

system name : Linux

hostname : cbc-01

OS release : 2.6.12-1.1380_FC3smp

OS version : #1 SMP Wed Oct 19 21:05:57 EDT 2005

machine : x86_64

Date of measurement: Thu Jan 12 14:20:52 2006
</pre></div><p>
Most vendors do not publish their message rate, instead putting out
their peak bandwidth and latency.  But bandwidth varies with the size of
the message, and peak bandwidth is achieved only at message sizes much
larger than most applications generate.  For most clustered applications,
the actual throughput of the interconnect is a fraction of peak, because
few clustered applications pass large messages back and forth between
nodes.  Rather, applications running on clusters pass a large number of
very small (8&ndash;1,024 byte) messages back and forth as nodes begin and
finish processing their small pieces of the overall task.
</p><p>
This means that for most applications, the number of simultaneous
messages that can be passed between nodes, or message rate, will tend to
limit the performance of the cluster more than the peak bandwidth of the
interconnect.
</p><p>
As end users attempt to solve more granular problems with bigger
clusters, the average message size goes down and the overall number of
messages goes up.  According to PathScale's testing with the WRF modeling
application, the average number of messages increases from 46,303 with a
32-node application to 93,472 with a 512-node application, while the mean
message size decreases from 67,219 bytes with 32 nodes to 12,037 bytes
with 512 nodes.  This means that the InfiniPath InfiniBand adapter will
become more effective as the number of nodes increases.  This is borne
out in other tests with large-scale clusters running other applications.
</p><p>
For developers, there is little difference between developing a standard
MPI application and one that supports InfiniPath.  Required software is
limited to some Linux drivers and the InfiniPath software stack.  Table 1
shows the versions of Linux that have been tested with the InfiniPath
1.2 release.  PathScale also offers the EKOPath Compiler Suite version
2.3, which includes high-performance C, C++ and Fortran 77/90/95
compilers as well as support for OpenMP 2.0 and PathScale-specific
optimizations.  But the compiler suite is not required to develop
InfiniPath applications because the InfiniPath software environment
supports gcc, Intel and PGI compilers as well.  The base software
provides an environment for high-performance MPI and IP applications.
</p><div class="table"><a name="N0x282a580.0x2922a80"></a><p class="title"><b>Table 1.  The InfiniPath 1.2 release has been tested on the
following Linux distributions for AMD Opteron systems (x86_64).</b></p><table     summary="Table 1.  The InfiniPath 1.2 release has been tested on the&#10;following Linux distributions for AMD Opteron systems (x86_64).9117t1.qrk" border="1"><colgroup><col><col></colgroup><thead><tr><th>
Linux Release</th><th>Version Tested</th></tr></thead><tbody><tr><td>Red Hat Enterprise Linux 4</td><td>2.6.9</td></tr><tr><td>CentOS 4.0-4.2 (Rocks 4.0-4.2)</td><td>2.6.9</td></tr><tr><td>Red Hat Fedora Core 3</td><td>2.6.11, 2.6.12</td></tr><tr><td>Red Hat Fedora Core 4</td><td>2.6.12, 2.6.13, 2.6.14</td></tr><tr><td>SUSE Professional 9.3</td><td>2.6.11</td></tr><tr><td>SUSE Professional 10.0</td><td>2.6.13</td></tr></tbody></table></div><p>
The optimized ipath_ether Ethernet driver provides high-performance
networking support for existing TCP- and UDP-based applications (in
addition to other protocols using Ethernet), with no modifications
required to the application.  The OpenIB (Open Fabrics) driver provides
complete InfiniBand and OpenIB compatibility.  This software stack is
freely available as a download on their Web site.  It currently supports
IP over IB, verbs, MVAPICH and SDP (Sockets Direct Protocol).
</p><p>
PathScale offers a trial program&mdash;you can compile and run your
application on its 32-node cluster to see what performance gains you
can attain.  See <a href="http://www.pathscale.com/cbc.php" target="_self">www.pathscale.com/cbc.php</a>.
</p><p>
In addition, you can test your applications on the Emerald cluster at
the AMD Developer Center, which offers 144 dual-socket, dual-core nodes,
for a total of 576 2.2GHz Opteron CPUs connected with InfiniPath HTX
adapters and a SilverStorm InfiniBand switch.
</p><p>
Tests performed on this cluster have shown excellent scalability at more
than
500 processors, including the LS-Dyna three-vehicle collision results
posted at <a href="http://www.topcrunch.org" target="_self">www.topcrunch.org</a>.  See
Table 2 for a listing of the top 40 results of the benchmark.  Notice
that the only other cluster in the top ten is the much more expensive per
node Cray XD1 system.
</p><div class="table"><a name="N0x282a580.0x29235d8"></a><p class="title"><b>
Table 2.  LS-Dyna Three-Vehicle Collision Results, Posted at
<a href="http://www.topcrunch.org" target="_self">www.topcrunch.org</a></b></p><table     summary="&#10;Table 2.  LS-Dyna Three-Vehicle Collision Results, Posted at&#10;9117t2.qrk" border="1"><colgroup><col><col><col><col><col></colgroup><thead><tr><th>Result (lower is better)</th><th>Manufacturer</th><th>Cluster Name</th><th>Processors</th><th>Nodes x CPUs x Cores</th></tr></thead><tbody><tr><td>184</td><td>Cray, Inc.</td><td>CRAY XDI/RapidArray</td><td>AMD dual-core Opteron 2.2GHZ</td><td>64 x 2 x 2 = 256</td></tr><tr><td>226</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron2.2GHz</td><td>64 x 2 x 1 = 128</td></tr><tr><td>239</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>32 x 2 x 2 = 128</td></tr><tr><td>239</td><td>Rackable Systems/AMD Emerald/PathScale</td><td>InfiniPath/SilverStorm InfiniBand switch</td><td>AMD dual-core Opteron 2.2GHz</td><td>64 x 2 x 2 = 256</td></tr><tr><td>244</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD Opteron 2.4GHz</td><td>64 x 2 x 1 = 128</td></tr><tr><td>258</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>48 x 2 x 1 = 96</td></tr><tr><td>258</td><td>Rackable Systems/AMD Emerald/PathScale</td><td>Infiniband/SilverStorm InfiniBand switch</td><td>AMD dual-core Opteron 2.2GHz</td><td>64 x 1 x 2 = 128</td></tr><tr><td>268</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD Opteron 2.4GHz</td><td>48 x 2 x 1 = 96</td></tr><tr><td>268</td><td>Rackable Systems/AMD Emerald/PathScale</td><td>InfiniPath/SilverStorm InfiniBand switch</td><td>AMD dual-core Opteron 2.2GHz</td><td>32 x 2 x 2 = 128</td></tr><tr><td>280</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>24 x 2 x 2 = 96</td></tr><tr><td>294</td><td>Rackable Systems/AMD Emerald/PathScale</td><td>InfiniPath/SilverStorm InfiniBand switch</td><td>AMD dual-core Opteron 2.2GHz</td><td>48 x 1 x 2 = 96</td></tr><tr><td>310</td><td>Galactic Computing (Shenzhen) Ltd.</td><td>GT4000/InfiniBand</td><td>Intel Xeon 3.6GHz</td><td>64 x 2 x 1 = 128</td></tr><tr><td>315</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>327</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD Opteron 2.4GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>342</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>16 x 2 x 2 = 64</td></tr><tr><td>373</td><td>Rackable Systems/AMD Emerald/PathScale</td><td>InfiniPath/SilverStorm InfiniBand switch</td><td>AMD dual-core Opteron 2.2GHz</td><td>32 x 1 x 2 = 64</td></tr><tr><td>380</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD Opteron 2.2GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>384</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>24 x 2 x 1 = 48</td></tr><tr><td>394</td><td>Rackable Systems/AMD Emerald/PathScale</td><td>InfiniPath/SilverStorm InfiniBand switch</td><td>AMD dual-core Opteron 2.2GHz</td><td>16 x 2 x 2 = 64</td></tr><tr><td>399</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD Opteron 2.4GHz</td><td>24 x 2 x 1 = 48</td></tr><tr><td>405</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD Opteron 2.2GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>417</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>12 x 2 x 2 = 48</td></tr><tr><td>418</td><td>Galactic Computing (Shenzhen) Ltd.</td><td>GT4000/InfiniBand</td><td>Intel Xeon 3.6GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>421</td><td>HP</td><td>Itanium 2 CP6000/InfiniBand TopSpin</td><td>Intel Itanium 2 1.5GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>429</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD Opteron 2.2GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>452</td><td>IBM</td><td>e326/Myrinet</td><td>AMD Opteron 2.4GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>455</td><td>Cray, Inc.</td><td>CRAY XD1 RapidArray</td><td>AMD Opteron 2.2GHz</td><td>24 x 2 x 1 = 48</td></tr><tr><td>456</td><td>HP</td><td>Itanium 2 Cluster/InfiniBand</td><td>Intel Itanium 2 1.5GHz</td><td>32 x 2 x 1 = 64</td></tr><tr><td>480</td><td>PathScale, Inc.</td><td>Microway Navion/PathScale InfiniPath/SilverStorm IB switch</td><td>AMD Opteron 2.6GHz</td><td>16 x 2 x 1 = 32</td></tr><tr><td>492</td><td>Appro/Level 5 Networks</td><td>1122Hi-81/Level 5 Networks - 1Gb Ethernet NIC</td><td>AMD dual-core Opteron 2.2GHz</td><td>16 x 2 x 2 = 64</td></tr><tr><td>519</td><td>HP</td><td>Itanium 2 CP6000/InfiniBand TopSpin</td><td>Intel Itanium 2 1.5GHz</td><td>24 x 2 x 1 = 48</td></tr><tr><td>527</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>16 x 2 x 1 = 32</td></tr><tr><td>529</td><td>HP</td><td>Opteron CP4000/TopSpin InfiniBand</td><td>AMD Opteron 2.6GHz</td><td>16 x 2 x 1 = 32</td></tr><tr><td>541</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD Opteron 2.4GHz</td><td>16 x 2 x 1 = 32</td></tr><tr><td>569</td><td>Cray, Inc.</td><td>CRAY XD1/RapidArray</td><td>AMD dual-core Opteron 2.2GHz</td><td>8 x 2 x 2 = 32</td></tr><tr><td>570</td><td>HP</td><td>Itanium 2 Cluster/InfiniBand</td><td>Intel Itanium 2 1.5GHz</td><td>24 x 2 x 1 = 48</td></tr><tr><td>584</td><td>Appro/Rackable/Verari</td><td>Rackable and Verari Opteron Cluster/InfiniCon InfiniBand</td><td>AMD Opteron 2GHz</td><td>64 x 1 x 1 = 64</td></tr><tr><td>586</td><td>IBM</td><td>e326/Myrinet</td><td>AMD Opteron 2.4GHz</td><td>16 x 2 x 1 = 32</td></tr><tr><td>591</td><td>Self-made (SKIF program)/United Institute of Informatics
Problems</td><td>Minsk Opteron Cluster/InfiniBand</td><td>AMD Opteron 2.2GHz (248)</td><td>35 x 1 x 1 = 35</td></tr></tbody></table></div></div></div>
<div class="authorblurb"><p>
Logan Harbaugh is a freelance reviewer and IT consultant located in
Redding, California.  He has been working in IT for 20 years and has
written two
books on networking, as well as articles for most of the major computer
publications.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../149/toc149.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>