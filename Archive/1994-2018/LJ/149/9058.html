<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Getting Started with Condor</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;How to try Condor for multiplatform distributed computing.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x2969580.0x2a60ab0"></a>
Getting Started with Condor</h1></div><div><div class="author"><h3 class="author">
Irfan
 
Habib
</h3></div><div class="issuemoyr">Issue #149, September 2006</div></div><div><p>
How to try Condor for multiplatform distributed computing.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2969580.0x2a612f0"></a></h2></div></div><p>
Cluster computing emerged in the early 1990s when hardware prices were
dropping and PCs were becoming more and more powerful.  Companies were
shifting from large mini-computers to small and powerful micro-computers, and many people
realized that this would lead to a large-scale
waste of computing power, as computing resources were being
fragmented more and more.  Organizations today have hundreds to thousands of
PCs in their offices.  Many of them are idle most of the time.  However, the
same organizations also face huge computation-intensive problems and thus require great
computing power to remain competitive&mdash;hence the stable demand for supercomputing
solutions that largely are built on cluster computing concepts.
</p><p>
Many vendors offer commercial cluster computing solutions.  By using
free and open-source software, it is possible to forego the purchase
of these expensive commercial cluster computing solutions and set up
your own cluster.  This article describes such a solution,
developed by University of Wisconsin, called Condor.
</p><p>
The idea behind Condor is simple.  Install it on every machine you
want to make part of the cluster.  (In Condor terminology, a Condor cluster
is called a pool.  This article uses both terms interchangeably.) You
can launch jobs from any machine, and Condor matches the requirements of the job
with the capabilities offered by the idle computers currently available.
Once it finds a suitable idle machine, it transfers the job to it,
executes it and retrieves the results of the execution.  One of the
features of Condor is that it doesn't require programs to be modified
to run on the cluster.
</p><p>
In practice, however, Condor is more complicated.  Condor is installed
in different configurations on each machine.  Each Condor pool has a
central manager.  The central manager, as the name implies, is the central
manager of the cluster.  It manages the detection of new idle machines
and coordinates the matchmaking between job requirements and
available resources.  Machines in a Condor pool also can have Submit
and Full Install configurations.  Submit machines are those machines that
can only submit jobs, but can't run any jobs; Full Install machines are
machines that can do both, submit and execute.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2969580.0x2a61500"></a>
Requirements and Installation</h2></div></div><p>
Condor does not require the addition of any new hardware to the network;
the existing network itself is sufficient.  Condor runs on a variety
of operating systems, including Linux, Solaris, Digital Unix, AIX, HP-UX and Mac OS X as well as MS Windows 2000 and XP.  It supports
various architectures, including Intel x86, PowerPC, SPARC and so on.  However,
jobs developed on one specific architecture, such as Intel x86, will
run only on Intel x86 computers.  So, it is best if all the computers in
a Condor pool are of a single architecture.  It is possible, however, for
Java applications to run on different architectures.
</p><p>
In this article, we cover the installation from basic tarballs
on Linux, although distribution/OS-specific packages also
may be available from the official site or sources.  (See the Condor Project site for more
details,
<a href="http://www.cs.wisc.edu/condor/downloads" target="_self">www.cs.wisc.edu/condor/downloads</a>.)
</p><p>
Download the tarball from the Project site, and uncompress it with:

<pre     class="programlisting">
tar -zvf condor.tar.gz
</pre>
</p><p>
The condor_install script, located in the sbin directory, is all you need
to run to set up Condor on a machine.  Before you run this script, add a
user named condor.  For security reasons, Condor does not allow you to run jobs
as root; thus, it is advisable to make a new user to protect the system.
</p><p>
One of the first questions the script asks is how many machines are
you setting up to be part of the pool? This is important if you have a
shared filesystem.  If you do, the installation script will prompt
you for the names of those machines, and the installation of Condor on
those machines will be handled by the software itself.  If a shared filesystem does not exist,
you have to install Condor manually on each
system.  Also, if you want to be able to use Java support, you
need to have Sun's Java virtual machine installed prior to installing
Condor.  The install script provides plenty of help and annotation on
each question it asks, and you always can turn to Condor's
comprehensive user manual and its associated mailing lists for help.
</p><p>
The variable $CONDOR is used from now on to denote the root path where
condor has been installed (untarred).
</p><p>
After the installation, start Condor by running:

<pre     class="programlisting">
$CONDOR/bin/condor_master
</pre>
</p><p>
This command should spawn all other processes that Condor requires.  On the
central manager, you should be able to see five condor_ processes running after entering:

<pre     class="programlisting">
ps -aux | grep condor
</pre>
</p><p>
On the central manager machine, you should have the following processes:
</p><div class="itemizedlist"><ul type="disc"><li><p>
condor_master
</p></li><li><p>
condor_collector
</p></li><li><p>
condor_negotiator
</p></li><li><p>
condor_startd
</p></li><li><p>
condor_schedd
</p></li></ul></div><p>
All other machines in the pool should have processes for the following:
</p><div class="itemizedlist"><ul type="disc"><li><p>
condor_master
</p></li><li><p>
condor_startd
</p></li><li><p>
condor_schedd
</p></li></ul></div><p>
And, on submit-only machines you will see:
</p><div class="itemizedlist"><ul type="disc"><li><p>
condor_master
</p></li><li><p>
condor_schedd
</p></li></ul></div><p>
After that, you should be able to see the central manager machine as part
of your Condor cluster when you run <tt  >condor_status</tt>:

<pre     class="programlisting">
$CONDOR/bin/condor_status
Name OpSys Arch State Activity LoadAv Mem ActvtyTime Mycluster
LINUX INTEL Unclaimed Idle 0.115 3567 0+00:40:04
Machines Owner Claimed Unclaimed Matched Preempting
INTEL/LINUX 1    0    0    1    0    0
Total 1   0   0   1   0   0
</pre>
</p><p>
If you now run <tt  >condor_master</tt> on the other machines in the pool, you should
see that they are added to this list within a few minutes (usually around
five minutes).
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2969580.0x2a62478"></a>
Launching Jobs on Your New Cluster</h2></div></div><p>
To test our new condor setup, let's create a simple &ldquo;Hello Condor&rdquo; job:

<pre     class="programlisting">
#include
int main()
{ printf("Hello World!\n");}
</pre>
</p><p>
Compile the application with gcc.
</p><p>
Now, to submit a job to Condor, we need to write a submit file.  A submit
file describes what Condor needs to do with the job&mdash;that is, where it
will get the input for the application, where to produce the output
and if any errors occur, where it should store them:

<pre     class="programlisting">
Universe = Vanilla
Executable = hello
Output = hello.out
Input = hello.in
Error = hello.err
Log = hello.log
Queue
</pre>
</p><p>
The first Universe entry defines the runtime environment under which
Condor should run the job.  Two Universes are noteworthy: for long jobs,
such as those that will last for weeks and months, the Standard Universe is recommended,
as it ensures reliability and the ability to save partial execution
state and relocate the job to another machine automatically if the first
machine crashes.  This saves a lot of vital processing effort.  However, to
use the Standard Universe, the application must be &ldquo;condor compiled&rdquo;, and
the source code is required.  The Vanilla Universe is for jobs that are
short-lived, but long jobs also can be executed if the stability
of the machines is guaranteed.  Vanilla jobs can run unmodified binaries.
</p><p>
Other Universes in Condor include PVM, MPI and Java, for PVM,
MPI and Java applications, respectively.  For more detail
on Condor Universes consult the documentation.
</p><p>
In this example, our executable file is called hello (the traditional
&ldquo;Hello Condor&rdquo; program), and we're using the Vanilla Universe.  The Input,
Output, Error and Log directives tell Condor which files to use for
stdin, stdout and stderr and to log the job's execution.  Finally,
the Queue directive specifies how many copies of the program to run.
</p><p>
After you have the submit file ready, run <tt  >condor_submit hello.sub</tt> to submit it to Condor.  You can check on the status of your job
using <tt  >condor_q</tt>, which will tell you how many jobs are in the queue, their
IDs and whether they're running or idle, along with some statistics.
</p><p>
Condor has many other features; so far we have covered only
the basics of getting it up and running.  A
number of tutorials are available on-line, along with the Condor Manual
(<a href="http://www.cs.wisc.edu/condor/manual" target="_self">www.cs.wisc.edu/condor/manual</a>), that will teach you the basic and
advanced capabilities of Condor.  When reading the Condor Manual,
pay particular attention to the Standard Universe, which allows you to
checkpoint your job, and the Java Universe, which allows you to run Java
jobs seamlessly.
</p><p>
You also can add Condor to the boot sequence of your central manager
and other machines.  You can shut down cluster machines, and their jobs
will continue or restart on a different machine (depending on whether it's
a Standard Universe job or a Vanilla job).  This allows for a lot of
flexibility in managing a system.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2969580.0x2a62b00"></a>
Beyond Clusters</h2></div></div><p>
Condor is not only about clusters.  An extension to Condor allows jobs
submitted within one pool of machines to execute on another (separate)
Condor pool.  Condor calls this flocking.  If a machine within the pool
where a job is submitted is not available to run the job, the job makes
its way to another pool.  This is enabled by special configuration of
the pools.
</p><p>
The simplest flocking configuration sets a few configuration variables
in the condor_config file.  For example, let's set up an environment where
we have two clusters, A and B, and we want jobs
submitted in A to be executed in B.  Let's say cluster A
has its central manager at a.condor.org and B at b.condor.org.  Here's the sample
configuration:

<pre     class="programlisting">
FLOCK_TO = b.condor.org
FLOCK_COLLECTOR_HOSTS = $(FLOCK_TO)
FLOCK_NEGOTIATOR_HOSTS = $(FLOCK_TO)
</pre>
</p><p>
The FLOCK_TO variable can specify multiple pools, by entering a comma-separated list of central managers.  The other two variables usually
point to the same settings that FLOCK_TO does.  The configuration macros
that must be set in pool B authorize jobs from pool A to
flock to pool B.  The following is a sample of configuration macros that
allows the flocking of jobs from A to B.  As in the FLOCK_TO field,
FLOCK_FROM allows users to authorize the flocking of incoming
jobs from specific pools:

<pre     class="programlisting">
FLOCK_FROM=a.condor.org
HOSTALLOW_WRITE_COLLECTOR = $(HOSTALLOW_WRITE), $(FLOCK_FROM)
HOSTALLOW_WRITE_STARTD = $(HOSTALLOW_WRITE), $(FLOCK_FROM)
HOSTALLOW_READ_COLLECTOR = $(HOSTALLOW_READ), $(FLOCK_FROM)
HOSTALLOW_READ_STARTD = $(HOSTALLOW_READ), $(FLOCK_FROM)
</pre>
</p><p>
The above settings set flocking from pool A to pool B, but
not the reverse.  To enable flocking in both directions, each direction
needs to be considered separately.  That is, in pool B you would need to
set the FLOCK_TO, FLOCK_COLLECTOR_HOSTS and FLOCK_NEGOTIATOR_HOST to point
to pool A, and set up the authorization macros in pool A for B.
</p><p>
Be careful with HOSTALLOW_WRITE and HOSTALLOW_READ.  These
settings let you define the hosts that are allowed to join your
pool, or those that can view the status of your pool but are not allowed
to
join it, respectively.
</p><p>
Condor provides flexible ways to define the hosts.  It is possible,
for example, to allow read access only to the hosts that belong to a
specific subnet, like this:

<pre     class="programlisting">
HOSTALLOW_READ=127.6.45.*
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2969580.0x2e5b118"></a>
Condor-G</h2></div></div><p>
Another way to link distributed Condor pools together is by using
Condor's grid computing features, which utilize the Globus Toolkit
(<a href="http://www.globus.org" target="_self">www.globus.org</a>).  The Globus Toolkit is an open-source software
toolkit used for building Grid systems and applications.  It provides an
infrastructure for authentication, authorization and remote job
submission (including data transfer) on Grid resources.  Condor-G, an
extension of Condor, provides all of Condor's job submission features,
but for far-removed resources on the Grid.
</p><p>
Condor-G is sort of a gateway to the Grid for Condor pools.  Condor-G is
a program that manages both a queue of jobs and the resources from one
or more sites where those jobs can execute.  It communicates with these
resources and transfers files to and from these resources using the Globus
mechanisms.  For more detail on setting up Condor-G, consult the
Condor Manual mentioned previously.
</p><p>
A sample submit file for a job to be executed over Globus looks like this:

<pre     class="programlisting">
executable = mygridjob
globusscheduler = grid.sample.net/jobmanager
input=mygridi.txt
universe = globus
output = mygridjob.out
log = mygridjob.log

queue
</pre>
</p><p>
As you can see, there are only two differences with Grid jobs and normal
local pool jobs.  The Universe is Globus, which tells Condor that this job
will be scheduled to the Grid.  And, we specify the globusscheduler, which
points to the Globus Job manager at the remote site.  The jobmanager is the
Globus service that is spawned at the remote site to submit, keep track of
and manage Grid I/O for jobs running on the local system there.  Grid jobs
can be monitored the same way as ordinary Condor jobs with condor_q.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2969580.0x2e5b3d8"></a>
Conclusion</h2></div></div><p>
Condor provides the unique possibility of using our current
computing infrastructure and investments to target processing of
jobs that are simply beyond the capabilities of our most powerful
systems.  Condor is easy-to-install and easy-to-use software for setting up
clusters.  Condor is scalable.  It provides options to extend its reach
from a single cluster to interconnecting clusters that can be located anywhere
in the world.  Condor has been fundamental software for many grid computing
projects.  Various success stories with Condor have been reported in the
press.  One of the recent ones is of Micron Technologies.  Micron is one
of the world's leading providers of advanced semiconductor solutions.  In
an interview with <span   class="emphasis"><em>GridToday</em></span> in April 2006, a senior fellow at Micron said
that they had deployed 11 Condor pools consisting of 11,000 processors,
located in four countries in seven different sites.  Why Condor? Because it
supported all the platforms Micron was interested in, and it was already
widely used, well supported, and of course, it was open source.  These
pools have become a vital asset for Micron.  They are used for everything
from manufacturing, engineering, reporting and software development to
security.  Condor is not only a research toy, but also a piece
of robust open-source software that solves real-world problems.
</p></div></div>
<div class="authorblurb"><p>
Irfan Habib is an undergraduate student in software engineering at the National
University of Sciences Technology Pakistan.  He has been deeply interested in free
and open-source software for years, and he does research in Distributed
and Grid Computing.  Condor combines both of his interests.
He can be reached at <a href="mailto:irfan.habib@niit.edu.pk">irfan.habib@niit.edu.pk</a>.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../149/toc149.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>