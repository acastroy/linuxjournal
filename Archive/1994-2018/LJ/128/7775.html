<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>At the Forge</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="So far, we have looked at ways in which people might create RSS and&#10;      Atom feeds for a Web site.  Of course, creating syndication feeds&#10;      is only one half of the equation. Equally as important and&#10;      perhaps even more useful is understanding how we can retrieve and use&#10;      syndication feeds, both from our own sites and from other sites of&#10;interest.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x192a580.0x1a21ab0"></a>At the Forge</h1></div><div><h3 class="subtitle"><i>
Aggregating Syndication Feeds</i></h3></div><div><div class="author"><h3 class="author">
Reuven
 M. 
Lerner
</h3></div><div class="issuemoyr">Issue #128, December 2004</div></div><div><p>So far, we have looked at ways in which people might create RSS and
      Atom feeds for a Web site.  Of course, creating syndication feeds
      is only one half of the equation. Equally as important and
      perhaps even more useful is understanding how we can retrieve and use
      syndication feeds, both from our own sites and from other sites of
interest.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x192a580.0x1a223f8"></a></h2></div></div><p>
Over the last few months, we have looked at RSS
and Atom, two XML-based file formats that make it
easy to create and distribute summaries of a Web
site. Although such syndication, as it is known,
traditionally is associated with Weblogs and news sites,
there is growing interest in its potential for other
uses.  Any Web-based information source is a potentially
interesting and useful candidate for either RSS
or Atom.
</p><p>
So far, we have looked at ways in which people might create RSS and
Atom feeds for a Web site.  Of course, creating syndication feeds
is only one half of the equation. Equally as important and
perhaps even more useful is understanding how we can retrieve and use
syndication feeds, both from our own sites and from other sites of
interest.
</p><p>
As we have seen, three different types of
syndication feeds exist: RSS 0.9x and its more modern version, RSS 2.0;
the incompatible RSS 1.0; and Atom.  Each does
roughly the same thing, and there is a fair amount of overlap among
these standards.  But networking protocols do not work well when we
assume that everything is good enough or close enough, and
syndication is no exception. If we want to read all of the syndicated
sites, then we need to understand all of the different protocols,
as well as versions of those protocols. For example, there actually are
nine different versions of RSS, which when combined with Atom, brings
us to a total of ten different syndication formats that a site might be using.  Most of the
differences probably are negligible, but it would be foolish to ignore
them completely or to assume that everyone is using the latest
version.  Ideally, we would have a module or tool that allows us to
retrieve feeds from a variety of different protocols, papering over
the differences as much as possible while still taking advantage of
each protocol's individual power.
</p><p>
This month, we look at the Universal Feed Parser, an open-source
solution to this problem written by Mark Pilgrim.  Pilgrim is a
well-known Weblog author and Python programmer, and he also was one of
the key people involved in the creation of the Atom syndication
format.  This should come as no surprise, given the pain that he
experienced in writing the Universal Feed Parser.  It also handles
CDF, a proprietary Microsoft format used for the publication of such
items as active desktop and software updates. This part might not be
of interest to Linux desktop users, but it raises interesting
possibilities for organizations with Microsoft systems installed.
The Universal Feed Parser (feedparser), in version 3.3 as of this
writing, appears to be the best tool of its kind, in any language, and
regardless of licensing.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x192a580.0x1a22608"></a>
Installing feedparser</h2></div></div><p>
Installing feedparser is extremely simple.  Download the latest
version, move into its distribution directory and type <tt  >python
setup.py install</tt>.  This activates Python's standard installation
utility, placing the feedparser in your Python site-packages
directory. Once you have done installed feedparser, you can test it using
Python interactively, from a shell window:

<pre     class="programlisting">

&gt;&gt;&gt; import feedparser

</pre>
</p><p>
The &gt;&gt;&gt; symbols are Python's standard prompt when working
in interactive mode.  The above imports the feedparser module into
Python.  If you have not installed feedparser, or if something went
wrong with the installation, executing this command results in a
Python ImportError.
</p><p>
Now that we have imported our module into memory, let's use it to
look at the latest news from <i  >Linux Journal</i>'s Web site.  We type:


<pre     class="programlisting">
&gt;&gt;&gt; ljfeed = feedparser.parse
&#8618;("http://www.linuxjournal.com/news.rss")
</pre>
</p><p>
We do not have to indicate the protocol or version
of the feed we are asking feedparser to work
with&mdash;the package is smart enough to determine such
versioning on its own, even when the RSS feed fails
to identify its version.  At the time of writing,
the <i  >LJ</i> site is powered by PHPNuke and the feed is
identified explicitly as RSS 0.91.
</p><p>
Now that we have retrieved a new feed, we can find out exactly how many
entries we received, which largely is determined by the configuration
of the server:

<pre     class="programlisting">
&gt;&gt;&gt; len(ljfeed.entries)
</pre>
</p><p>
Of course, the number of items is less interesting than the items
themselves, which we can see with a simple for loop:

<pre     class="programlisting">
&gt;&gt;&gt; for entry in ljfeed.entries:
...     print entry['title']
...
</pre>
</p><p>
Remember to indent the print statement to tell
Python that it's part of the loop.  If you are new
to Python, you might be surprised by the lines that
begin with ... and indicate that Python is ready and
waiting for input after the for.  Simply press
&lt;Enter&gt; to conclude the block begun by for, and you can
see the latest titles.
</p><p>
We also can get fancy, looking at a combination of URL and title,
using Python's string interpolation:

<pre     class="programlisting">

&gt;&gt;&gt; for entry in ljfeed.entries:
...     print '&lt;a href="%s"&gt;%s&lt;/a&gt;' % \
...     (entry['link'], entry['title'])

</pre>
</p><p>
As I indicated above, feedparser tries to paper over the differences
between different protocols, allowing us to work with all syndicated
content as if it were roughly equivalent.  I thus can repeat the
above commands with the syndication feed from my Weblog. I recently
moved to WordPress, which provides an Atom feed:

<pre     class="programlisting">

&gt;&gt;&gt; altneufeed = feedparser.parse(
... "http://altneuland.lerner.co.il/wp-atom.php")
&gt;&gt;&gt; for entry in altneufeed.entries:
...     print '&lt;a href="%s"&gt;%s&lt;/a&gt;' % \
...     (entry.link, entry.title)

</pre>
</p><p>
Notice how this last example uses attributes entry.link and
entry.title, while the previous example uses dictionary keys
entry['link'] and entry['title'].  feedparser tries to be flexible,
providing several interfaces to the same information to suit different
needs and styles.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x192a580.0x1a22d40"></a>
How New Is that News?</h2></div></div><p>
The point of a news aggregator or other application that uses RSS and
Atom is to collect and present newly updated information.
An aggregator can show only the items that a server provides; if an
RSS feed includes only the two most recently published items, then it
becomes the aggregator's responsibility to poll, cache and display
those items no longer being syndicated and summarized.
</p><p>
This raises two different but related questions: How can we ensure
that our aggregator displays only items we have not seen yet?
And is there a way for our aggregator to reduce the load on Weblog
servers, retrieving only those items that were published since our
last visit?  Answering the first question requires looking at the
modification date, if it exists, for each item.
</p><p>
The latter question has, as of this writing, been an increasingly
popular issue of debate in the Web community. As a Weblog grows in
popularity, the number of people who subscribe to its syndication
feed also grows.  If a Weblog has 500 subscribers to its
syndication feed, and if each of these subscribers' aggregators look
for updates each hour, that means an additional 500 requests per hour
are made against a Web server.  If the syndication feed
provides the site's entire content, this can result in a great deal of
wasted bandwidth&mdash;reducing the site's response time for other
visitors and potentially forcing the site owner to pay for exceeding
allocated monthly bandwidth.
</p><p>
feedparser allows us to be kind to syndicating servers and ourselves
by providing a mechanism for retrieving a syndication feed
only when there is something new to show.  This is possible because
modern versions of HTTP allow the requesting client to include an
If-Modified-Since header, followed by a date.  If the requested URL
has changed since the date mentioned in the request, the server
responds with the URL's content.  But if the requested URL is
unchanged, the server returns a 304 response code, indicating that the
previously downloaded version remains the most current content.
</p><p>
We accomplish this by passing an optional modified parameter to our
call to feedparser.parse().  This parameter is a standard, as defined
by the time module, Python tuple in which the first six elements are
the year, month number, day number, hour, minutes and seconds. The
final three items don't concern us, and can be left as zeroes.  So if
I were interested in seeing feeds posted since September 1, 2004, I could
say:

<pre     class="programlisting">
last_retrieval = (2004, 9, 1, 0, 0, 0, 0, 0, 0)
ljfeed = feedparser.parse(
         "http://www.linuxjournal.com/news.rss")
</pre>
</p><p>
If <i  >Linux Journal</i>'s server is configured well, the above code
either results in ljfeed containing the complete syndication feed&mdash;returned
with an HTTP OK status message, with a numeric code of
200--or an indication that the feed has not changed since its last
retrieval, with a numeric code of 304. Although keeping track of the
last time you requested a particular syndication feed might require
more record-keeping on your part, it is important to do, especially if
you requestfeed updates on a regular basis.  Otherwise,
you might find your application unwelcome at certain sites.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x192a580.0x1a230b0"></a>
Working with Feeds</h2></div></div><p>
Now that we have a basic idea of how to work with feedparser, let's
create a simple aggregation tool.  This tool gets its input from a
file called feeds.txt and produces its output in the form of
an HTML file called feeds.html.  Running this program by cron
and looking at the resulting HTML file once per day provides a
crude-but-working news feed from the sites that most interest you.
</p><p>
Feeds.txt contains URLs of actual feeds rather than of
the sites from which we would like to get the feed.  In other words,
it's up to the user to find and enter the URL for each feed.  More
sophisticated aggregation tools usually are able to determine the
feed's URL from a link tag in the header of the site's home page.
</p><p>
Also, despite my above warning that every news aggregator
should keep track of its most recent request so as not to overwhelm
servers, this program leaves out such features as part of my attempt
to keep it small and readable.
</p><p>
The program, aggregator.py, can be read in Listing 1 and is
divided into four parts:
</p><div class="orderedlist"><ol type="1"><li><p>
We first open the output file, which is an HTML-formatted
text file called myfeeds.html. The file is designed to be used
from within a Web browser. If you are so inclined, you could add
this local file, which has a file:/// URL, to your list of
personal bookmarks or even make it your startup page.  After
making sure that we indeed can write to this file, we start the
HTML file.
</p></li><li><p>
We then read the contents of feeds.txt, which contains one
feed URL per line.  In order to avoid
problems with whitespace or blank lines, we strip off the
whitespace and ignore any line without at least one printable
character.
</p></li><li><p>
Next, we iterate over the list of feeds, feeds_list, invoking
feedparser.parse() on that URL.  When we receive a response, we
write it to the output file, myfeeds.html, with both the URL and
the title of the article.
</p></li><li><p>
Finally, we close the HTML and the file.
</p></li></ol></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x192a580.0x1a235d8"></a></h2></div></div><div class="sidebar"><p class="title"><b>Listing 1. aggregator.py</b></p><pre     class="programlisting">

#!/usr/bin/python

import feedparser
import sys

# ---------------------------------------------------
# Open the personal feeds output file

aggregation_filename = "myfeeds.html"
max_title_chars = 60

try:
    aggregation_file = open(aggregation_filename,"w")
    aggregation_file.write("""&lt;html&gt;
&lt;head&gt;&lt;title&gt;My news&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;""")
except IOError:
    print "Error: cannot write '%s' " % \
    aggregation_filename
    exit

# ---------------------------------------------------
# Each non-blank line in feeds.txt is a feed source.

feeds_filename = "feeds.txt"
feeds_list = []

try:
    feeds_file = open(feeds_filename, 'r')
    for line in feeds_file:
        stripped_line = line.strip().rstrip()

        if len(stripped_line) &gt; 0:
            feeds_list.append(stripped_line)
            sys.stderr.write("Adding feed '" + \
            stripped_line + "'\n")

    feeds_file.close()

except IOError:
    print "Error: cannot read '%s' " % feeds_filename
    exit

# ---------------------------------------------------
# Iterate over feeds_list, grabbing the feed for each

for feed_url in feeds_list:
    sys.stderr.write("Checking '%s'..." % feed_url)
    feed = feedparser.parse(feed_url)
    sys.stderr.write("done.\n")

    aggregation_file.write('&lt;h2&gt;%s&lt;/h2&gt;\n' % \
                           feed.entries[0].title)

    # Iterate over each entry from this feed,
    # displaying it and putting it in the summary
    for entry in feed.entries:
        sys.stderr.write("\tWrote: '%s'" % \
                      entry.title[0:max_title_chars])

        if len(entry.title) &gt; max_title_chars:
            sys.stderr.write("...")

        sys.stderr.write("\n")

        aggregation_file.write(
           '&lt;li&gt;&lt;a href="%s"&gt;%s&lt;/a&gt;\n' %
           (entry.link, entry.title))

    aggregation_file.write('&lt;/u2&gt;\n')

# ---------------------------------------------------
# Finish up with the HTML

aggregation_file.write("""&lt;/body&gt;
&lt;/html&gt;
""")
aggregation_file.close()


</pre></div><p>
As you can see from looking at the code listing, creating such a news
aggregator for personal use is fairly simple and straightforward.
This is merely a skeletal application, however. To be more useful
in the real world, we probably would want to move feeds.txt and
myfeeds.html into a relational database, determine the feed URL
automatically or semi-automatically based on a site URL and handle
categories of feeds, so that multiple feeds can be read as if they
were one.
</p><p>
If the above description sounds familiar, then you might be a user of
Bloglines.com, a Web-based blog aggregator that probably works in the
above way.  Obviously, Bloglines handles many more feeds and many more
users than we had in this simple toy example. But, if you are
interested in creating an internal version of Bloglines for your
organization, the combination of the Universal Feed Parser with a
relational database, such as PostgreSQL, and some personalization code
is both easy to implement and quite useful.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x192a580.0x1a23898"></a>
Conclusion</h2></div></div><p>
The tendency to reinvent the wheel often is cited as a widespread
problem in the computer industry.  Mark Pilgrim's Universal Feed
Parser might fill only a small need in the world of software, but that
need is almost certain to grow as the use of syndication increases
for individuals and organizations alike.  The bottom line is if
you are interested in reading and parsing syndication feeds, you
should be using feedparser.  It is heavily tested and documented,
often updated and improved and it does its job quickly and well.
</p></div></div>
<div class="authorblurb"><p>
Reuven M. Lerner, a longtime Web/database consultant and developer,
now is a graduate student in the Learning Sciences program at
Northwestern University. His Weblog is at <a href="http://altneuland.lerner.co.il" target="_self">altneuland.lerner.co.il</a>, and you can reach him at
<a href="mailto:reuven@lerner.co.il">reuven@lerner.co.il</a>.

</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../128/toc128.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>