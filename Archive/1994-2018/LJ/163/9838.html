<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Getting Started with Heartbeat
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Your first step toward high-availability bliss.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x2881580.0x2978ab0"></a>
Getting Started with Heartbeat
</h1></div><div><div class="author"><h3 class="author">
Daniel
 
Bartholomew
</h3></div><div class="issuemoyr">Issue #163, November 2007</div></div><div><p>
Your first step toward high-availability bliss.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2979240"></a></h2></div></div><p>
In every work environment with which I have been involved, certain servers
absolutely always must be up and running for the business to keep
functioning smoothly. These servers provide services that always need to
be
available&mdash;whether it be a database, DHCP, DNS, file, Web, firewall
or mail server.
</p><p>
A cornerstone of any service that always needs be up with no
downtime is being able to transfer the service from one system
to another gracefully. The magic that makes this happen on Linux is a service called
Heartbeat. Heartbeat is the main product of the High-Availability Linux
Project.
</p><p>
Heartbeat is very flexible and powerful. In this article, I 
touch on only basic active/passive clusters with two members, where the active
server is providing the services and the passive server is waiting to
take over if necessary.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x29794a8"></a>
Installing Heartbeat</h2></div></div><p>
Debian, Fedora,
Gentoo, Mandriva, Red Flag, SUSE, Ubuntu and others have prebuilt
packages in their repositories. Check your
distribution's main and supplemental repositories for a package named
heartbeat-2.
</p><p>
After installing a prebuilt package, you may see a &ldquo;Heartbeat
failure&rdquo; message. This is normal. After the Heartbeat package is
installed, the package manager is trying to start up the Heartbeat
service. However, the service does not have a valid configuration yet,
so the service fails to start and prints the error message.
</p><p>
You can install Heartbeat manually too. To get the most recent
stable version, compiling from source may be necessary. There are a
few dependencies, so to prepare on my Ubuntu systems, I first run the
following command:

<pre     class="programlisting">
sudo apt-get build-dep heartbeat-2
</pre>
</p><p>
Check the Linux-HA Web site for the complete list of dependencies. With
the dependencies out of the way, download the latest source tarball and untar it. Use
the ConfigureMe script to compile and install Heartbeat. This script
makes educated guesses from looking at your environment as to how 
best to configure and install Heartbeat. It also does everything with one
command, like so:

<pre     class="programlisting">
sudo ./ConfigureMe install
</pre>
</p><p>
With any luck, you'll walk away for a few minutes, and when you return,
Heartbeat will be compiled and installed on every node in your cluster.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2979818"></a>Configuring Heartbeat</h2></div></div><p>
Heartbeat has three main configuration files:
</p><div class="itemizedlist"><ul type="disc"><li><p>
/etc/ha.d/authkeys
</p></li><li><p>
/etc/ha.d/ha.cf
</p></li><li><p>
/etc/ha.d/haresources
</p></li></ul></div><p>
The authkeys file must be owned by root and be chmod 600. The actual
format of the authkeys file is very simple; it's only two lines. There is an
auth directive with an associated method ID number, and there is a
line that has the authentication method and the key that go with the ID
number of the auth directive. There are three supported authentication
methods: crc, md5 and sha1. Listing 1 shows an example. You can have more
than one authentication method ID, but this is useful only when you
are changing authentication methods or keys. Make the key long&mdash;it will
improve security and you don't have to type in the key ever again.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2979be0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 1. The /etc/ha.d/authkeys File</b></p><pre     class="programlisting">
auth 1
1 sha1 ThisIsAVeryLongAndBoringPassword
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2979df0"></a>
The ha.cf File</h2></div></div><p>
The next file to configure is the ha.cf file&mdash;the main Heartbeat
configuration file. The contents of this file should be the same on all
nodes with a couple of exceptions.
</p><p>
Heartbeat ships with a detailed example file in the documentation
directory that is well worth a look. Also, when creating your ha.cf file,
the order in which things appear matters. Don't move them around! Two
different example ha.cf files are shown in Listings 2 and 3.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2979f50"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 2. The /etc/ha.d/ha.cf File on Briggs &amp; Stratton</b></p><pre     class="programlisting">
keepalive 2
deadtime 32
warntime 16
initdead 64
baud 19200
# On briggs the serial device is /dev/ttyS1
# On stratton the serial device is /dev/ttyS0
serial /dev/ttyS1
auto_failback on
node briggs
node stratton
use_logd yes
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x297a160"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 3. The /etc/ha.d/ha.cf File on Deimos &amp; Phobos</b></p><pre     class="programlisting">
keepalive 1
deadtime 10
warntime 5
udpport 694
# deimos' heartbeat ip address is 192.168.1.11
# phobos' heartbeat ip address is 192.168.1.21
ucast eth1 192.168.1.11
auto_failback off
stonith_host deimos wti_nps ares.example.com erisIsTheKey
stonith_host phobos wti_nps ares.example.com erisIsTheKey
node deimos
node phobos
use_logd yes
</pre></div><p>
The first thing you need to specify is the keepalive&mdash;the time
between heartbeats in seconds. I generally like to have this set to one or
two, but servers under heavy loads might not be able to send heartbeats
in a timely manner. So, if you're seeing a lot of warnings about late
heartbeats, try increasing the keepalive.
</p><p>
The deadtime is next. This is the time to wait without hearing from
a cluster member before the surviving members of the array declare the
problem host as being dead.
</p><p>
Next comes the warntime. This setting determines how long to wait before
issuing a &ldquo;late heartbeat&rdquo; warning.
</p><p>
Sometimes, when all members of a cluster are booted at the same time,
there is a significant length of time between when Heartbeat is started
and before the network or serial interfaces are ready to send and
receive heartbeats. The optional initdead directive takes care of this
issue by setting an initial deadtime that applies only when Heartbeat
is first started.
</p><p>
You can send heartbeats over serial or Ethernet links&mdash;either works
fine. I like serial for two server clusters that are physically close
together, but Ethernet works just as well. The configuration for
serial ports is easy; simply specify the baud rate and then the
serial device you are using. The serial device is one place where the
ha.cf files on each node may differ due to the serial port having 
different names on each host. If you don't know the tty to which your serial port
is assigned, run the following command:

<pre     class="programlisting">
setserial -g /dev/ttyS*
</pre>
</p><p>
If anything in the output says &ldquo;UART: unknown&rdquo;, that device is not
a real serial port. If you have several serial ports, experiment to
find out which is the correct one.
</p><p>
If you decide to use Ethernet, you have several choices of how 
to configure things. For simple two-server clusters, ucast (uni-cast)
or bcast (broadcast) work well.
</p><p>
The format of the ucast line is: 

<pre     class="programlisting">

ucast &lt;device&gt; &lt;peer-ip-address&gt;

</pre>
</p><p>
Here is an example: 

<pre     class="programlisting">
ucast eth1 192.168.1.30
</pre>
</p><p>
If I am using a crossover cable to connect two hosts together, I 
just broadcast the heartbeat out of the appropriate interface. Here is
an example bcast line: 

<pre     class="programlisting">
bcast eth3
</pre>
</p><p>
There is also a more complicated method called mcast. This method
uses multicast to send out heartbeat messages. Check the Heartbeat
documentation for full details.
</p><p>
Now that we have Heartbeat transportation all sorted out, we can define
auto_failback. You can set auto_failback either to on or off. If set
to on and the primary node fails, the secondary node will
&ldquo;failback&rdquo;
to its secondary standby state when the primary node returns. If set to
off, when the primary node comes back, it will be the secondary.
</p><p>
It's a toss-up as to which one to use. My thinking is that so long as the
servers are identical, if my primary node fails, then the secondary node
becomes the primary, and when the prior primary comes back, it becomes the
secondary. However, if my secondary server is not as powerful a machine
as the primary, similar to how the spare tire in my car is not a
&ldquo;real&rdquo;
tire, I like the primary to become the primary again as soon as it
comes back.
</p><p>
Moving on, when Heartbeat thinks a node is dead, that is just a best
guess. The &ldquo;dead&rdquo; server may still be up. In some cases, if
the &ldquo;dead&rdquo;
server is still partially functional, the consequences are disastrous to
the other node members. Sometimes, there's only one way to be sure
whether a
node is dead, and that is to kill it. This is where STONITH comes
in.
</p><p>
STONITH stands for Shoot The Other Node In The Head. STONITH devices
are commonly some sort of network power-control device. To see the full
list of supported STONITH device types, use the <tt  >stonith
-L</tt> command, and
use <tt  >stonith -h</tt> to see how to configure them.
</p><p>
Next, in the ha.cf file, you need to list your nodes. List each one on its
own line, like so:

<pre     class="programlisting">
node deimos
node phobos
</pre>
</p><p>
The name you use must match the output of <tt  >uname
-n</tt>.
</p><p>
The last entry in my example ha.cf files is to turn on logging:

<pre     class="programlisting">
use_logd yes
</pre>
</p><p>
There are many other options that can't be touched on here. Check the
documentation for details.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2d731c8"></a>
The haresources File</h2></div></div><p>
The third configuration file is the haresources file. Before configuring
it, you need to do some housecleaning. Namely, all services that you
want Heartbeat to manage must be removed from the system init for all
init levels.
</p><p>
On Debian-style distributions, the command is:

<pre     class="programlisting">

/usr/sbin/update-rc.d -f &lt;service_name&gt; remove

</pre>
</p><p>
Check your distribution's documentation for how to do the same on
your nodes.
</p><p>
Now, you can put the services into the haresources file. As with the other
two configuration files for Heartbeat, this one probably won't be
very large. Similar to the authkeys file, the haresources file must be
<span   class="emphasis"><em>exactly</em></span> the same on every node. And, like the ha.cf file, position is
<span   class="emphasis"><em>very</em></span> important in this file. When control is
transferred to a node,
the resources listed in the haresources file are started left to right,
and when control is transfered to a different node, the resources are
stopped right to left. Here's the basic format:

<pre     class="programlisting">

&lt;node_name&gt; &lt;resource_1&gt; &lt;resource_2&gt; &lt;resource_3&gt; . . .

</pre>
</p><p>
The node_name is the node you want to be the primary on initial startup
of the cluster, and if you turned on auto_failback, this server always will
become the primary node whenever it is up. The node name must
match the name of one of the nodes listed in the ha.cf file.
</p><p>
Resources are scripts located either in /etc/ha.d/resource.d/ or
/etc/init.d/, and if you want to create your own resource scripts,
they should conform to LSB-style init scripts like those found in
/etc/init.d/. Some of the scripts in the resource.d folder can take
arguments, which you can pass using a :: on the resource line. For
example,
the IPAddr script sets the cluster IP address, which you specify like so:

<pre     class="programlisting">
IPAddr::192.168.1.9/24/eth0
</pre>
</p><p>
In the above example, the IPAddr resource is told to set up a cluster
IP address of 192.168.1.9 with a 24-bit subnet mask (255.255.255.0) and
to bind it to eth0. You can pass other options as well; check
the example haresources file that ships with Heartbeat for more
information.
</p><p>
Another common resource is Filesystem. This resource is for mounting
shared filesystems. Here is an example:

<pre     class="programlisting">
Filesystem::/dev/etherd/e1.0::/opt/data::xfs
</pre>
</p><p>
The arguments to the Filesystem resource in the example above are,
left to right, the device node (an ATA-over-Ethernet drive in this case),
a mountpoint (/opt/data) and the filesystem type (xfs).
</p><p>
For regular init scripts in /etc/init.d/, simply enter them by name. As
long as they can be started with <tt  >start</tt> and stopped
with <tt  >stop</tt>, there
is a good chance that they will work.
</p><p>
Listings 4 and 5 are haresources files for two of the clusters I run. They
are paired with the ha.cf files in Listings 2 and 3, respectively.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2d73900"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 4. A Minimalist haresources File</b></p><pre     class="programlisting">
stratton 192.168.1.41 apache2
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2d73b10"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 5. A More Substantial haresources File</b></p><pre     class="programlisting">
deimos \
        IPaddr::192.168.12.1 \
        Filesystem::/dev/etherd/e1.0::/opt/storage::xfs \
        killnfsd \
        nfs-common \
        nfs-kernel-server
</pre></div><p>
The cluster defined in Listings 2 and 4 is very simple, and it has only
two resources&mdash;a cluster IP address and the Apache 2 Web server. I use
this for my personal home Web server cluster. The servers themselves are
nothing special&mdash;an old PIII tower and a cast-off laptop. The content
on the servers is static HTML, and the content is kept in sync with an
hourly rsync cron job. I don't trust either &ldquo;server&rdquo; very much, but with
Heartbeat, I have never had an outage longer than half a second&mdash;not bad
for two old castaways.
</p><p>
The cluster defined in Listings 3 and 5 is a bit more complicated. This is
the NFS cluster I administer at work. This cluster utilizes shared storage
in the form of a pair of Coraid SR1521 ATA-over-Ethernet drive arrays,
two NFS appliances (also from Coraid) and a STONITH device. STONITH is
important for this cluster, because in the event of a failure, I need
to be sure that the other device is really dead before mounting the
shared storage on the other node. There are five resources managed in
this cluster, and to keep the line in haresources from getting too long
to be readable, I break it up with line-continuation slashes. If the
primary cluster member is having trouble, the secondary cluster
kills the primary, takes over the IP address, mounts the shared storage
and then starts up NFS. With this cluster, instead of having maintenance
issues
or other outages lasting several minutes to an hour (or more), outages
now don't last beyond a second or two. I can live with that.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2d73e28"></a>
Troubleshooting</h2></div></div><p>
Now that your cluster is all configured, start it with:

<pre     class="programlisting">
/etc/init.d/heartbeat start
</pre>
</p><p>
Things might work perfectly or not at all. Fortunately, with logging
enabled, troubleshooting is easy, because Heartbeat outputs informative log
messages. Heartbeat even will let you know when a previous log message
is not something you have to worry about. When bringing a new cluster
on-line, I usually open an SSH terminal to each cluster member and
tail the messages file like so:

<pre     class="programlisting">
tail -f /var/log/messages
</pre>
</p><p>
Then, in separate terminals, I start up Heartbeat. If there are any
problems, it is usually pretty easy to spot them.
</p><p>
Heartbeat also comes with very good documentation. Whenever I run
into problems, this documentation has been invaluable. On my system, it
is located under the /usr/share/doc/ directory.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2d74140"></a>
Conclusion</h2></div></div><p>
I've barely scratched the surface of Heartbeat's capabilities
here. Fortunately, a lot of resources exist to help you
learn about Heartbeat's more-advanced features. These include active/passive and active/active clusters with N number of nodes,
DRBD, the Cluster Resource Manager and more. Now that your feet are
wet, hopefully you won't be quite as intimidated as I was when I first
started learning about Heartbeat. Be careful though, or you might end
up like me and want to cluster everything.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2881580.0x2d74248"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
The High-Availability Linux Project: <a href="http://www.linux-ha.org" target="_self">www.linux-ha.org</a>
</p><p>
Heartbeat Home Page: <a href="http://www.linux-ha.org/Heartbeat" target="_self">www.linux-ha.org/Heartbeat</a>
</p><p>
Getting Started with Heartbeat Version 2: <a href="http://www.linux-ha.org/GettingStartedV2" target="_self">www.linux-ha.org/GettingStartedV2</a>
</p><p>
An Introductory Heartbeat Screencast: <a href="http://linux-ha.org/Education/Newbie/InstallHeartbeatScreencast" target="_self">linux-ha.org/Education/Newbie/InstallHeartbeatScreencast</a>
</p><p>
The Linux-HA Mailing List: <a href="http://lists.linux-ha.org/mailman/listinfo/linux-ha" target="_self">lists.linux-ha.org/mailman/listinfo/linux-ha</a>
</p></div></div></div>
<div class="authorblurb"><p>
Daniel Bartholomew has been using computers since the early 1980s when his
parents purchased an Apple IIe. After stints on Mac and Windows machines,
he discovered Linux in 1996 and has been using various distributions
ever since. He lives with his wife and children in North Carolina.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../163/toc163.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>