<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Linux System Administration</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Got that sinking feeling that often follows an overzealous&#10;    rm? Our system doctor has prescription.&#10;    "><meta name="keywords" content="command, administration, rm"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1228580.0x131fab0"></a>Linux System Administration</h1></div><div><div class="author"><h3 class="author">Mark Komarinski</h3></div><div class="issuemoyr">Issue #9, January 1995</div></div><div><p>
    Got that sinking feeling that often follows an overzealous
    rm? Our system doctor has prescription.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1228580.0x13203f8"></a></h2></div></div><p>There was recently a bit of traffic on
the Usenet newsgroups about the need for (or lack of) an undelete
command for Linux. If you were to type <b  >rm * tmp</b>
instead of <b  >rm *tmp</b> and such a command were
available, you could quickly recover your files.
</p><p>The main problem with this idea from a filesystem standpoint
involves the differences between the way DOS handles its
filesystems and the way Linux handles its filesystems.</p><p>Let's look at how DOS handles its filesystems. When DOS
writes a file to a hard drive (or a floppy drive) it begins by
finding the first block that is marked &ldquo;free&rdquo; in the File
Allocation Table (FAT). Data is written to that block, the next
free block is searched for and written to, and so on until the file
has been completely written. The problem with this approach is that
the file can be in blocks that are scattered all over the drive.
This scattering is known as <span   class="emphasis"><em>fragmentation</em></span> and
can seriously degrade your filesystem's performance, because now
the hard drive has to look all over the place for file fragments.
When files are deleted, the space is marked &ldquo;free&rdquo; in the FAT and
the blocks can be used by another file.</p><p>The good thing about this is that, if you delete a file that
is out near the end of your drive, the data in those blocks may not
be overwritten for months. In this case, it is likely that you will
be able to get your data back for a reasonable amount of time
afterwards.</p><p>Linux (actually, the second extended filesystem that is
almost universally used under Linux) is slightly smarter in its
approach to fragmentation. It uses several techniques to reduce
fragmentation, involving segmenting the filesystem into
independently-managed groups, temporarily reserving large chunks of
contiguous space for files, and starting the search for new blocks
to be added to a file from the current end of the file, rather than
from the start of the filesystem. This greatly decreases
fragmentation and makes file access much faster. The only case in
which significant fragmentation occurs is when large files are
written to an almost-full filesystem, because the filesystem is
probably left with lots of free spaces too small to tuck files into
nicely.</p><p>Because of this policy for finding empty blocks for files,
when a file is deleted, the (probably large) contiguous space it
occupied becomes a likely place for new files to be written. Also,
because Linux is a multi-user, multitasking operating system, there
is often more file-creating activity going on than under DOS, which
means that those empty spaces where files used to be are more
likely to be used for new files. &ldquo;Undeleteability&rdquo; has been traded
off for a very fast filesystem that normally
<span   class="emphasis"><em>never</em></span> needs to be defragmented.</p><p>The easiest answer to the problem is to put something in the
filesystem that says a file was just deleted, but there are four
problems with this approach:</p><div class="orderedlist"><ol type="1"><li><p>You would need to write a new filesystem or modify
a current one (i.e. hack the kernel).</p></li><li><p>How long should a file be marked &ldquo;deleted&rdquo;?</p></li><li><p>What happens when a hard drive is filled with files
that are &ldquo;deleted&rdquo;?</p></li><li><p>What kind of performance loss and fragmentation
will occur when files have to be written around &ldquo;deleted&rdquo;
space?</p></li></ol></div><p>Each of these questions can be answered and worked around. If
you want to do it, go right ahead and try&mdash;the ext2 filesystem has
space reserved to help you. But I have some solutions that require
zero lines of C source code.</p><p>I have two similar solutions, and your job as a system
administrator is to determine which method is best for you. The
first method is a user-by-user no-root-needed approach, and the
other is a system-wide approach implemented by root for all (or
almost all) users.</p><p>The user-by-user approach can be done by anyone with shell
access and it doesn't require root privileges, only a few changes
to your <b  >.profile</b> and <b  >.login</b>
or <b  >.bashrc</b> files and a bit of drive space. The
idea is that you alias the rm command to move the files to another
directory. Then, when you log in the next time, those files that
were moved are purged from the filesystem using the real
<b  >/bin/rm</b> command. Because the files are not
actually deleted by the user, they are accessible until the next
login. If you're using the bash shell, add this to your
<b  >.bashrc</b> file:</p><pre     class="programlisting">
alias waste='/bin/rm'
alias rm='mv $1 ~/.rm'
</pre><p>and in your</p><pre     class="programlisting">
.profile:
if [ -x ~/.rm ];
 then
   /bin/rm -r ~/.rm
   mkdir ~/.rm
   chmod og-r ~/.rm
 else
   mkdir ~/.rm
   chmod og-r ~/.rm
 fi
</pre><p>Advantages:</p><div class="itemizedlist"><ul type="disc"><li><p>can be done by any user</p></li><li><p>only takes up user space</p></li><li><p>/bin/rm is still available as the command
waste</p></li><li><p>automatically gets rid of old files every time you
log in.</p></li></ul></div><p>Disadvantages:</p><div class="itemizedlist"><ul type="disc"><li><p>takes up filesystem space (bad if you have a
quota)</p></li><li><p>not easy to implement for many users at once</p></li><li><p>files get deleted each login (bad if you log in
twice at the same time)</p></li></ul></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1228580.0x1321580"></a>System-Wide</h2></div></div><p>The second method is similar to the user-by-user method, but
everything is done in <b  >/etc/profile</b> and cron
entries. The <b  >/etc/profile</b> entries do almost the
same job as above, and the cron entry removes all the old files
every night. The other big change is that deleted files are stored
in <b  >/tmp</b> before they are removed, so this will
not create a problem for users with quotas on their home
directories.</p><p>The cron daemon (or <b  >crond</b>) is a program
set up to execute commands at a specified time. These are usually
frequently-repeated tasks, such as doing nightly backups or dialing
into a SLIP server to get mail every half-hour. Adding an entry
requires a bit of work. This is because the user has a
<b  >crontab</b> file associated with him which lists
tasks that the <b  >crond</b> program has to perform. To
get a list of what <b  >crond</b> already knows about,
use the <b  >crontab -l</b> command, for &ldquo;list the
current cron tasks&rdquo;. To set new cron tasks, you have to use the
<b  >crontab &lt;<i><tt>file</tt></i></b>
command for &ldquo;read in cron assignments from this file&rdquo;. As you can
see, the best way to add a new cron task is to take the list from
<b  >crontab -l</b>, edit it to suit your needs, and use
<b  >crontab &lt;<i><tt>file</tt></i></b> to
submit the modified list. It will look something like this:</p><pre     class="programlisting">
~# crontab -l &gt; cron.fil
~# vi cron.fil
</pre><p>To add the necessary cron entry, just type the commands above
as root and go to the end of the <b  >cron.fil</b> file.
Add the following lines:</p><pre     class="programlisting">
# Automatically remove files from the
# /tmp/.rm directory that haven't been
# accessed in the last week.
0 0 * * * find /tmp/.rm -atime +7 -exec /bin/rm {} \;
</pre><p>Then type:</p><pre     class="programlisting">
~# crontab cron.fil
</pre><p>Of course, you can change <b  >-atime +7</b> to
<b  >-atime +1</b> if you want to delete files every day;
it depends on how much space you have and how much room you want to
give your users.</p><p>Now, in your <b  >/etc/profile</b> (as
root):</p><pre     class="programlisting">
if [ -n "$BASH" == "" ] ;
then # we must be running bash
   alias waste='/bin/rm'
   alias rm='mv $1 /tmp/.rm/"$LOGIN"'
   undelete () {
     if [ -e /tmp/.rm/"$LOGIN"/$1 ] ; then
       cp /tmp/.rm/"$LOGIN"/$1 .
     else
       echo "$1 not available"
     fi
   }   if [ -n -e /tmp/.rm/"$LOGIN" ] ;
   then
     mkdir /tmp/.rm/"$LOGIN"
     chmod og-rwx /tmp/.rm/"$LOGIN"
   fi
fi
</pre><p>Once you restart cron and your users log in, your new
`undelete' is ready to go for all users running bash. You can
construct a similar mechanism for users using csh, tcsh, ksh, zsh,
pdksh, or whatever other shells you use. Alternately, if all your
users have <b  >/usr/bin</b> in their paths ahead of
<b  >/bin</b>, you can make a shell script called
<b  >/usr/bin/rm</b> which does essentially the same
thing as the alias above, and create an undelete shell script as
well. The advantage of doing this is that it is easier to do
complete error checking, which is not done here.</p><p>Advantages:</p><div class="itemizedlist"><ul type="disc"><li><p>one change affects all (or most) users</p></li><li><p>files stay longer than the first method</p></li><li><p>does not take up user's file space</p></li></ul></div><p>Disadvantages:</p><div class="itemizedlist"><ul type="disc"><li><p>some users may not want this feature</p></li><li><p>can take up a lot of space in /tmp, especially if
users delete a lot of files</p></li></ul></div><p>These solutions will work for simple use. More demanding
users may want a more complete solution, and there are many ways to
implement these. If you implement a very elegant solution, consider
packaging it for general use, and send me an e-mail message about
it so that I can tell everyone about it here.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1228580.0x171a850"></a>Tar Tips</h2></div></div><p>And, as a last-minute correction/addition to a previous
article (specifically my article on mtools in
<i  >LJ</i> issue 5), an alert reader noticed that
while mtools can copy Unix files to a DOS diskette, how can you
preserve the 256 character name of the original Unix file if DOS
can only handle 11 characters at most, and is not case-sensitive?
The case was one in which two Unix machines could use DOS
diskettes, but could not communicate directly. However, this can
apply to backups in which you want your files stored on DOS
floppies, or to any other case in which you want long file names
preserved. There is a way to do it.</p><p>The <b  >tar</b> command is used to create one big
file which can contain a number of little files. Using the
<b  >tar</b> command, you can create an archive file
which contains a bunch of 256 character file names, while the tar
file itself is a legal DOS name. DOS (or the FAT filesystem,
anyway) does not care what is in the file, as long as it has at
most eight characters plus a three character extension.</p><p>Be sure that when you copy the tar file that you do not give
the -t (text) option to mtools. The tar file has to be copied in
binary format, even if the tar file only contains text
files.</p><p>So, to copy a few long filenames to the first floppy drive
(<b  >A:</b> or <b  >/dev/fd0</b>):</p><pre     class="programlisting">
tar -cvf file.tar longfilename \
reallylongfilename \ Not.In.Dos.Format.Filename.9999 /
mcopy file.tar a:
</pre><p>Then at the remote Unix machine (or to restore it):</p><pre     class="programlisting">
mcopy a:file.tar file.tar
tar -xvf file.tar
</pre><p>or</p><pre     class="programlisting">
mread a:file.tar | tar -xf -
</pre><p>And assuming the remote Unix system has mtools and supports
256 character filenames, a copy of the files will now be on each
system.</p><p>Tune in next time when I find the real relationships between
virtual beer, BogoMIPS, and a VIC-20. In the meantime, please send
me your comments or questions or even suggestions for future
articles to: komarimf@ craft.camp.clarkson.edu.</p></div></div>
<div class="authorblurb"><p>
           <span   class="bold"><b>Mark Komarinski</b></span>
           (<a href="mailto:komarimf@craft.camp.clarkson.edu">komarimf@craft.camp.clarkson.edu</a>)
           graduated from Clarkson University (in very
           cold Potsdam, New York) with a degree in computer science and
           technical communication. He now lives in Troy, New York, and spends
           much of his free time working for the Department of Veterans
           Affairs where he is a programmer.
        </p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../009/toc009.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>