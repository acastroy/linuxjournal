<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>More About Searching</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Learning to be more efficient in searching by pre-indexing&#10;    files.&#10;    "><meta name="keywords" content="WWW, programming, search, pre-indexing, join"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1a24580.0x1b1bab0"></a>More About Searching</h1></div><div><div class="author"><h3 class="author">Reuven M. Lerner</h3></div><div class="issuemoyr">Issue #70, February 2000</div></div><div><p>
    Learning to be more efficient in searching by pre-indexing
    files.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b1c608"></a></h2></div></div><p>Last month, we looked at a simple search
engine for web sites. The program was little more than a CGI
program strapped to the <span   class="bold"><b>File::Find</b></span>
Perl module: each time a user would enter a search term in the HTML
form, the search program would dutifully open and examine each of
the files under the web hierarchy.
</p><p>While this sort of search engine works, it is exceedingly
inefficient. A site containing several dozen files will not feel
too much of a hit when its documents are searched repeatedly by a
CGI program, but a site with hundreds or thousands of files,
attracting thousands of hits per day, will watch its server's load
average skyrocket without very much difficulty.</p><p>This month, we will explore ways of making a search engine
more efficient. In the end, we will have a search engine which
might not work as efficiently as other software, but is simple to
install and use. Most importantly, we will get a chance to explore
an interesting type of software with inner workings usually
invisible to us.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b1c818"></a>The Secret: Pre-indexing</h2></div></div><p>Searching through files sequentially, trying to find matches
for a user's input, is an inherently inefficient business. Each
file must be opened, read, scanned and closed, which takes time.
Perl programs tend to consume a fair amount of memory, so the slow
execution speed means more copies of the CGI program will be
running at once. This in turn increases the risk that the web
server will have to use virtual memory, rather than physical RAM.
Slow web servers make for unhappy users, and often convince users
not to return at all.</p><p>To solve this problem, we must reduce or remove the need for
the search program to read through files. If the CGI search program
did not have to open each individual file, things would speed up
quite a bit.</p><p>A tried-and-true solution is to divide it up between two
programs. Once or twice each day, an indexing program traverses the
web-document tree, reading through each document and analyzing its
word use. This program runs behind the scenes without user
intervention or knowledge. Rather than sending its results to a
user, the indexer dumps all information it has about word frequency
and usage and places it in a file on disk.</p><p>This means the search program the user invokes via CGI does
not actually have to search. Instead, the search program merely
opens the index file, finds those files where the user's search
term appears the greatest number of times, and displays that list
in the user's browser.</p><p>Indexing a page is not difficult in Perl, because of its rich
library of regular expressions. The <b  >m//</b> operator
normally matches the regular expression between its delimiters.
When invoked with the <b  >/g</b> modifier and when
operating in list context, it returns all matches it can find.
Thus, in the expression</p><pre     class="programlisting">
my $found = join ' ',
  ("encyclopedia" =~ m/[aeiou]/g);
print "$found\n";
</pre><p>the first statement finds all vowels in &ldquo;encyclopedia&rdquo; and
returns them as a list to the caller. In this case, the caller is
Perl's <span   class="bold"><b>join</b></span> operator, which pushes
the elements together, separated by spaces. Executing the two lines
of code above displays the following on the user's screen:
<pre     class="programlisting">
e o a e i a
</pre>


Using the built-in character class for non-whitespace characters,
<b  >\S</b>, we can apply the same algorithm in order to
extract words from a text string. For example:
<pre     class="programlisting">
my $sentence = "The rain in Spain falls mainly\n\n on the plain.";
my $found = join '|', ($sentence =~ m/\b\S+\b/g);
print "$found\n";
</pre>


The code above prints the following results:
<pre     class="programlisting">
The|rain|in|Spain|falls|mainly|on|the|plain
</pre>


Notice how using <b  >\b</b> (which matches a word
boundary) means our program need not worry about newline
characters, extra spaces or punctuation.
</p><p>Indexers have to consider whether to keep case relevant. My
personal preference is to ignore case, since users do not
necessarily remember, and it also removes an obstacle to finding
desired text. We can thus turn all of the words into lowercase
letters:</p><pre     class="programlisting">
my $sentence = "The rain in Spain falls mainly\n\n on the plain.";
my $found = join '|', map {lc $_}
   ($sentence =~
   m/\b\S+\b/g);
print "$found\n";
</pre></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b1cea0"></a>Storing the Index</h2></div></div><p>To store index information, we will use a hash,
<b  >%IGNORE</b>. The keys will be words we wish to
ignore when indexing. Any non-zero value will indicate this word
should be ignored when indexing:</p><pre     class="programlisting">
%IGNORE = ("the" =&gt; 1, "in" =&gt; 1, "on" =&gt; 1);
my $sentence = "The rain in Spain falls mainly\n\n on the plain.";
my $found = join '|',
   grep {!$IGNORE{$_}}
   map {lc $_} ($sentence =~ m/\b\S+\b/g);
print "$found\n";
</pre><p>Notice how we can stack different items together:
<b  >m//</b> returns a list, which is passed to
<span   class="bold"><b>map</b></span>, which returns a list, which
is fed to <span   class="bold"><b>grep</b></span>, which is in turn
fed to <span   class="bold"><b>join</b></span>, and which is in turn
assigned to <b  >$found</b>.
</p><p>Finally, we will index the words by creating a hash
(<b  >%index</b>) in which the collected words are the
keys. The value will be a hash reference, where the key is the name
of the current file, and the value is the number of times this word
appears in the file. In other words,</p><pre     class="programlisting">
$index{spain}-&gt;{foo.html} = 5;
</pre><p>means the word &ldquo;spain&rdquo; appears in foo.html five times. Here
is some code that performs the indexing in this way:
<pre     class="programlisting">
%IGNORE = ("the" =&gt; 1, "in" =&gt; 1, "on" =&gt; 1);
my $sentence = "The rain in Spain falls mainly\n\n on the plain.";
my @found =
  grep {!$IGNORE{$_}} map {lc $_} ($sentence =~
m/\b\S+\b/g);
    foreach my $word (@found)
    {
        $index{$word}-&gt;{$filename}++;
    }
</pre>


</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b1d420"></a>Traversing the Web Hierarchy</h2></div></div><p>Now that we know how to index the words from a string, we
will use File::Find to perform this task on the entire web
hierarchy. File::Find comes with the standard Perl distribution,
and exports a subroutine named
<span   class="bold"><b>find</b></span>. This subroutine takes a
subroutine reference and a list of directories as arguments. It
invokes the named subroutine (known as a &ldquo;callback&rdquo;) on every
file it finds in the named directories. It is the subroutine's
responsibility to open the file, retrieve its contents, and index
it as necessary.</p><p>Listing 3 (see Resources) contains a simple program
(index-files.pl) that traverses the web hierarchy, searches through
the contents, and stores word frequency in the
<b  >%index</b> hash. The callback subroutine ignores
files in any directory named .noindex, as well as any
subdirectories of such directories. This is accomplished with
another hash named <b  >%ignore_directory</b>. It also
ignores non-HTML files and removes HTML tags from the contents of
each file before indexing the words it contains. Rather than
removing the HTML tags altogether, index-files.pl turns them into
whitespace. This means that two words separated by an HTML tag (but
no whitespace) will not be jammed together, but will remain
separate words.</p><p>To avoid problems with HTML entities such as
<b  >&amp;gt;</b>, <b  >&amp;amp;</b> and other
items needed for symbols and accented Latin characters, we use the
<span   class="bold"><b>HTML::Entities</b></span> module. This module
exports a <span   class="bold"><b>decode_entities</b></span>
subroutine that turns the entities back into their regular
characters. By running this and then removing the HTML tags, we are
able to index a clean document containing only text.</p><p>While hashes are convenient and fast, they tend to consume a
lot of memory. A hash that points to another hash consumes even
more memory, and storing each individual word as a key in the
primary hash means the memory usage will skyrocket even more.
Running index-file.pl is thus not for the faint of heart or for
computers without a significant amount of RAM. This particular
version of index-file.pl caused my Linux system (with 72MB of RAM
and another 64MB of swap) to run out of memory, particularly when
piping the output through
<span   class="bold"><b>less</b></span>.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b1d8f0"></a>Writing %index to Disk</h2></div></div><p>While index-files.pl performs an admirable job of indexing
the files in a web hierarchy, it does not write the information to
disk. Once index-files.pl exits, all indexing information is lost
forever.</p><p>The solution is clearly to write the index to disk. But how?
The most obvious answer is to use a DBM file. DBM files are most
easily described as an on-disk version of hashes. Each entry in a
DBM file has a key and a value, both of which may be any character
string. As with hashes, DBM files are optimized for speed, making
them much faster to use than straight text files. There are several
different kinds of DBM files out there, ranging from plain old DBM
to the GNU Project's GDBM to Berkeley DB, which has gained quite a
bit of popularity in the last few years partly as a result of its
integration into Sendmail.</p><p>Perl supports many types of DBM files through its &ldquo;tie&rdquo;
interface, each of which has its own object module. Tying is a
means of making a variable magical, attaching additional behavior
every time a value is stored or retrieved. Thus, tying a hash to a
DBM class means that every time a value is stored to the hash, the
value is written to a corresponding DBM file on disk. By the same
token, every value retrieval reads the information from
disk.</p><p>There is one problem here, though: DBM files can store text
strings, but nothing more complicated than that. Because each value
of <b  >%index</b> is a reference to another hash, the
normal DBM classes will not work correctly. Attempting to store
references in a normal DBM file will actually store their printed
representation, which is not at all useful.</p><p>The solution is to use the
<span   class="bold"><b>MLDBM</b></span> class, which is designed
precisely for this purpose. MLDBM works in conjunction with another
DBM class to store references in a format that can be retrieved
later.</p><p>To use MLDBM, import it at the top of a program with
<span   class="bold"><b>use</b></span>, specifying the type of DBM
file to use with it:</p><pre     class="programlisting">
use MLDBM qw(DB_File);
</pre><p>In this particular case, we will use
<b  >DB_File</b>, which uses the Berkeley DB module that
comes with Perl. (Most Linux systems come with Berkeley DB, as
well.) It is also possible to specify the underlying method used to
store the references; we will use the default, which takes
advantage of the <span   class="bold"><b>Data::Dumper</b></span>
module available on CPAN. Other options include
<span   class="bold"><b>FreezeThaw</b></span> and
<span   class="bold"><b>Storable</b></span>, which perform similar
tasks in different ways.
</p><p>Our hash can then be tied to a DBM file with the
<span   class="bold"><b>tie</b></span> command:</p><pre     class="programlisting">
# In what file should the index be stored?
my $index_file = "/tmp/lj.db";
# Create the word index hash
my %data;
tie %data, "MLDBM", $index_file, O_RDWR|O_CREAT
or die "Error tying %data: $! ";
</pre><p>From this point onward, any value stored to
<b  >%index</b> will be stored in the file /tmp/lj.db.
Any value retrieved from <b  >%index</b> will actually be
retrieved from /tmp/lj.db. Storing index data in /tmp is a mistake
on a production web server, since the file can be deleted by the
system.
</p><p>While MLDBM attempts to function as transparently as
possible, it will sometimes cause confusing errors. For example,
there is normally no problem with the following Perl code:</p><pre     class="programlisting">
$data{$word}-&gt;{$url}++;
</pre><p>As we saw earlier, this will increment the counter for a
particular word in a particular file. When <b  >%data</b>
is tied to MLDBM, the above code will no longer work. Instead, the
assignment must be performed in two stages, retrieving the hash
reference, assigning to it, and placing it back inside of
<b  >%data</b>:
<pre     class="programlisting">
my $hashref = $data{$word};
$hashref-&gt;{$url}++;
$data{$word} = $hashref;
</pre>


The index should be regenerated on a regular basis to ensure it is
as fresh as possible. Consider using
<span   class="bold"><b>cron</b></span>, which comes with all Linux
and UNIX systems, to run index-files.pl every day at 4 AM or at
another convenient time when people are unlikely to be using the
computer.
</p><p>While our version of index-files.pl does not support such
functionality, it would not be difficult to add a hash key
indicating when the file system was last indexed. index-files.pl
could then be changed to index only those files that were created
or modified after a particular date.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b14c28"></a>Reading the Index</h2></div></div><p>Now that we have created an index, we will need a program to
retrieve results from it. Indeed, that's just the beginning&mdash;if we
want, we can write multiple programs that read through the DBM
file, searching in a variety of ways.</p><p>Our first and simplest search will accept input from an HTML
form. The form will allow users to specify a single word to search
for. Unfortunately, the way we have indexed the files makes it easy
to retrieve single words, but impossible to retrieve phrases. A
different data structure would make it easier to perform such
searches, but would undoubtedly increase the size of the
index.</p><p><a href="3800l1.html" target="_self">Listing 1</a></p><p>Listing 1 is a simple HTML form we can use for searching.
This form expects to submit its results to a CGI program called
dbsearch.pl, which is in Listing 4 (see Resources). dbsearch.pl
opens the DBM file&mdash;again, using MLDBM and DB_File&mdash;and retrieves
the keys from the hash associated with the search term. By sorting
the keys (which contain URLs) by their values (which are integers
indicating how many times a word appeared in a file), it's easy to
create a simple frequency chart. In this particular case,
dbsearch.pl displays all of the results, but it would not be
difficult to display only the top 20.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b14e90"></a>Boolean Search Terms</h2></div></div><p>Searching for a single term might be easy to implement, but
the best search engines allow for AND and OR searches. Listing 2 is
an HTML form that differs from the previous version by adding a
radio button. If the radio button is set to the default OR
position, any one of the search terms may appear in the document.
An AND search requires that both or all of the search terms appear
in the document in order for it to match.</p><p><a href="3800l2.html" target="_self">Listing 2</a></p><p>A version of better-dbsearch.pl, which allows for AND and OR
searches, is in Listing 5 (see Resources).</p><p>If the user chooses OR, dbsearch.pl will have to find the
highest total for a combination of hashes, rather than just one. A
simple example would be the hashes <b  >%odds</b> and
<b  >%evens</b>, as follows:</p><pre     class="programlisting">
%odds =  (one =&gt; 1, three =&gt; 3, five =&gt; 5);
%evens = (two =&gt; 2, four =&gt; 4, six =&gt; 6);
</pre><p>We can turn this into a simple hash by mixing them together:
<pre     class="programlisting">
%numbers = (%odds, %evens);
</pre>


This technique will not work in our case, because it is quite
possible that both words will appear in the same file. If that
happens, there will be duplicate keys&mdash;unlike
<b  >%odds</b> and <b  >%evens</b>, where no
key appears in both hashes. For our OR search to work, we will need
to combine the values in a master hash:
<pre     class="programlisting">
foreach my $word (@terms)
{
my $frequency = $data{$word};
foreach my $filename (keys %$frequency)
{
$full_results{$filename} += $frequency-&gt;{$filename};
}
}
</pre>


The above code is not very different from its predecessor in
dbsearch.pl. The main change is that it introduces a new hash,
<b  >%full_results</b>, where the keys are file names and
the values reflect the total scores for each of the search terms.
This algorithm means a file containing one copy of both search
terms will be ranked the same as one in which the same search term
appears twice.
</p><p>To handle AND searches, we must keep track of how many search
terms appear in each file. If a file contains the same number of
search terms as are in <b  >@terms</b>, the file is
acceptable. If it contains fewer matches than
<b  >@terms</b>, the file should be dropped from
consideration.</p><p>We can implement this by using the
<b  >%match_counter</b> hash, where the keys are file
names and the values are integers representing the number of search
terms contained in a file. By adding a single line to the foreach
loop, we can keep track of how many search terms appear in each
file:</p><pre     class="programlisting">
$match_counter{$filename}++ if ($boolean eq "and");
</pre><p>Once we have exited from the foreach loop, but before we
announce the results to the user, we prune file names from
<b  >%full_results</b>. Because the keys for
<b  >%full_results</b> are the same as for
<b  >%match_counter</b>, the code is relatively simple:
<pre     class="programlisting">
foreach my $filename (keys %full_results)
{
delete $full_results{$filename}
unless ($match_counter{$filename} == @terms);
}
</pre>


Notice how we use <span   class="bold"><b>delete</b></span> to remove
the hash element. Using <span   class="bold"><b>undef</b></span> on
<b  >$full_results{$filename}</b> would make the value
undefined, but would leave the hash key unaffected.
<span   class="bold"><b>delete</b></span> removes the key and its
corresponding value from the hash completely.
</p><p>Also notice how we can get the size of
<b  >@terms</b> simply by naming the array itself. The
<b  >==</b> forces scalar context on both of its
operands, so there is no need to use the <b  >scalar</b>
keyword before <b  >@terms</b>.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b15af0"></a>Downsides to Pre-Indexing</h2></div></div><p>Our pre-indexing solution is, indeed, faster than the search
program we examined last month. I did not perform any serious
benchmarks on this set of programs, but the difference was readily
apparent to the naked eye. Not only that, but our pre-indexed
implementation made it easy to rank files according to the number
of matches.</p><p>However, pre-indexing is not a panacea. For starters, it
requires a good deal of disk space, weighing in at 2.6MB for an
index of fewer than 400 files. However, given the rapidly dropping
price of disk space, even a 10MB index should not prove to be much
of a deterrent for most systems.</p><p>A bigger problem is the lack of flexibility that pre-indexing
forces on a search system. For example, our index programs all used
Perl's <span   class="bold"><b>lc</b></span> operator to force all of
the letters to lowercase. Now that we have removed any trace of
case from our index, how can we offer a case-sensitive search? The
only real answer is to build the DBM file in a slightly different
way, perhaps by storing an additional <b  >literal</b>
element in the hash for each file. Then we would know that
<b  >abc</b> appeared five times in a particular file,
but that two of these were <b  >ABC</b> and the remaining
three were <b  >abc</b>.</p><p>Pre-indexing also means we can no longer offer a phrase
search, in addition to AND and OR searches. There are ways to solve
this problem; the easiest might be to store &ldquo;next word&rdquo;
information in the database hash. Then we could search for the
first word of a phrase and use the &ldquo;next word&rdquo; information to see
if the other words were found, one by one.</p><p>Finally, pre-indexing always means the index will lag behind
other content on a web site. If the index is generated once every
24 hours, it might take up to one day for a new document to be read
into the index. One solution is to run the indexer more often, such
as every three or six hours.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1a24580.0x1b15f10"></a>Conclusion</h2></div></div><p>Searching is an essential part of every good web site. It
means users can find what interests them quickly, and can perform
analyses that the site's administrators never expected. But as we
saw last month, a straight search through a site's files can take a
long time to execute. Pre-indexing is the standard solution to this
problem, trading additional disk space and a slightly out-of-date
index in exchange for faster execution speed. Understanding the
trade-offs involved in writing a search engine makes it easier to
evaluate free and commercial offerings, and thus make your site a
more enjoyable place for users to visit.</p><p><a href="3800s1.html" target="_self">Resources</a></p></div></div>
<div class="authorblurb"><p>
        <div       class="mediaobject"><img src="3800aa.jpg"></div>


      <span   class="bold"><b>Reuven M. Lerner</b></span>
      is an Internet and Web
      consultant living in Haifa, Israel, who has been using the Web
      since early 1993. His book Core Perl will be published
      by Prentice-Hall in the spring. Reuven can be reached at
      reuven@lerner.co.il. The ATF home page, including archives and
      discussion forums, is at
      <a href="http://www.lerner.co.il/atf" target="_self">http://www.lerner.co.il/atf/</a>.</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../070/toc070.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>