<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Scaling Linux to New Heights: the SGI Altix 3000 System</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    With 64 processors and 512GB of memory, SGI claims the title&#10;    of world's most powerful Linux system.&#10;    "><meta name="keywords" content="SGI, Altix, 3000, hardware"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x2144580.0x223bab0"></a>Scaling Linux to New Heights: the SGI Altix 3000 System</h1></div><div><div class="author"><h3 class="author">Steve Neuner</h3></div><div class="issuemoyr">Issue #106, February 2003</div></div><div><p>
    With 64 processors and 512GB of memory, SGI claims the title
    of world's most powerful Linux system.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2144580.0x223c558"></a></h2></div></div><p>SGI recently debuted its new 64-bit,
64-processor, Linux system based on the Intel Itanium 2
processor&mdash;a significant announcement for the company and for
Linux. This system marks the opening of a new frontier as
scientists working on complex and demanding high-performance
computing (HPC) problems can now use and deploy Linux in ways never
before possible. HPC environments continually push the limits of
the operating system by requiring larger numbers of CPUs, higher
I/O bandwidth and faster and more efficient parallel programming
support.
</p><p>Early on in the system's development, SGI made the decision
to use Linux exclusively as the operating system for this new
platform. It proved to be a solid and very capable operating system
for the technical compute environments that SGI targets. With the
combination of SGI NUMAflex global shared-memory architecture,
Intel Itanium 2 processors and Linux, we were breaking records long
before the system was introduced.</p><p>The new system, called the SGI Altix 3000, has up to 64
processors and 512GB of memory. A future version will offer up to
512 processors and 4TB. In this article, we explore the hardware
design behind the new SGI system, describe the software development
involved to bring this new system to market and show how Linux can
readily scale and be deployed in the most demanding HPC
environments.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2144580.0x223c710"></a>Hardware and System Architecture
Background</h2></div></div><p>The SGI Altix 3000 system uses Intel Itanium 2 processors and
is based on the SGI NUMAflex global shared-memory architecture,
which is the company's implementation of a non-uniform memory
access (NUMA) architecture. NUMAflex was introduced in 1996 and has
since been used in the company's renowned SGI Origin family of
servers and supercomputers based on the MIPS processor and the IRIX
64-bit operating system. The NUMAflex design enables the CPU,
memory, I/O, interconnect, graphics and storage to be packaged into
modular components, or bricks. These bricks can then be combined
and configured with tremendous flexibility to match a customer's
resource and workload requirements better. Leveraging this
third-generation design, SGI was able to build the SGI Altix 3000
system using the same bricks for I/O (IX- and PX-bricks), storage
(D-bricks) and interconnect (router bricks/R-bricks). The primary
difference in this new system is the CPU brick (C-brick), which
contains the Itanium 2 processors. Figure 1 shows the types of
bricks used on the SGI Altix 3000 system. Figure 2 depicts how
these bricks can be combined into two racks to make a
single-system-image 64-processor system.</p><div       class="mediaobject"><img src="6440f1.jpg"><div class="caption"><p>
Figure 1. NUMAflex Brick Types
</p></div></div><div       class="mediaobject"><img src="6440f2.jpg"><div class="caption"><p>
Figure 2. Two Possible NUMAflex Configurations
</p></div></div><p><a href="6440s1.html" target="_self">SGI Altix 3000 C-Brick Block Diagram
and Specifications</a></p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2144580.0x223cc38"></a>Preparing Linux for a New Hardware
Platform</h2></div></div><p>On a well-designed and balanced hardware architecture such as
NUMAflex, it is the operating system's job to ensure that users and
applications can fully exploit the hardware without being hindered
due to inefficient resource management or bottlenecks. Achieving
balanced hardware resource management on a large NUMA system
requires starting kernel development long before the first Itanium
2 processors and hardware prototype systems arrive. In this case,
we also used the first-generation Itanium processors for making the
CPU scaling, I/O performance and other changes to Linux necessary
for demanding HPC environments.</p><p>The first step in preparing the software before the prototype
hardware arrives is identifying, as best you can, the necessary
low-level hardware register and machine programming changes the
kernel will need for system initialization and runtime. System
manufacturers developing custom ASICs for highly advanced systems
typically use simulation software and tools to test their hardware
design. Before hardware was available, we developed and used
simulators extensively for both the system firmware and kernel
development to get the system-level software ready.</p><p>When the original prototype hardware based on
first-generation Itanium processors arrived, it was time for
power-on. One of the key milestones was powering the system on for
the first time and taking a processor out of reset, then fetching
and executing the first instructions from PROM.</p><div       class="mediaobject"><img src="6440f3.jpg"><div class="caption"><p>
Figure 3. SGI engineers celebrate power-on success.
</p></div></div><p>After power-on, the fun really began with long hours and
weekends in the hardware &ldquo;bring-up&rdquo; lab. This is where hardware,
diagnostic and platform-software engineers worked together closely
to debug the system and get the processor through a series of
important milestones: PROM to boot prompt, Linux kernel through
initialization, reading and mounting root, reaching single-user
mode and then going into multi-user mode and then connecting to the
network. After that, we did the same thing all over again with
multiple processors and multiple nodes&mdash;typically pursued in
parallel&mdash;with several other bring-up teams at other stations that
trail closely behind the lead team's progress.</p><div       class="mediaobject"><img src="6440f4.jpg"><div class="caption"><p>
Figure 4. During bring-up, a hardware engineer, a PROM engineer and
an OS engineer discuss a bug.
</p></div></div><p>Once we had Linux running on the prototype systems with
first-generation Itanium processors, software engineers could
proceed with ensuring that Linux ran and, in particular, scaled
well on large NUMA systems. We built and used numerous in-house,
first-generation Itanium-based systems to help ensure that Linux
performed well on large systems. By early 2001, we had succeeded in
running a 32-processor Itanium-based system&mdash;the first of its
kind.</p><div       class="mediaobject"><img src="6440f5.jpg"><div class="caption"><p>
Figure 5. The author's son in front of an early 32-processor
Itanium-based system, Summer 2001.
</p></div></div><p>These first-generation Itanium-based systems were key in
having Linux ready for demanding HPC requirements. Well before the
first Itanium 2 processors were available from Intel, the bulk of
the scaling, I/O performance and other changes for Linux could be
developed and tested.</p><p>As one group of SGI software engineers was busy working on
performance, scaling and other issues, using prototypes with
first-generation Itanium processors, another team of hardware and
platform-software engineers was getting the next-generation SGI
C-brick with Itanium 2 processors ready for power-on to repeat the
bring-up process all over again.</p><div       class="mediaobject"><img src="6440f6.jpg"><div class="caption"><p>
Figure 6. First power-on of the Itanium 2-based C-brick.
</p></div></div><p>By mid-2002, the bring-up team had made excellent progress,
from power-on of a single processor to running a 64-processor
system. The 64-processor system with Itanium 2 processors again
marked the first of its kind. All this, of course, was with Linux
running in a single-system image.</p><p>Throughout this whole process, we passed any changes in Linux
or bugs found back to the kernel developers for inclusion in a
future release of Linux.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2144580.0x223d6e0"></a>A Closer Look at Linux on Big Iron</h2></div></div><p>Other Linux developers often ask, &ldquo;What kind of changes did
you have to make to get Linux to run on that size system?&rdquo; or
&ldquo;Isn't Linux CPU scaling limited to eight or so processors?&rdquo;
Answering these questions involves examining further what SGI is
using as its software base, the excellent changes made by the
community and the other HPC-related enhancements and tools provided
by SGI to help make Linux scale far beyond the perceived limit of
eight processors.</p><p>On the SGI Altix 3000 system, the system software consists of
a standard Linux distribution for Itanium processors and SGI
ProPack, an overlay product that provides additional features for
Linux. SGI ProPack includes a newer 2.4-based Linux kernel, HPC
libraries highly tuned to exploit SGI's hardware, NUMA tools and
drivers.</p><p>The 2.4-based Linux kernel used on the SGI Altix 3000 system
consists of the standard 2.4.19 kernel for Itanium processors
(<a href="http://kernel.org" target="_self">kernel.org</a>), plus other
improvements. These improvements fall into one of three categories:
general bug fixes and platform support, improvements from other
work occurring within the Linux community and SGI changes.</p><p>The first category of kernel changes is simply ongoing fixes
to bugs found during testing and the continued improvements for the
underlying platform and NUMA support. For these changes, SGI works
with the kernel team's designated maintainer to get these changes
incorporated back into the mainline kernel.</p><p>The second category of kernel improvements consists of the
excellent work and performance patches developed by others within
the community that have not been accepted officially yet or were
deferred until the 2.5 development stream. These improvements can
be found on the following VA Software SourceForge sites: &ldquo;Linux on
Large Systems Foundry&rdquo;
(<a href="http://large.foundries.sourceforge.net" target="_self">large.foundries.sourceforge.net</a>)
and the &ldquo;Linux Scalability Effort Project&rdquo;
(<a href="http://sourceforge.net/projects/lse" target="_self">sourceforge.net/projects/lse</a>).
We used the following patches from these projects: CPU scheduler,
Big Kernel Lock usage reduction improvements, dcache_lock-usage
reduction improvements based on the Read-Copy-Update spinlock
paradigm and xtime_lock (gettimeofday) usage reduction improvements
based on the FRlock locking paradigm.</p><p>We also configured and used the Linux device filesystem
(devfs,
<a href="http://www.atnf.csiro.au/people/rgooch/linux/docs/devfs.html" target="_self">www.atnf.csiro.au/people/rgooch/linux/docs/devfs.html</a>)
on our systems to handle large numbers of disks and I/O busses.
Devfs ensures that device path names persist across reboots after
other disks or controllers are added or removed. The last thing a
system administrator of a very large system wants is to have a
controller go bad and have some 50 or more disks suddenly
renumbered and renamed. We have found devfs to be reliable and
stable in high-stress system environments with configurations
consisting of up to 64 processors with dozens of fibre channel
loops with hundreds of disks attached. Devfs is an optional part of
the 2.4 Linux kernel, so a separate kernel patch was not
needed.</p><p>The third category of kernel change consists of improvements
by SGI that are still in the process of getting submitted into
mainline Linux, were accepted after 2.4 or will probably remain
separate due to the specialized use or nature of the patch. These
open-source improvements can be found at the &ldquo;Open Source at SGI&rdquo;
web site (<a href="http://oss.sgi.com" target="_self">oss.sgi.com</a>). The
improvements we made included: XFS filesystem software, Process
AGGregates (PAGG), CpuMemSets (CMS), kernel debugger (kdb) and a
Linux kernel crash dump (lkcd).</p><p>In addition, SGI included its SCSI subsystem and drivers
ported from IRIX. Early tests of the Linux 2.4 SCSI I/O subsystem
showed that our customers' demanding storage needs could not be met
without a major overhaul in this area. While mainstream kernel
developers are working on this for a future release, SGI needed an
immediate fix for its 2.4-based kernel, so the SGI XSCSI
infrastructure and drivers from IRIX were used as an interim
solution.</p><p>Figures 7-9 illustrate some of the early performance
improvements that were achieved with Linux on the SGI Altix 3000
system using the previously described changes. Figure 7 compares
XFS to other Linux filesystems. (Note, for a more detailed study on
Linux filesystem performance, see &ldquo;Filesystem Performance and
Scalability in Linux 2.4.17&rdquo;, 2002 USENIX Annual Technical
Conference, which is also available at
<a href="http://oss.sgi.com" target="_self">oss.sgi.com</a>). Figure 8
compares XSCSI to SCSI in Linux 2.4, and Figure 9 shows CPU
scalability using AIM7.</p><div       class="mediaobject"><img src="6440f7.jpg"><div class="caption"><p>
Figure 7. Filesystem performance comparison: AIM7 multi-user kernel
workload, 2.4.17 kernel; 28 P Itanium prototype, 14GB, 120 disks;
work-in-progress, interim example; varied filesystems only, but
includes SGI enhancements and SGI tuned kernel.
</p></div></div><div       class="mediaobject"><img src="6440f8.jpg"><div class="caption"><p>
Figure 8. Linux XSCSI performance example: work-in-progress,
interim example using 2.4.16 kernel; 120 processes reading from 120
disks (through driver only).
</p></div></div><div       class="mediaobject"><img src="6440f9.jpg"><div class="caption"><p>
Figure 9. CPU scaling example with AIM7: AIM7 multi-user kernel
workload, 2.4.16 kernel; work-in-progress, interim example; SGI
enhancements and SGI-tuned kernel.
</p></div></div><p>While SGI is focused more toward high-performance and
technical computing environments&mdash;where the majority of CPU cycles
is typically spent in user-level code and applications instead of
in the kernel&mdash;the AIM7 benchmark does show that Linux can still
scale well with other types of workloads common in enterprise
environments. For HPC application performance and scaling examples
for Linux, see the Sidebar &ldquo;Already Solving Real-World
Problems&rdquo;.</p><p>Figure 10 shows the scaling results achieved on an early SGI
64-processor prototype system with Itanium 2 processors running the
STREAM Triad benchmark, which tests memory bandwidth. With this
benchmark, SGI demonstrated near-linear scalability from two to 64
processors and achieved over 120GB per second. This result marks a
significant milestone for the industry by setting a new world
record among a microprocessor-based system, which was achieved
running Linux within a single-system image! This impressive result
also demonstrates that Linux can indeed scale well beyond the
perceived limitation of eight processors. For more information on
STREAM Triad, see
<a href="http://www.cs.virginia.edu/stream" target="_self">www.cs.virginia.edu/stream</a>.</p><div       class="mediaobject"><a href="6440f10.large.jpg"><img src="6440f10.jpg"></a><div class="caption"><p>
Figure 10. Near-linear STREAM Triad scalability up to 64
processors.
</p></div></div><p>When you look at the list of kernel additions included in SGI
ProPack the list is actually surprisingly small, which speaks
highly of Linux's robust original design. What is even more
impressive is that many of these and other changes are already in
the 2.5 development kernel. At this pace, Linux is quickly evolving
as a serious HPC operating system.</p><p><a href="6440s2.html" target="_self">Already Solving Real-World
Problems</a></p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2144580.0x2234e38"></a>Other Enhancements to Linux for HPC</h2></div></div><p>SGI ProPack also includes several tools and libraries to help
improve performance on large NUMA systems for solving a complex
problem with an application that needs large numbers of CPUs and
memory, or when multiple applications are running simultaneously
within the same large system. On Linux, SGI provides the commands
<span   class="bold"><b>cpuset</b></span> and
<span   class="bold"><b>dplace</b></span>, which give predictable and
improved CPU and memory placement control for HPC applications.
These tools help unrelated jobs carve out and use the resources
they each need without getting into each other's way or help
prevent a smaller job from inadvertently thrashing across a larger
pool of resources than it can effectively use. Therefore system
resources are used efficiently and deliver results in a consistent
time period&mdash;two characteristics critical to HPC
environments.</p><p>Also, the SGI Message Passing Toolkit (MPT) in SGI ProPack
provides industry-standard message passing libraries optimized for
SGI computers. MPT contains MPI and SHMEM APIs, which transparently
utilize and exploit the low-level capabilities within the SGI
hardware, such as its block transfer engine (BTE) for fast
memory-to-memory transfers and the hardware memory controller's
fetch operation (fetchop) support. Fetchop support enables direct
communication and synchronization between multiple MPI processes
while eliminating the overhead associated with system calls to the
operating system.</p><p>The SGI ProPack NUMA tools, HPC libraries and additional
software support layered on top of a standard Linux distribution
provide a powerful HPC software environment for big compute and
data-intensive workloads. Much like a custom ASIC on hardware
providing the &ldquo;glue logic&rdquo; to leverage and use commodity
processors, memory and I/O parts, SGI ProPack software provides the
&ldquo;glue logic&rdquo; to leverage the Linux operating system as a
commodity building block for large HPC environments.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2144580.0x22350a0"></a>Conclusion</h2></div></div><p>No one believed Linux could scale so well, so soon. By
combining Linux with SGI NUMAflex system architecture and Itanium 2
processors, SGI has built the world's most powerful Linux system.
Bringing the SGI Altix 3000 system to market involved a tremendous
amount of work, and we consider it to be only the beginning. The
aggressive standards-based strategy that SGI has for using Linux on
Itanium 2-based systems is raising the bar on what Linux can do
while providing customers an exciting, no-compromises alternative
for large HPC servers and supercomputers. SGI engineers&mdash;and the
entire company for that matter&mdash;are fully committed to building on
Linux capabilities and pushing the envelope even further to bring
more exciting breakthroughs and opportunities for the Linux and HPC
communities.</p></div></div>
<div class="authorblurb"><p>
        <div       class="mediaobject"><img src="6440aa.jpg"></div>
       </p><p><span   class="bold"><b>Steve Neuner</b></span> has
      been working in UNIX kernel development for the past 19 years at
      major computer manufacturers including MAI Basic Four, Sequent
      Computer Systems, Digital Equipment Corporation and SGI. Now with
      SGI, Steve is the Linux engineering director and has been working
      on Linux and Itanium-based systems since joining SGI four years
      ago.</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../106/toc106.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>