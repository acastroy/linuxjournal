<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta charset="utf-8"/>
<link href="https://fonts.googleapis.com/css?family=Lateef" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<title>From the Editor: How Can We Bring FOSS to the Virtual World?</title>


<link href="../../css/archive.css" type="text/css" rel="stylesheet"/>
<script type="text/javascript" src="../../js/archive.js"></script>
<script type="text/javascript" src="../../js/highlight.js"></script>
</head>

<body class="from_sigil">
  
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>


  <div class="tophrdiv">
  </div>

  
  <div id="top_search">
    <table class="page_search" summary="">
      <tr>
        <td valign="top" align="left">
          <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
        </td>
        <td valign="top" align="right">
          <form method="get" action="/zoom/search.cgi">
            <input type="hidden" name="zoom_sort" value="0" />
            <input type="hidden" name="zoom_xml" value="0" />
            <input type="hidden" name="zoom_per_page" value="10" />
            <input type="hidden" name="zoom_and" value="1" />
            Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
            <input type="submit" value="Submit" />
          </form>
        </td>
      </tr>
    </table>
  </div>

<h1 class="title">From the Editor: How Can We Bring FOSS to the Virtual World?</h1>

<h2 class="sigil_not_in_toc">Is there room for FOSS in the AI, VR, AR, MR, ML and XR revolutions—or
vice versa? By Doc Searls</h2>

<p>Will the free and open-source revolution end when our most personal computing
happens inside the walled gardens of proprietary AI VR, AR, MR, ML and XR companies?
I ask, because that's the plan.</p>

<p>I could see that plan when I met the <a href="https://www.magicleap.com/magic-leap-one">Magic Leap One</a> at <a href="http://iiworkshop.org">IIW</a> in October (only a few days ago as I write
this). The ML1 (my abbreviation) gave me an MR (mixed reality) experience when I wore
all of this:</p>

<ul>
<li>Lightwear (a headset).</li>
<li>Control (a handset).</li>
<li>Lightpack (electronics in a smooth disc about the size of a saucer).</li>
</ul>

<p>So far, all Magic Leap offers is a Creator Edition. That was the one I met. Its
price is $2,295, revealed only at the end of a registration gauntlet that requires
name, email address, birth date and agreement with two click-wrap contracts
totaling more than 7,000 words apiece. Here's what the page with the price says you
get:</p>

<blockquote>
<p>Magic Leap One Creator Edition is a lightweight, wearable computer that seamlessly
blends the digital and physical worlds, allowing digital content to coexist with real
world objects and the people around you. It sees what you see and uses its
understanding of surroundings and context to create unbelievably believable
experiences.</p>
</blockquote>

<p>Also recommended on the same page are a shoulder strap ($30), a USB (or USB-like)
dongle ($60) and a "fit kit" ($40), bringing the full price to $2,425.</p>

<p>Buying all this is the cost of entry for chefs working in the kitchen, serving
apps and experiences to customers paying to play inside Magic Leap's walled garden: a
market Magic Leaps hopes will be massive, given an investment sum that now totals
close to $2 billion.</p>

<p>The experience it created for me, thanks to the work of one early developer, was
with a school of digital fish swimming virtually in my physical world. Think of a
hologram without a screen. I could walk through them, reach out and make them
scatter, and otherwise interact with them. It was a nice demo, but far from anything
I might crave.</p>

<p>But I wondered, given Magic Leap's secretive and far-advanced tech, if it could
eventually <em>make</em> me crave things. I ask because <em>immersive</em> doesn't
cover what this tech does. A better adjective might be <em>invasive</em>.</p>

<p>See, the Lightwear headset has cameras facing both outward at the physical world
and inward at your eyes (each of which, as the saying goes, is a "<a href="https://www.phrases.org.uk/bulletin_board/41/messages/1097.html">window to your
soul</a>"). The outward ones take in your physical world, while the inward ones
profile your eyeballs and project those 3D images directly onto the retinas of your
eyes. They do that using "light wave" (aka "<a href="https://uploadvr.com/waveguides-smartglasses/">waveguide</a>") technology. The
control has onboard electronics (such as GPS) and connections to Magic Leap's
cloud, which, I am told, map both the users and their physical environments to maximal
depths, surely for purposes far beyond what any of us can guess at.</p>

<p>I'll spare you other details, most of which you can read about elsewhere. (One
place to start: <a href="https://www.kguttag.com/">Karl Guttag</a>'s <a href="https://www.kguttag.com/2018/02/22/magic-leap-one-tunnel-vision/">Magic Leap
One — FOV and Tunnel Vision</a>.) I have learned enough, so far, to make two
points:</p>

<ol>
<li>Magic Leap's MR experience is of a proprietary and closed digital world
mixed with the free and open physical one.</li>
<li>No matter how appealing they may be, proprietary and closed digital
worlds are all quarantined futures. And maybe this is a good thing, because it limits
the degrees to which it can infect the open world where most useful development takes
place.</li>
</ol>

<p>Those quarantined futures are modeled currently by the <a href="https://en.wikipedia.org/wiki/Video_game_industry">video game industry</a>.
Nearly all electronic gaming in the world today happens with closed and proprietary
applications running on closed and proprietary hardware. The main exception to that
is a conditional one: Windows games running on the same kind of open hardware most
Linux developers and users run. But hey, it's still on Windows, which remains a
closed and proprietary platform, even though it can run on open hardware.</p>

<p>And yes, there are native games that run on Linux, and Windows ones you can run in
emulation. <a href="https://www.linuxjournal.com/search/node?keys=game">We've been
covering both of those here in <em>Linux Journal</em> for decades</a>. Still, those
are beside my point here, which is that the electronic gaming industry is a vast
mosaic of walled gardens. On the MR front, in addition to Magic Leap, there's <a href="https://www.microsoft.com/en-us/windows/windows-mixed-reality">Windows Mixed
Reality</a> (formerly <a href="https://en.wikipedia.org/wiki/Microsoft_HoloLens">Microsoft HoloLens</a>), <a href="https://en.wikipedia.org/wiki/HTC_Vive">HTC Vive</a>, <a href="https://www.lenovo.com/us/en/daydreamvr/">Lenovo Daydream Mirage</a>, <a href="https://en.wikipedia.org/wiki/Oculus_Rift">Oculus Rift</a>, <a href="https://en.wikipedia.org/wiki/PlayStation_VR">Sony Playstation VR</a> and <a href="https://en.wikipedia.org/wiki/Samsung_Gear_VR">Samsung Gear VR</a>.</p>

<p>Each points toward a future that presumably requires massive investments in
science and manufacture: investments that can be recouped only by trapping
customers inside corporate gardens, each walled in by patents, proprietary commercial
licenses and restrictive one-sided agreements between owners of those gardens and
their paying visitors.</p>

<p>So the smart guess is that the primary use for all of them will be gaming. I won't
wish them good luck with that, because they'll have it. What they won't have is any
more leverage into the open world than we already see with gaming on headset-less
hardware—in other words, limited and likely to stay that way. In its quarantined
state, the gaming world has thrived in blissful near-oblivity to the FOSS world that
grew outside its gardens—for going on 50 years. (The first video game console was
the <a href="https://en.wikipedia.org/wiki/Magnavox_Odyssey">Magnavox Odyssey</a>,
released in 1972, and built to run on the open-source hardware of its time:
televisions.)</p>

<p>Our job in this space is to write and build the tech required for a free and open
digital world we can mix with the free and open physical one we entered at birth. In
other words, we need to do for all those two-letter acronyms what Linus did for UNIX
with Linux.</p>

<p>Can we do that at a time when nearly all the big bucks are flowing to
companies making closed worlds, each on the old proprietary mainframe model that
personal computing, the internet and the web were all designed to obsolesce?</p>

<p>My first source of optimism in that midst is <a href="https://liambroza.com/">Liam
Broza</a>, (<a href="https://twitter.com/LiamBroza">@LiamBroza</a>), co-founder of <a href="https://bitscoop.com">Bitscoop Labs</a> and main developer of <a href="https://lifescope.io">LifeScope</a>. He's the guy who brought the Magic Leap
One to IIW, treated a bunch of us to experiences with it, and salted those
experiences with dark warnings about what will likely become of our virtual worlds if
only tech's scary giants and their billionaire friends provide them.</p>


<p>I love that Bitscoop and LifeScope aren't just about FOSS, but start, as Linus
did, with the individual. Says the home page, "Control of your personal data is a
human right. LifeScope is an open platform for personal data whereby ownership is
returned to the user." LifeScope's <a href="https://lifescope.io/manifesto/">Manifesto</a> goes farther. Here's the whole
thing:</p>

<blockquote>
<p><strong>Built by millions of individuals for everyone to use</strong></p>

<p>The internet is the single most inspiring achievement of engineering and
collaboration in human history. Contained within the internet's data is both the
promise of enriching the human condition as well as the danger of spreading
misinformation, seeding divisiveness, and propagating mass manipulation. Our photos,
emails, social media, biometrics, geolocation, and more tell the story of us. It is
our personal contribution to the global scale dataset of human interaction. But each
of us can only see and control a small fraction of the overwhelming data cloud we
give off. We have left a record of reality in a digital memory we can't
trust.</p>

<p><strong>Silicon Valley can't be trusted with our history</strong></p>

<p>We create everything on the internet, but we have power over none of it. Large
organizations gather our individual data to understand and control our psychographic
and psychometric profiles. Incumbent powers use machine learning to gain insights and
influence over our behavior to advance their own agenda. A handful of giant companies
are centralizing command of the internet, and our courts and government are going
along with it. We, as a civilization, are at a crossroads between Black Mirror and
Star Trek.</p>

<p><strong>There is a better way</strong></p>

<p>By incorporating the power of big data, blockchain, and machine learning,
LifeScope gives everyone a perfect digital memory allowing a truly objective
reflection of themselves. As our data become more organized we become more
directionalized. We are seeing a million fold efficiency in human understanding. Who
am I? How do we fit together? Data can give us better answers. Trustworthy, complete,
and organized data can tell our true story. Freedom, privacy, decentralization, and
openness are the values that drive us. We aim to work together with people and
organizations everywhere who share these goals to restore trust and restore control
over our personal data.</p>
</blockquote>

<p>In the FOSS tradition, <a href="https://twitter.com/lifescopelabs">@LifescopeLabs</a> on Twitter throws credit
toward developers of a similar mind, for example, <a href="https://twitter.com/mozilla">@Mozilla</a>, <a href="https://twitter.com/mozillareality">@MozillaReality</a> (the Mozilla mixed
reality team), <a href="https://twitter.com/mozillareality">@TensorFlow</a> ("a fast,
flexible, and scalable open-source machine learning library for research and
production") and other allies, including <a href="https://twitter.com/Sketchfab">@Sketchfab</a> ("the largest platform to publish
and find 3D models online"), <a href="https://twitter.com/MongoDB">@MongoDB</a> and
<a href="//twitter.com/graphql">@GraphQl</a>.</p>

<p>Liam presented his Bitscoop and LifeScope work both at IIW (see Sessions 4 and 5
for <a href="http://iiw.idcommons.net/IIW_27_Session_Notes#Session_4_3">Thursday, 25
October</a>) and <a href="http://vrmday2018b.eventbrite.com">VRM Day</a> (which <a href="http://projectvrm.org">ProjectVRM</a> held in advance of IIW). A talk he gave
in July is on YouTube <a href="https://www.youtube.com/watch?v=OeX-2midxK4">here</a>.
His slides from that talk are <a href="https://docs.google.com/presentation/d/1hRcv22xlY1yFOOrJaLC1eWoT28sPKOkLc3ry--X03tY/edit#slide=id.g3da0e2ba50_0_37">here</a>.
I advise checking out all of it. (That's him at IIW in the photo, by the
whiteboard where he detailed the open foundations of his work.)</p>

<div class="caption">
<img alt="Liam Broza" src="12607c.jpg"/>

<p class="caption">Liam
Broza, Co-founder of Bitscoop Labs and Main Developer of Lifescope</p>
</div>


<p>The same goes for everything Evo Heyning (<a href="https://twitter.com/EvoHeyning">@EvoHeyning</a>, <a href="https://twitter.com/amoration">@amoration</a>) touches: <a href="http://lightlodges.com/">Light Lodges</a>(<a href="https://twitter.com/LightLodges">@LightLodges</a>), <a href="http://xrstudio.co/">XRStudio</a> (<a href="https://twitter.com/xrstudiosf?">@XRStudioSF</a>), <a href="https://twitter.com/PlayFlies">@PlayFiles</a>, <a href="http://searls.com/2018_12/toyshoppesystems.com">Toyshoppe Systems</a> (<a href="https://twitter.com/TSsystems">@TSSystems</a>), <a href="https://www.exo.works/">ExO Works</a> (<a href="https://twitter.com/Exo_Works">@Exo_Works</a>), <a href="https://www.hyperledger.org/">Hyperledger</a> (<a href="https://twitter.com/hyperledger">@hyperledger</a>), <a href="https://www.exolever.com/">ExO Lever</a> (<a href="https://twitter.com/theExOLever">@theExOLever</a>), <a href="https://unity3d.com">Unity</a> (<a href="https://twitter.com/unity3d">@Unity3D</a>) and much more. She's my second
source on all this stuff (to which I am still very new). When I asked her by email to
review a draft of this piece, she encouraged me to stress the importance of WebXR,
calling it "the open web solution to walled gardens," adding, "Mozilla's Hubs and
Spoke are just one example—we will see many of the main tech players publishing
to WebXR directly in the future." When I asked for more context on WebXR, she
replied:</p>

<blockquote>
<p>WebXR today probably feels like UNIX early on....the forks are experimental as
hackers try to build tools and platforms on top of the open source code. It's a bit
early web wild Westworld in terms of consistent immersive experiences across
interfaces. Fully 4D interactive experiences on URLs will flip content and media
models as much as YouTube &amp; Netflix changed TV but that will take us through the
next decade to realize. 5G&lt;ubiquity. Smartglasses with full HUDs change our data
integration equation and you are right—it's invasive and deeply intimate.</p>
</blockquote>

<p>Others working in personal AI and adjacent spaces are being added to and updated
frequently on ProjectVRM's <a href="https://cyber.harvard.edu/projectvrm/VRM_Development_Work">VRM Development
Work</a> page as well. Please check those out too, and let us know what you think.
Better yet, tell us what you're working on. If it's free and open, we need it.</p>

  <h3 class="sigil_not_in_toc">About the Author</h3>

  <div class="authorblurb">
  <p>Doc Searls is a veteran journalist, author and part-time academic who spent more than two decades elsewhere on the <em>Linux Journal</em> masthead before becoming Editor in Chief when the magazine was reborn in January 2018. His two books are <em>The Cluetrain Manifesto</em>, which he co-wrote for Basic Books in 2000 and updated in 2010, and <em>The Intention Economy: When Customers Take Charge</em>, which he wrote for Harvard Business Review Press in 2012. On the academic front, Doc runs ProjectVRM, hosted at Harvard's Berkman Klein Center for Internet and Society, where he served as a fellow from 2006–2010. He was also a visiting scholar at NYU's graduate school of journalism from 2012–2014, and he has been a fellow at UC Santa Barbara's Center for Information Technology and Society since 2006, studying the internet as a form of infrastructure.</p>
<img alt="Doc Searls" src="12607aa.jpg"/></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../293/toc293.html">Issue Table of Contents</a>
    <a class="link3" href="../293/12607.html">Article</a>
  </div>
  <div class="bottomhrdiv"></div>

  <div id="bottom_search">
    <table class="page_search" summary="">
      <tr>
        <td valign="top" align="left">
          <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
        </td>
        <td valign="top" align="right">
          <form method="get" action="/zoom/search.cgi">
            <input type="hidden" name="zoom_sort" value="0" />
            <input type="hidden" name="zoom_xml" value="0" />
            <input type="hidden" name="zoom_per_page" value="10" />
            <input type="hidden" name="zoom_and" value="1" />
            Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
            <input type="submit" value="Submit" />
          </form>
        </td>
      </tr>
    </table>
  </div>
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>

  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
</body>
</html>