<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
FS-Cache and FUSE for Media Playback QoS
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Use FS-Cache to remove fluctuating performance issues from media&#10;playback.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x127a580.0x1371ab0"></a>
FS-Cache and FUSE for Media Playback QoS
</h1></div><div><div class="author"><h3 class="author">
Ben
 
Martin
</h3></div><div class="issuemoyr">Issue #162, October 2007</div></div><div><p>
Use FS-Cache to remove fluctuating performance issues from media
playback.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1372240"></a></h2></div></div><p>
The FS-Cache Project works with network filesystems like NFS to
maintain a local on-disk cache of network files. The project is split
into a kernel module (fscache) and a d&aelig;mon (cachefilesd), which help
to maintain the disk cache. The local on-disk cache is maintained
under a directory on a local filesystem. For example, the /var/fscache
directory on the ext3 filesystem /var. The filesystem containing the
fscache directory must have the ability to use Extended
Attributes (EAs). Such filesystems are quite common and include ext3 and
xfs.
</p><p>
Early Fedora Core 6 kernel RPMs contained the fscache kernel
module. Unfortunately, around version 2.6.18-1.2868.fc6 of the updated
kernels, the module was no longer included. Fedora 7 kernels do not
include the kernel module. Hopefully in the future, this module will be
available again in standard Fedora kernels. The Fedora Core 6 update
kernel 2.6.20-1.2948.fc6 has an FS-Cache patch included, but it does not
include the kernel module.
</p><p>
Patches are available for the Linux kernel for the FS-Cache kernel
module (see Resources).
</p><p>
The cachefilesd d&aelig;mon communicates with the kernel module using
either a file in /proc (/proc/fs/cachefiles) or a device file
(/dev/cachefiles). Version 0.7 and earlier versions of cachefilesd could 
communicate only via the proc file; Version 0.8 also can use the device
file if it is available with fallback to the proc file.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1372500"></a>
Setting Up cachefilesd</h2></div></div><p>
For Fedora Core 6 and Fedora 7, there is a cachefilesd RPM.
Installation without package management should be fairly easy also, as
the d&aelig;mon mainly consists of a single executable and a configuration
file (/etc/cachefilesd.conf).
</p><p>
The two main things that need to be set up in the configuration file are the path
of the directory to use under which to store the filesystem cache and
options for controlling how much space is acceptable to use on the filesystem containing
the cache directory. You also can supply a tag
for the cache if you want to have multiple local disk caches operating
at the same time.
</p><p>
The space constraints all have acceptable defaults, so the cache
directory is the only configuration option you need to pay attention
to. Make sure that this directory is acceptable for storing caches and
that it exists prior to trying to start cachefilesd. For a media PC,
using a directory on a Flash memory card or on a RAM disk is a
good option.
</p><p>
Because the cache directory must have extended attributes, and your
tmpfs might not include support for them, you may have to create an
ext3 filesystem in a single file inside your tmpfs filesystem and then
use the embedded ext3 filesystem for the cachefilesd path. The ext3 filesystem
inside the single file happily will support extended attributes. Because
the whole ext3 filesystem is in a single file on a RAM disk, it will not
cause distracting disk IO on the media PC.
</p><p>
The fstab entry in Listing 1 sets up both a 64MB of RAM
filesystem and the mountpoint for the embedded ext3 filesystem. The
commands shown in Listing 2 set up the embedded
ext3 filesystem. As the cache.ext3fs filesystem exists only in RAM,
you have to add these commands to your /etc/rc.local or a
suitable boot-time script to set up the cache directory after a reboot.
This script has to be called before cachefilesd is started. Leaving
cachefilesd out of your standard init run-level startups and starting
it manually from the rc.local just after you set up the cache.ext3fs
embedded filesystem is a good solution.
</p><p>
If the cache directory is on a persistent filesystem, such as /var, 
set cachefilesd to start automatically, as shown in
Listing 3.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x13727c0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 1. Using a RAM Disk to Store the Local fscache On-Disk
Cache</b></p><pre     class="programlisting">
tmpfs /var/fscache tmpfs size=64m,user,user_xattr   0 0
/var/fscache/cache.ext3fs /var/fscache/cache 
 &#8618;ext3 loop=/dev/loop1,user_xattr,noauto 0 0
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x13729d0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 2. Setting Up the Embedded ext3 Filesystem</b></p><pre     class="programlisting">
# mount /var/fscache
# cd /var/fscache
# dd if=/dev/zero of=cache.ext3fs \
      bs=1024 count=65536
# mkfs.ext3 -F cache.ext3fs 
# mount cache.ext3fs
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1372be0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 3. Starting the cachefilesd D&aelig;mon and Setting It to Auto-Start
Next Boot</b></p><pre     class="programlisting">
$ su -l
# service cachefilesd start
# chkconfig cachefilesd on
</pre></div><p>
The space constraints in the configuration file are used to set the
percentage of available blocks and files on the filesystem containing
the local cache directory that should be used. For each of these two
resource types, there are three thresholds: cull-off, cull-start and
cache-off. When the cull-off limit is reached, no culling of the
disk cache is performed, and when the cull-start limit is reached, 
culling of the disk cache begins. For example, for the disk block type
constraint, setting cull-off at 20% and cull-start at 10% means
that as long as the disk has more than 20% free blocks, nothing
from the cache will be culled. Once the disk reaches 10% free blocks,
cache culling begins to free up some space. If the disk
manages to get to the cache-off limit (say, 5%), the cache will be
disabled until there is more cache-off space available again.
</p><p>
The configuration options are prefixed with b for block type
constraint and f for the files-available constraint. The
configuration file has a slightly different naming method from that used above. For
block constraints, the cull-off limit is called brun. For cull-start,
the limit is called bcull, and cache-off is called bstop.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1372ea0"></a>
Modifying Mounts</h2></div></div><p>
To turn on FS-Cache for a mountpoint, you have to pass it the fsc mount
option. I noticed that I had to enable FS-Cache for all mountpoints
for a given NFS server, or FS-Cache would not maintain its cache. This
should not be much of an issue for a machine being used as a media PC,
because you likely will not mind having all NFS mounts from the file
server being cached.
</p><p>
The fstab entry shown in Listing 4 includes
the fsc option. Adding this fsc option to all mountpoint references
to fileserver:/... will enable FS-Cache.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1373000"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 4. fstab Entry for Mounting an NFS Directory on the Fileserver
with FS-Cache</b></p><pre     class="programlisting">
fileserver:/foo  /foo  nfs bg,intr,soft,fsc  0 0
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1373210"></a>
Preemptive Caching</h2></div></div><p>
At this stage, FS-Cache will store a local cache copy of files, or part
thereof, which is read off the file server. What we really want is to
have data from files we are viewing on the media PC to be read ahead
into the local disk cache. 
</p><p>
To get information into the local disk cache, we can use a FUSE module
as a shim between the NFS mountpoint and the application viewing the
media. With FUSE, you can write a filesystem as an application in the
user address space and access it through the Linux kernel just like
any other filesystem. To keep things simple, I refer to the
application that provides a FUSE filesystem simply as a FUSE module.
</p><p>
The FUSE filesystem should take the path to the NFS filesystem we want
to cache (the delegate) and a mountpoint where the FUSE filesystem is
exposed by the kernel. For example, if we have a /HomeMovies NFS mount
where we store all our digital home movies, the FUSE module might be
mounted on /CacheHomeMovies and will take the path /HomeMovies as the
delegate path.
</p><p>
When /CacheHomeMovies is read, the FUSE module will read the delegate
(/HomeMovies) and show the same directory contents. When the file
/CacheHomeMovies/venice-2001.dv is read, the FUSE module reads the
information from /HomeMovies/venice-2001.dv and returns that.
Effectively, /CacheHomeMovies will appear just the same as /HomeMovies
to an application.
</p><p>
At this stage, we have not gained anything over using /HomeMovies
directly. However, in the read(2) implementation of the FUSE module, we
could just as easily ask the delegate (/HomeMovies) to read in what
the application requested and the next 4MB of data. The FUSE module
could just throw away that extra information. The mere act of the FUSE
module reading the 4MB of data will trigger FS-Cache to read it over
the network and store it in the local disk cache.
</p><p>
The main advantage of using FUSE is to allow caching to work properly
when the video playback is sought. The main disadvantage is the extra
local copying where the FUSE module asks for more information than is
returned to the video player. This can be mitigated by having the FUSE
module request only the extra information every now and then&mdash;for
example, reading ahead only when 2MB of data has been consumed by the
application.
</p><p>
For optimal performance, the read-ahead should happen either in a
separate thread of control in the FUSE module and use readahead(2) or
asynchronous IO, so that the video playback application is not
blocked waiting for a large read-ahead request to complete.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1373528"></a>
The FUSE Shim</h2></div></div><p>
The fuselagefs package is a C++ wrapper for FUSE. It includes the
Delegatefs superclass, which provides support for implementing FUSE
modules that take a delegate filesystem and add some additional
functionality. The Delegatefs is a perfect starting point for writing
simple shim filesystems like the above nfs-readahead FUSE module.
</p><p>
The read-ahead algorithm is designed to read 8MB using asynchronous IO,
and when the first 4MB of that is shown to the application using the
FUSE filesystem, it then reads another 8MB using asynchronous IO. So there
should be, at worst, 4MB of cached data always available to the FUSE
module.
</p><p>
The C++ class to implement the shim is about 70 lines of code, as shown in
Listing 5. Two offsets are declared to keep
track of what the file offset was in the previous call to fs_read()
and at what offset we should launch another asynchronous read-ahead
call. The aio_buffer_sz is declared constant as an enum so it can use it to
declare the size of aio_buffer. When aio_consume_window
bytes of the aio_buffer are shown to the application using the FUSE
filesystem, another read-ahead is performed. If
debug_readahread_aio is true, the FUSE module explicitly waits for
the asynchronous read-ahead to finish before returning. This is handy
when debugging to ensure that the return value of the asynchronous IO is
valid. A non-illustrative example would have some callback report if
an asynchronous IO operation has failed.
</p><p>
The main job of schedule_readahread_aio() is possibly to execute a
single asynchronous read-ahead call. It updates m_startNextAIOOffset to
tell itself when the next asynchronous read-ahead call should be made.
The forceNewReadAHead parameter allows the caller to force a new
asynchronous
read-ahead for cases such as when a seek has been performed.
</p><p>
The fs_read() method is a virtual method from Delegatefs. It has
similar semantics to the pread(2) system call. Data should be read
into a buffer of a given size at a nominated offset. The fs_read()
method is called by FUSE indirectly. The main logic of our fs_read()
is to check whether the given offset is in a logical sequence from the last
read call. If the offset is not sequential from the last byte returned
from the previous read(), fs_read() will force
schedule_readahread_aio() to perform another
read ahead. schedule_readahread_aio() is always called from fs_read()
so it can handle the sliding asynchronous read-ahead window. 
</p><p>
As Delegatefs knows how to read bytes from the Delegate
filesystem, we then can simply return by calling up to the base class.
The remainder of nfs-fuse-readahead-shim.cpp is taken up by parsing
command-line options, and instead of returning from main(), it calls the
main method of a Delegatefs through an instance of the
CustomFilesystem class. The shim is compiled with the Makefile shown
in Listing 6.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x13737e8"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 5. Entire FUSE Shim C++ Class</b></p><pre     class="programlisting">

#include &lt;fuselagefs/fuselagefs.hh&gt;
using namespace Fuselage;
using namespace Fuselage::Helpers;

#include &lt;aio.h&gt;
#include &lt;errno.h&gt;

#include &lt;string&gt;
#include &lt;iostream&gt;
using namespace std;
...
class CustomFilesystem
 :
 public Delegatefs
{
 typedef Delegatefs _Base;
 off_t m_oldOffset;
 off_t m_startNextAIOOffset;
 enum
 {
   aio_buffer_sz = 8 * 1024 * 1024,
   aio_consume_window = aio_buffer_sz / 2,
   debug_readahread_aio = false
 };
 char aio_buffer[ aio_buffer_sz ];
    
 void schedule_readahread_aio( int fd, 
     off_t offset, bool forceNewReadAHead )
 {
   if( m_startNextAIOOffset &lt;= offset 
        || forceNewReadAHead )
   {
     cerr &lt;&lt; "Starting an async read request"
          &lt;&lt; " at offset:" &lt;&lt; offset &lt;&lt; endl;

     ssize_t retval; ssize_t nbytes; 
     struct aiocb arg; 
     bzero( &amp;arg, sizeof (struct aiocb)); 
     arg.aio_fildes = fd;
     arg.aio_offset = offset; 
     arg.aio_buf = (void *) aio_buffer; 
     arg.aio_nbytes = aio_buffer_sz; 
     arg.aio_sigevent.sigev_notify = SIGEV_NONE; 
 
     retval = aio_read( &amp;arg );
     if( retval &lt; 0 )
       cerr &lt;&lt; "error starting aio request!" 
            &lt;&lt; endl;
 
     m_startNextAIOOffset = offset 
        + aio_consume_window;

     if( debug_readahread_aio )
     {
       while ( (retval = aio_error( &amp;arg ) ) 
           == EINPROGRESS )
       {}
       cerr &lt;&lt; "aio_return():" 
            &lt;&lt; aio_return( &amp;arg ) 
             &lt;&lt; endl;
      }
    }
 }
    
public:

 CustomFilesystem()
 :
 _Base(),
 m_startNextAIOOffset( 0 ),
 m_oldOffset( -1 )
 {
 }
    
 virtual int fs_read( const char *path, 
    char *buf, size_t size,
    off_t offset, struct fuse_file_info *fi)
 {
   cerr &lt;&lt; "fs_read() offset:" &lt;&lt; offset
        &lt;&lt; " sz:" &lt;&lt; size &lt;&lt; endl;
   int fd = fi-&gt;fh;

   bool forceNewReadAHead = false;
   if( (offset - size) != m_oldOffset )
   {
     cerr &lt;&lt; "possible seek() between read()s!" 
          &lt;&lt; endl;
     forceNewReadAHead = true;
     aio_cancel( fd, 0 );
   }
   schedule_readahread_aio( fd, offset, 
                            forceNewReadAHead );
   m_oldOffset = offset;
   return _Base::fs_read( path, buf, 
                          size, offset, fi );
 }
};

</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x13739f8"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 6. Makefile for the FUSE Shim</b></p><pre     class="programlisting">
nfs-fuse-readahead-shim: nfs-fuse-readahead-shim.cpp
	g++ nfs-fuse-readahead-shim.cpp \
          -o nfs-fuse-readahead-shim \
          -D_FILE_OFFSET_BITS=64 -lfuselagefs
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1373c08"></a>
Taking It for a Spin</h2></div></div><p>
A simple application that reads from a given file at a predetermined
rate can verify that the cache is being populated as expected, as shown
in Listing 7. There isn't a great deal of error checking
going on, but things that would cause grief, such as failed read()s,
are reported to the console. The application repeatedly reads 4KB
chunks at a time from a nominated file and throws away the result.
Every 256KB status is reported, so that the application can be closed
knowing roughly what byte of the file was last read.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x167fda0"></a></h2></div></div><div class="sidebar"><p class="title"><b>Listing 7. simpleread.cpp Reads from argv[1] at a Nominated usec
Rate in argv[2]</b></p><pre     class="programlisting">

#include &lt;sys/types.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;errno.h&gt;

#include &lt;iostream&gt;
#include &lt;sstream&gt;
using namespace std;

int main( int argc, char** argv )
{
    cerr &lt;&lt; "opening argv[1]:" &lt;&lt; argv[1] &lt;&lt; endl;
    
    long offset = 0;
    int fd = open( argv[1], O_RDONLY );

    unsigned long usec = 10000;
    if( argc &gt; 2 )
    {
        stringstream ss;
        ss &lt;&lt; argv[2];
        ss &gt;&gt; usec;
    }
    cerr &lt;&lt; "using delay of usec:" &lt;&lt; usec &lt;&lt; endl;
    
    const int bufsz = 4096;
    char buf[ bufsz ];
    bool error = false;
    
    while( true )
    {
        ssize_t rc = read( fd, buf, bufsz );
        if( rc &gt; 0 )
        {
            if( error )
            {
                cerr &lt;&lt; "reading resumed" &lt;&lt; endl;
            }
            error = false;
            offset += rc;
        }
        else if( rc == 0 )
        {
            cerr &lt;&lt; "end of file" &lt;&lt; endl;
            exit(0);
        }
        else
        {
            error = true;
            cerr &lt;&lt; "read error:" &lt;&lt; errno 
                 &lt;&lt; " at offset:" &lt;&lt; offset 
                 &lt;&lt; endl;
        }
        usleep( usec );
        if( offset % (1024*256) == 0 )
            cerr &lt;&lt; "offset:" &lt;&lt; offset &lt;&lt; endl;
    }
    return 0;
}

</pre></div><p>
As shown in Listing 8, we first clean out the cache
directory and restart cachefilesd. Then, the NFS share is mounted and
the FUSE shim run against it to create a /Cache-HomeMovies
directory. The FUSE executable is told to remain in the foreground,
which stops FUSE from running it as a d&aelig;mon, allowing standard output
and standard error of the FUSE filesystem to be displayed. We use bash
to put the nfs-fuse-readahead-shim into the background (though still
having its standard outputs redirected into a capture file) and run
the simpleread for a little more than 500KB of data. Then, both the
simpleread and nfs-fuse-readahead-shim are stopped to investigate whether
the cache has been populated as expected.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1680008"></a></h2></div></div><div class="sidebar"><p class="title"><b>Listing 8. Running simpleread against the FUSE Shim</b></p><pre     class="programlisting">

# rm -rf /var/fscache/*
# /etc/init.d/cachefilesd restart
# mount fileserver:/HomeMovies /HomeMovies -o fsc
# nfs-fuse-readahead-shim --fuse-forground \
  -u /HomeMovies /Cached-HomeMovies \
  &gt;|/tmp/nfs-fuse-out 2&gt;&amp;1 \
  &amp;

# simpleread /Cached-HomeMovies/venice-2001.dv 1000
using delay of usec:1000
offset:262144
offset:524288
^C
# fg
^C
# 

</pre></div><p>
The simpleread was stopped after reading only a little more than half a
megabyte. However, the FUSE module has an asynchronous IO call at the
start, requesting 8MB of data be sent to it. Poking around in
/var/fscache for a file with the same size as venice-2001.dv should
reveal the cache file. Comparing the first 8MB of this cache file to
the version on the NFS share should show that the first 8MB is
identical. Note that the local cached file is read first to make sure
that the subsequent use of the NFS share does not populate the
cache file before it is read. This is shown in Listing 9.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1680270"></a></h2></div></div><div class="sidebar"><p class="title"><b>Listing 9. Checking That the Cache Has Read the First 8MB</b></p><pre     class="programlisting">
# cd /var/fscache
# ll -R
...
---------- 1 root root 800M Jun 10 02:19 Ek0...000000
# dd if=./path/to/Ek0...000000 \
   of=/tmp/8mb bs=1024 count=8192
# dd if=/HomeMovies/venice-2001.dv \
   of=/tmp/8mb.real bs=1024 count=8192
# diff /tmp/8mb.real /tmp/8mb
#
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1680480"></a>
Wrap-Up</h2></div></div><p>
One restriction on FS-Cache is that it will not cache files opened with
O_DIRECT or for writing.
</p><p>
By taking advantage of the kernel FS-Cache code, the FUSE module to
handle read-ahead can be very simple to create. The Delegatefs C++
FUSE base class allows one to implement additional
features very easily when applications perform IO.
</p><p>
The FUSE nfs-fuse-readahead-shim module is started just as shown in
Listing 8 and when the --fuse-forground option is not
passed, nfs-fuse-readahead-shim runs silently as a d&aelig;mon.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x127a580.0x1680638"></a></h2></div></div><div class="sidebar"><p class="title"><b>Resources</b></p><p>
Filesystem in USErspace (FUSE): <a href="http://fuse.sourceforge.net" target="_self">fuse.sourceforge.net</a>
</p><p>
FS-Cache: <a href="http://people.redhat.com/~dhowells/cachefs" target="_self">people.redhat.com/~dhowells/cachefs</a> and 
<a href="http://people.redhat.com/~steved/fscache" target="_self">people.redhat.com/~steved/fscache</a>
</p><p>
fuselagefs and Delegatefs: <a href="http://sourceforge.net/project/showfiles.php?group_id=16036&amp;package_id=225200" target="_self">sourceforge.net/project/showfiles.php?group_id=16036&amp;package_id=225200</a>
</p><p>
NFS and fscache Kernel Patches: <a href="http://people.redhat.com/~dhowells/cachefs/patches" target="_self">people.redhat.com/~dhowells/cachefs/patches</a>
</p></div></div></div>
<div class="authorblurb"><p>
Ben Martin has been working on filesystems for more than ten years. He is
currently working toward a PhD combining Semantic Filesystems with
Formal Concept Analysis to improve human-filesystem interaction.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../162/toc162.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>