<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Mainstream Parallel Programming</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Whether you're a scientist, graphic artist,&#10;musician or movie executive, you can benefit from the speed and&#10;price of today's high-performance Beowulf clusters.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1f98580.0x208fab0"></a>
Mainstream Parallel Programming</h1></div><div><div class="author"><h3 class="author">
Michael-Jon
 
Ainsley Hore
</h3></div><div class="issuemoyr">Issue #149, September 2006</div></div><div><p>
Whether you're a scientist, graphic artist,
musician or movie executive, you can benefit from the speed and
price of today's high-performance Beowulf clusters.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x2090298"></a></h2></div></div><p>
When Donald Becker introduced the idea of a Beowulf cluster while
working at NASA in the early 1990s, he forever changed the face of
high-performance computing.  Instead of institutions forking out millions
of dollars for the latest supercomputer, they now could spend hundreds
of thousands and get the same performance.  In fact, a quick scan of the
TOP500 project's list of the world's fastest supercomputers shows just how
far-reaching the concept of computer clusters has become.  The emergence
of the Beowulf cluster&mdash;a computer cluster created from off-the-shelf
components and that runs Linux&mdash;has had an unintended effect as well.
It has captivated the imaginations of computer geeks everywhere, most
notably, those who frequent the Slashdot news site.
</p><p>
Unfortunately, many people believe that Beowulfs don't lend
themselves particularly well to everyday tasks.  This does have a bit
of truth to it.  I, for one, wouldn't pay money for a version of <span   class="emphasis"><em>Quake
4</em></span> that runs on a Beowulf! On one extreme, companies such as Pixar use
these computer systems to render their latest films, and on the
other, scientists around the world are using them this minute to
do everything from simulations of nuclear reactions to the
unraveling of the human genome.  The good news is that high-performance computing doesn't have to be confined to academic
institutions and Hollywood studios.
</p><p>
Before you make your application parallel, you should consider
whether it really needs to be.  Parallel applications normally
are written because the data they process is too large for conventional
PCs or the processes involved in the program require a large amount
of time.  Is a one-second increase in speed worth the effort of
parallelizing your code and managing a Beowulf cluster? In many
cases, it is not.  However, as we'll see later in this article, in a
few situations, parallelizing your code can be done with a minimum
amount of effort and yield sizeable performance gains.
</p><p>
You can apply the same methods to tackle image processing, audio
processing or any other task that is easily broken up into parts.  As
an example of how to do this for whatever task you have at hand, I
consider applying an image filter to a rather large image of your
friend and mine, Tux.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x2090500"></a>
A Typical Setup</h2></div></div><p>
The first thing we need is a set of identical computers running
Linux, connected to each other through high-speed Ethernet.  Gigabit
Ethernet is best.  The speed of the network connection is one factor
that can bring a fast cluster to a crawl.  We also need a shared
filesystem of some sort and some clustering software/libraries.  Most
clusters use NFS to share the hard drive, though more exotic
filesystems exist, like IBM's General Parallel Filesystem (GPFS).  For
clustering software, there are a few available choices.  The standard
these days is the Message Passing Interface (MPI), but the Parallel
Virtual Machine (PVM) libraries also should work just fine.  MOSIX and
openMOSIX have been getting a lot of attention lately, but they are
used primarily for programs that are not specifically written to run
on clusters, and they work by distributing threads in multithreaded
programs to other nodes.  This article assumes you have MPI
installed, though the process for parallelizing an algorithm with PVM
is exactly the same.  If you have never installed MPI, Stan Blank and
Roman Zaritski have both written nice articles on how to set up an
MPI-based cluster on the <i  >Linux Journal</i> Web site (see
the on-line Resources).
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x2090660"></a>
Initializing the Program</h2></div></div><p>
At the beginning of each MPI program are a few calls to subroutines
that initialize communication between the computers and figure out
what &ldquo;rank&rdquo; each node is.  The rank of a node is a number that
identifies it uniquely to the other computers, and it varies from 0 to
one less than the total cluster size.  Node 0 typically is called the
master node and is the controller for the process.  After the
program is finished, you need to make one additional call to finish
up the process before exiting.  Here's how it's done:

<pre     class="programlisting">

#include &lt;mpi.h&gt;
#include &lt;stdlib.h&gt;

int main (void) {

           int myRank, clusterSize;
           int imgHeight, lowerBoundY, upperBoundY,
                boxSize;

          // Initialize MPI
         MPI_Init((void *) 0, (void *) 0);

         // Get which node number we are.
         MPI_Comm_rank(MPI_COMM_WORLD, &amp;myRank);

         // Get how many total nodes there are.
         MPI_Comm_size(MPI_COMM_WORLD, &amp;clusterSize);

        // boxSize - the amount of the image each node
        //                  will process
        boxSize = imgHeight / clusterSize;

         // lowerBoundY - where each node starts processing.
         lowerBoundY = myRank*boxSize;

        // upperBoundY - where each node stops processing.
        upperBoundY = lowerBoundY + boxSize;

        // Body of program goes here

        // Clean-up and exit:
        MPI_Finalize(MPI_COMM_WORLD);
        return 0;
}

</pre>
</p><p>
This code runs independently on each machine that is part of the
process, so the values for lowerBoundY and upperBoundY will vary on
each machine.  These will be used in the next section.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x2090870"></a>
Breaking Up the Image</h2></div></div><div       class="mediaobject"><a href="9082f1.large.jpg"><img src="9082f1.jpg"></a><div class="caption"><p>
Figure 1.  The process of breaking up a problem into smaller chunks is
called domain decomposition, and it usually is done as shown here.
</p></div></div><p>
Applying a filter to an image can take minutes or even hours to
complete, depending on the complexity of the filter, the size of the
image and the speed of the computer.  To fix these problems, we need to get smaller chunks of the image to several computers to help
speed up the task.  Figure 1 shows common ways of doing this&mdash;we'll
cut our image into strips.  If we have one large image file, we can do
this in C/C++, like this:

<pre     class="programlisting">

FILE *imageFile = fopen("image_in.ppm", "rb");

// Safety check.
if (imageFile != NULL) {

   // Read in the header.
   fread(imageHeader, sizeof(char),
                    HEADER_LENGTH, imageFile);

   // fseek puts us at the point in the image
   // that this node will process.
   fseek(imageFile, lowerBoundY*WIDTH*3,
             SEEK_SET);

   // Here is where we read in the colors:
   // i is the current row in the image.
   // j is the current column in the image.
   // k is the color, 0 for red, 1 for blue,
   //    and 2 for green.
   for (i=0; i&lt;boxSize+1; i++) {
      for (j=0; j&lt;WIDTH; j++) {
         for(k=0; k&lt;3; k++) {
           fread(&amp;byte, 1, 1, imageFile);
            pixelIndex = i*WIDTH+j+k;
            origImage[pixelIndex] = byte;
         }
      }
   }
}
fclose(imageFile);

</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x2090b88"></a>
Applying the Filter</h2></div></div><p>
Now that we have each node storing a piece of the image for
processing, we need to apply the filter to the image.  The GIMP
Documentation Team has done a good job of describing how to do this
using a convolution matrix in the GIMP Documentation.  Many image
effects&mdash;such as sharpen, blur, Gaussian blur, edge detect and
edge enhance&mdash;have unique matrices that provide the desired effect.
The convolution matrix works by studying each pixel of an image and
changing its value based on the values of neighboring pixels.  We
consider the edge detect matrix in this article, shown in Figure 2.
</p><div       class="mediaobject"><img src="9082f2.jpg"><div class="caption"><p>
Figure 2.  This matrix represents the edge detect filter.  The red
square represents the pixel to be considered, and the other numbers
represent the neighboring pixels.
</p></div></div><p>
When we apply this filter to the image, we multiply each pixel by &ndash;4
and add the values of the pixels above, below and to the left and
right to that.  This becomes the new value of the pixel.  Because there
are zeros in the corners of the matrix, we can simplify our program
and get better performance by skipping those calculations.  Below I've
shown how this is done in practice.  In Figure 3, I show what the
filter does to our image of Tux.

<pre     class="programlisting">

for (i=0; i&lt;boxSize; i++) {
   for (j=0; j&lt;WIDTH; j++) {
      if (i&gt;0 &amp;&amp; i&lt;(HEIGHT-1) &amp;&amp;
          j&gt;0 &amp;&amp; j&lt;(WIDTH-1)){

        // Now we apply the filter matrix

        // First to the current pixel.
        pixelIndex = i*WIDTH + j;
        r = origImage[pixelIndex];
        g = origImage[pixelIndex+1];
        b = origImage[pixelIndex+2];
        filter_r = -4*r;
        filter_g = -4*g;
        filter_b = -4*b;

        // Next to the left neighbor.
        pixelIndex = i*WIDTH + j - 1;
        r = origImage[pixelIndex];
        g = origImage[pixelIndex+1];
        b = origImage[pixelIndex+2];
        filter_r += 1*r;
        filter_g += 1*g;
        filter_b += 1*b;

        // Next to the right neighbor.
        pixelIndex = i*WIDTH + j + 1;
        r = origImage[pixelIndex];
        g = origImage[pixelIndex+1];
        b = origImage[pixelIndex+2];
        filter_r += 1*r;
        filter_g += 1*g;
        filter_b += 1*b;

        // The neighbor above.
        pixelIndex = (i-1)*WIDTH + j;
        r = origImage[pixelIndex];
        g = origImage[pixelIndex+1];
        b = origImage[pixelIndex+2];
        filter_r += 1*r;
        filter_g += 1*g;
        filter_b += 1*b;

        // The neighbor below.
        pixelIndex = (i+1)*WIDTH + j;
        r = origImage[pixelIndex];
        g = origImage[pixelIndex+1];
        b = origImage[pixelIndex+2];
        filter_r += 1*r;
        filter_g += 1*g;
        filter_b += 1*b;
      }

      // Record the new pixel.
      pixelIndex = i*WIDTH + j;
      filterImage[pixelIndex]   = filter_r;
      filterImage[pixelIndex+1] = filter_g;
      filterImage[pixelIndex+2] = filter_b;
   }
}

</pre>
</p><p>
We can mimic the readImage() subroutine to create a writeImage() subroutine to write the image to disk in chunks.
</p><div       class="mediaobject"><a href="9082f3.large.jpg"><img src="9082f3.jpg"></a><div class="caption"><p>
Figure 3.  On the left is our original image of Tux, and on the right
is the image after we apply the edge detect filter.
</p></div></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x2091108"></a>
Compiling and Running Your Code</h2></div></div><p>
Both of the popular MPI distributions&mdash;LAM and MPICH&mdash;include
wrapper scripts to allow users to compile their programs easily with
the required MPI libraries.  These wrapper scripts allow you to pass
parameters to GCC like you always do:
</p><div class="itemizedlist"><ul type="disc"><li><p>
mpicc: for C programs
</p></li><li><p>
mpi++: for C++ programs
</p></li><li><p>
mpif77: for FORTRAN 77 programs
</p></li></ul></div><p>
Use mpirun to execute your newly compiled program.  For example, I
compiled my code with the command <tt  >mpicc -O3 -o parallel parallel.c</tt>
and then executed it with <tt  >mpirun n0 ./parallel</tt>.  The n0 signifies
that the program is to run on node 0 only.  To run it on additional
nodes, you can specify a range like n0-7 (for eight processors) or use
<tt  >mpirun C</tt> to signify that the program is to run on all available nodes.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x20915d8"></a>
Performance Gains</h2></div></div><p>
So, with only a few simple MPI calls, we have parallelized
our image filter algorithm very easily, but did we gain any performance? There
are a few ways that we can gain performance.  The first is in terms of
speed, and the second is in terms of how much work we can do.  For
example, on a single computer, a 16,000 x 16,000 pixel image would
require an array of 768,000,000 elements! This is just too much for
many computers&mdash;GCC complained to me that the array was simply too
big! By breaking the image down as we did above, we can ease memory
requirements for our application.
</p><p>
I tested the code above on a 16-node Beowulf cluster running Fedora
Core 1.  Each node had 1.0GB of RAM, a 3.06GHz Pentium 4 processor
and was connected to the other nodes through Gigabit Ethernet.  The
nodes also shared a common filesystem through NFS.  Figure 4 shows the
amount of time required to read in the image, process it and write
it back to disk.
</p><div       class="mediaobject"><a href="9082f4.large.jpg"><img src="9082f4.jpg"></a><div class="caption"><p>
Figure 4.  Shown here are the times that the program took for various
image sizes and various cluster sizes.  Image sizes ranged from 1,600 x
1,600 pixels to 16,000 x 16,000 pixels.  A minimum of four nodes was
required for the largest image.
</p></div></div><p>
From Figure 4, we can see that parallelizing this image filter sped
things up for even moderately sized images, but the real performance
gains happened for the largest images.  Additionally, for images of more than
10,000 x 10,000 pixels, at least four nodes were required due to memory
constraints.  This figure also shows where it is a good idea to
parallelize the code and where it was not.  In particular, there was
hardly any difference in the program's performance from 1,600 x 1,600
pixel images to about 3,200 x 3,200 pixel images.  In this region, the
images are so small that there is also no benefit in parallelizing
the code from a memory standpoint, either.
</p><p>
To put some numbers to the performance of our image-processing
program, one 3.06GHz machine takes about 50 seconds to read, process
and write a 6,400 x 6,400 image to disk, whereas 16 nodes working
together perform this task in about ten seconds.  Even at 16,000 x
16,000 pixels, 16 nodes working together can process an image faster
than one machine processing an image 6.25 times smaller.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x20919a0"></a>
Possible Mainstream Applications</h2></div></div><p>
This article demonstrates only one possible way to take advantage of
the high performance of Beowulf clusters, but the same concepts are
used in virtually all parallel programs.  Typically, each node reads
in a fraction of the data, performs some operation on it, and either
sends it back to the master node or writes it out to disk.  Here are
four examples of areas that I think are prime candidates for
parallelization:
</p><div class="orderedlist"><ol type="1"><li><p>
Image filters: we saw above how parallel processing can
tremendously speed up image processing and also can give users the
ability to process huge images.  A set of plugins for applications
such as The GIMP that take advantage of clustering could be very useful.
</p></li><li><p>
Audio processing: applying an effect to an audio file also
can take a large amount of time.  Open-source projects such as Audacity
also stand to benefit from the development of parallel plugins.
</p></li><li><p>
Database operations: tasks that require processing of large
amounts of records potentially could benefit from parallel processing
by having each node build a query that returns only a portion of the
entire set needed.  Each node then processes the records as needed.
</p></li><li><p>
System security: system administrators can see just how secure
their users' passwords are.  Try a brute-force decoding of /etc/shadow
using a Beowulf by dividing up the range of the checks across several
machines.  This will save you time and give you peace of mind
(hopefully) that your system is secure.
</p></li></ol></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x239d8a0"></a>
Final Remarks</h2></div></div><p>
I hope that this article has shown how parallel programming is
for everybody.  I've listed a few examples of what I believe to be
good areas to apply the concepts presented here, but I'm sure there
are many others.
</p><p>
There are a few keys to ensuring that you successfully parallelize
whatever task you are working on.  The first is to keep network
communication between computers to a minimum.  Sending data between
nodes generally takes a relatively large amount of time compared to
what happens on single nodes.  In the example above, there was no
communication between nodes.  Some tasks may require it, however.
Second, if you are reading your data from disk, read only what each
node needs.  This will help you keep memory usage to a bare minimum.
Finally, be careful when performing tasks that require the nodes to
be synchronized with each other, because the processes will not be
synchronized by default.  Some machines will run slightly faster than
others.  In the sidebar, I have included some common MPI
subroutines that you can utilize, including one that will help you
synchronize the nodes.
</p><p>
In the future, I expect computer clusters to play an even more
important role in our everyday lives.  I hope that this article has
convinced you that it is quite easy to develop applications for these
machines, and that the performance gains they demonstrate are
substantial enough to use them in a variety of tasks.  I would also
like to thank Dr Mohamed Laradji of The University of Memphis
Department of Physics for allowing me to run these applications on
his group's Beowulf cluster.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1f98580.0x239da58"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Some Essential and Useful MPI Subroutines</b></p><p>
There are more than 200 subroutines that are a part of MPI, and they
are all useful for some purpose.  There are so many calls because MPI
runs on a variety of systems and fills a variety of needs.  Here are
some calls that are most useful for the sort of algorithm
demonstrated in this article:
</p><div class="orderedlist"><ol type="1"><li><p>
<tt  >MPI_Init((void*) 0, (void*) 0)</tt> &mdash; initializes MPI.
</p></li><li><p>
<tt  >MPI_Comm_size(MPI_COMM_WORLD, &amp;clstSize)</tt>
&mdash; returns the size of the cluster in clstSize (integer).
</p></li><li><p>
<tt  >MPI_Comm_rank(MPI_COMM_WORLD, &amp;myRank)</tt> &mdash; returns the rank of the
node in myRank (integer).
</p></li><li><p>
<tt  >MPI_Barrier(MPI_COMM_WORLD)</tt> &mdash; pauses until all nodes in the
cluster reach this point in the code.
</p></li><li><p>
<tt  >MPI_Wtime()</tt> &mdash; returns the time since some undefined event in the
past.  Useful for timing subroutines and other processes.
</p></li><li><p>
<tt  >MPI_Finalize(MPI_COMM_WORLD)</tt> &mdash; halts all MPI processes.  Call this
before your process terminates.
</p></li></ol></div></div><p><span   class="bold"><b>Resources for this article:</b></span> <a href="../149/9135.html" target="_self">/article/9135</a>.
</p></div></div>
<div class="authorblurb"><p>
Michael-Jon Ainsley Hore is a student and all around nice guy at The University
of Memphis working toward an MS in Physics with a concentration in
Computational Physics.  He will graduate in May 2007 and plans on
starting work on his PhD immediately after.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../149/toc149.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>