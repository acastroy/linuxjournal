<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta charset="utf-8"/>
<link href="https://fonts.googleapis.com/css?family=Lateef" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<title>LJ Archive</title>


<link href="../../css/archive.css" type="text/css" rel="stylesheet"/>
<script type="text/javascript" src="../../js/archive.js"></script>
<script type="text/javascript" src="../../js/highlight.js"></script>
</head>

<body class="from_sigil">
  
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>


  <div class="tophrdiv">
  </div>

  
  <div id="top_search">
    <table class="page_search" summary="">
      <tr>
        <td valign="top" align="left">
          <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
        </td>
        <td valign="top" align="right">
          <form method="get" action="/zoom/search.cgi">
            <input type="hidden" name="zoom_sort" value="0" />
            <input type="hidden" name="zoom_xml" value="0" />
            <input type="hidden" name="zoom_per_page" value="10" />
            <input type="hidden" name="zoom_and" value="1" />
            Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
            <input type="submit" value="Submit" />
          </form>
        </td>
      </tr>
    </table>
  </div>

<h1 class="title">
FOSS Project Spotlight: Sawmill, the Data Processing Project</h1>

<div class="caption">
    <img alt="Sawmill Logo" src="12353c.jpg"/>

    <p class="caption"></p>
  </div>

<p>
If you're into centralized logging, you are probably familiar with
the ELK Stack: Elasticsearch, Logstash and Kibana. Just in case
you're not, ELK (or Elastic Stack, as it's being renamed these
days) is a package of three open-source components, each responsible for
a different task or stage in a data pipeline. 
</p>

<p>
Logstash is responsible for aggregating the data from your different data
sources and processing it before sending it off for indexing and storage
in Elasticsearch. This is a key role. How you process your log data
directly impacts your analysis work. If your logs are not structured
correctly and you have not configured Logstash correctly, your logs will
not be parsed in a way that enables you to query and visualize them in
Kibana. 
</p>

<p>
<a href="https://logz.io">Logz.io</a> used to rely heavily on Logstash for ingesting data from our
customers, running multiple Logstash instances at any given time.
However, we began to experience some pain points that ultimately led us
down the path to the project that is the subject of this article:
Sawmill.
</p>

<h3 id="sigil_toc_id_14">
Explaining the Motivation</h3>

<p>
Over time, and as our data pipelines became more complex and heavy, we
began to encounter serious performance issues. Our Logstash configuration
files became extremely complicated, which resulted in extremely long
startup times. Processing also was taking too long, especially in the
case of long log messages and in cases where there was a mismatch between
the configuration and the actual log message. 
</p>

<p>
The above points resulted in serious stability issues, with Logstash
coming to a halt or sometimes crashing. The worst thing about it was that
troubleshooting was a huge challenge. We lacked visibility and felt a
growing need for a way to monitor key performance metrics. 
</p>

<p>
There were additional issues we encountered, such as dynamic
configuration reload and the ability to apply business logic, but suffice
it to say, Logstash was simply not cutting it for us. 
</p>

<h3 id="sigil_toc_id_15">
Introducing Sawmill</h3>

<p>
Before diving into Sawmill, it's important to point out that
Logstash has developed since the time we began working on this project,
with new features that help deal with some of the pain points described
above.
</p>

<p>
So, what is Sawmill?
</p>

<p>
<a href="https://github.com/logzio/sawmill">Sawmill</a> is an open-source Java library for enriching, transforming and
filtering JSON documents. 
</p>

<p>
For Logstash users, the best way to understand Sawmill is as a
replacement of the filter section in the Logstash configuration file.
Unlike Logstash, Sawmill does not have any inputs or outputs to read and
write data. It is responsible only for data transformation. 
</p>

<p>
Using Sawmill pipelines, you can use your groks, geoip, user-agent
resolving, add or remove fields/tags and more, in a descriptive manner,
using configuration files or builders, in a simple DSL, allowing you to
change transformations dynamically.
</p>

<h3 id="sigil_toc_id_16">
Sawmill Key Features</h3>

<p>
Here's a list of the key features and processing capabilities that
Sawmill supports:
</p>

<ul><li>
Written in Java, Sawmill is thread-safe and efficient, and uses caches
where needed.
</li>

<li>
Sawmill can be configured in HOCON or JSON.
</li>

<li>
Sawmill allows you to configure a timeout for long processing using a
configurable threshold.
</li>

<li>
Sawmill generates metrics for successful, failed, expired and dropped
executions, and a metric for processing exceeding a defined threshold.
All metrics are available per pipeline and processor.
</li>

<li>
25+ processors, including grok, geoip, user-agent, date, drop,
key-value, json, math and more.
</li>

<li>
Nine logical conditions, including the basics as well as field-exists,
has-value, match-regex and math-compare.
</li>
</ul>

<h3 id="sigil_toc_id_17">
Using Sawmill</h3>

<p>
Here is a basic example illustrating how to use Sawmill:

</p>
<pre><code>
Doc doc = new Doc(myLog);
PipelineExecutor pipelineExecutor = new PipelineExecutor();
pipelineExecutor.execute(pipeline, doc);
</code>
</pre>
<p></p>

<p>
As you can see, there are a few entities in Sawmill:
</p>

<ul><li>
<em>Doc</em> — essentially a Map representing a JSON.
</li>

<li>
<em>Processor</em> — a single <em>document</em> logical transformation. Either
grok-processor, key-value-processor, add-field and so on.
</li>

<li>
<em>Pipeline</em> — specifies a series of processing steps using an ordered list
of <em>processors</em>. Each <em>processor</em> transforms the
<em>document</em> in some specific
way. For example, a <em>pipeline</em> might have one <em>processor</em> that removes a
field from the <em>document</em>, followed by another <em>processor</em> that renames a
field.
</li>

<li>
<em>PipelineExecutor</em> — executes the <em>processors</em> defined in
the <em>pipeline</em> on a
<em>document</em>. The <em>PipelineExecutor</em> is responsible for the
execution flow—handling onFailure and onSuccess flows, stops on
failure, exposes metrics
of the execution and more.
</li>

<li>
<em>PipelineExecutionTimeWatchdog</em> — responsible for warning on long
processing time, interrupts and stops processing on timeout (not shown in
the example above). 
</li>
</ul>

<h3 id="sigil_toc_id_18">
Sawmill Configuration</h3>

<p>
A Sawmill pipeline can get built from a <a href="https://github.com/lightbend/config/blob/master/HOCON.md">HOCON
string</a> (Human-Optimized
Config Object Notation).
</p>

<p>
Here is a simple configuration snippet, to get the feeling of it:

</p>
<pre><code>
{
"steps": [{
    "grok": {
        "config": {
            "field": "message",
            "overwrite": ["message"],
"patterns":["%{COMBINEDAPACHELOG}+%{GREEDYDATA:extra_fields}"]
           }
        }
    }]
}
</code>
</pre>
<p></p>

<p>
Which is equivalent to the following in HOCON:

</p>
<pre><code>
steps: [{
    grok.config: {
            field : "message"
            overwrite : ["message"]
            patterns :
["%{COMBINEDAPACHELOG}+%{GREEDYDATA:extra_fields}"]
           }
    }]
</code>
</pre>
<p></p>

<p>
To understand how to use Sawmill, here's a simple example showing
GeoIP resolution:

</p>
<pre><code>
package io.logz.sawmill;

import io.logz.sawmill.Doc;
import io.logz.sawmill.ExecutionResult;
import io.logz.sawmill.Pipeline;
import io.logz.sawmill.PipelineExecutor;

import static io.logz.sawmill.utils.DocUtils.createDoc;

public class SawmillTesting {

    public static void main(String[] args) {

        Pipeline pipeline = new Pipeline.Factory().create(
                "{ steps :[{\n" +
                "    geoIp: {\n" +
                "      config: {\n" +
                "        sourceField: \"ip\"\n" +
                "        targetField: \"geoip\"\n" +
                "        tagsOnSuccess: [\"geo-ip\"]\n" +
                "      }\n" +
                "    }\n" +
                "  }]\n" +
                "}");

        Doc doc = createDoc("message", "testing geoip resolving", 
         ↪"ip", "172.217.11.174");
        ExecutionResult executionResult = new
PipelineExecutor().execute(pipeline, doc);

        if (executionResult.isSucceeded()) {
            System.out.println("Success! result 
             ↪is:"+doc.toString());
        }
    }
}
</code>
</pre>
<p></p>

<h3 id="sigil_toc_id_19">
End Results</h3>

<p>
We've been using Sawmill successfully in our ingest pipelines for
more than a year now, processing the huge amounts of log data shipped to us by
our users.  
</p>

<p>
We know Sawmill is still missing some key features, and we are looking
forward to getting contributions from the community. We also realize
that at the end of the day, Sawmill was developed for our specific needs
and might not be relevant for your use case. Still, we'd love to get
your feedback.
</p>

<p>
—Daniel Berman
</p>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../286/toc286.html">Issue Table of Contents</a>
    <a class="link3" href="../286/12353.html">Article</a>
  </div>
  <div class="bottomhrdiv"></div>

  <div id="bottom_search">
    <table class="page_search" summary="">
      <tr>
        <td valign="top" align="left">
          <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
        </td>
        <td valign="top" align="right">
          <form method="get" action="/zoom/search.cgi">
            <input type="hidden" name="zoom_sort" value="0" />
            <input type="hidden" name="zoom_xml" value="0" />
            <input type="hidden" name="zoom_per_page" value="10" />
            <input type="hidden" name="zoom_and" value="1" />
            Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
            <input type="submit" value="Submit" />
          </form>
        </td>
      </tr>
    </table>
  </div>
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>

  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
</body>
</html>