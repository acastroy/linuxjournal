<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Improving Application Performance on HPC Systems with Process
Synchronization</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Hey, cluster node, don't work on that housekeeping task right now&mdash;we're&#10;all waiting&#10;for you to finish your part of the MPI job! Here's a scheduling&#10;policy designed for cluster efficiency.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x17ec580.0x18e3ab0"></a>
Improving Application Performance on HPC Systems with Process
Synchronization</h1></div><div><div class="authorgroup"><div class="author"><h3 class="author">
Paul
 
Terry
</h3></div><div class="author"><h3 class="author">
Amar
 
Shan
</h3></div><div class="author"><h3 class="author">
Pentti
 
Huttunen
</h3></div><div class="issuemoyr">Issue #127, November 2004</div></div></div><div><p>
Hey, cluster node, don't work on that housekeeping task right now&mdash;we're
all waiting
for you to finish your part of the MPI job! Here's a scheduling
policy designed for cluster efficiency.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x17ec580.0x18e4608"></a></h2></div></div><p>
One would expect that doubling the processing power available to an
application would double the application performance or cut the run
time in half. Unfortunately, HPC users know this is far from true,
with actual performance efficiency dropping to as low as only
5% of a system's theoretical peak performance. HPC researchers
and application developers have spent and continue to spend much effort
trying to find the source of these performance losses and boost
sustained application performance. When we set about developing the
Cray XD1 system, we joined the ranks of researchers attacking this
problem. This article describes how we learned from those who went before
us
and how we built on that knowledge to develop a new Linux scheduling-based solution that promises to improve real application
performance significantly on Linux HPC systems.
</p><p>
The majority of research has focused on the structure of the HPC
applications themselves. Various research teams attempted to improve
the efficacy of caching, looked for ways to minimize interprocessor
communications and explored a variety of similar measures, but each
strategy
offered performance gains of only a few percent. Another area of
research has shown particular promise, however. By understanding the interaction
between the HPC application and the system background processes, one
can find ways to modify this interaction to improve performance.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x17ec580.0x18e4768"></a>
Where Does the Performance Go? </h2></div></div><p>
In a seminal paper documenting their research into
this interaction, researchers Petrini, Kerbyson
and Paking of the Los Alamos National Labs (see
the on-line Resources) quantified the loss in application
performance caused by what they deemed &ldquo;noise&rdquo;&mdash;the
interaction between large multiprocess MPI jobs and
background processes. They observed that housekeeping
tasks, or noise, delayed individual processors from
reaching MPI barriers (synchronization points in the
application) and caused all other processors to wait
while one processor finished up its housekeeping. This
resulted in wasted cycles on all other processors.
</p><p>
The top half of Figure 1 illustrates this interaction and how it results
in lost performance. The processes illustrated are part of a parallel
job, each running on a separate processor and periodically synchronizing
through the use of MPI barriers. In the first part of the computation,
Process 1 is delayed because the node's scheduler pauses process execution
to run background processes, such as those found on every Linux or UNIX
node. Processes 2 and 3 also are delayed. Repetition of this pattern
results in substantially reduced sustained application performance.
The magnitude of the impact is a function of the frequency with which
barriers are encountered and the number of processors.
</p><p>
Petrini and colleagues quantified this
loss of performance running SAGE, a compressible Eulerian hydrodynamics
code, on their HPC system, named ASCI Q. ASCI Q is a cluster of 2,048 HP
ES45 nodes, where each node is a four-way SMP. Petrini, et al., observed that
better performance was obtained when they restricted SAGE to run on only
three of the four processors in the SMP when more than 256 nodes were utilized.
They theorized that this result was caused by background noise,
and the theory was verified by eliminating many of the sources of noise
and observing the improvement in performance.
</p><p>
This research points to lack of process synchronization and wait
time as the culprit that is robbing fine-grained and highly parallel
HPC applications of up to 50% (and perhaps more) of their potential
performance. Unfortunately, a means to stop this thievery still
was not at hand. The method employed by Petrini, et al., to identify the
culprit&mdash;restricting the system's freedom to run housekeeping
tasks&mdash;doesn't
present a practical solution for most HPC applications. The prospect of
relegating one-quarter of the processors on an HPC system to running
housekeeping tasks is not palatable to many HPC sites. In addition,
many background processes cannot be removed, limiting the performance
gain achievable using this approach.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x17ec580.0x18e49d0"></a>
Recovering the Missing Performance </h2></div></div><p>
When we set out to build a new high-performance computer, we also set
out to find a way to prevent this performance theft. We considered a
new approach using the Linux scheduler to synchronize scheduling of MPI
jobs and housekeeping tasks. Previous work and our research suggest that
this new synchronized scheduling approach can deliver a 50% or greater
performance boost to many fine-grained parallel applications running on
32 or more processors.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x17ec580.0x18e4ad8"></a>
Implementing a Synchronized Scheduling Policy </h2></div></div><p>
Our approach was to create a new Linux scheduling policy. To achieve
the desired gains, this policy must synchronize the schedulers on
all nodes in a Linux HPC system to ensure that MPI processes run
concurrently on all processes and that Linux housekeeping processes execute
at the same time. Thus, the scheduler must have a means to achieve
global synchronization, as illustrated in Figure 1. To achieve global
synchronization, we designed a feature in the communications processor to
synchronize the clock in each processing node.
</p><p>
The new Linux scheduler policy defines a scheduling frame of 128
time slots, 120 of them reserved for application execution and eight for
housekeeping processes. Schedulers on different processors are able
to align their scheduling frames by exploiting a globally synchronized
clock, which guarantees sub-microsecond variation in time between nodes in
the system. At any moment in time, all processors either are executing
the HPC application or running housekeeping processes (bottom
half of Figure 1).
</p><p>
This approach to process synchronization is scalable to high processor
counts, because scheduling decisions are made locally on each node. This
provides a significant boost to sustained application performance by
eliminating wasted CPU cycles caused by waiting at barriers.
</p><div       class="mediaobject"><a href="7690f1.large.jpg"><img src="7690f1.jpg"></a><div class="caption"><p>
Figure 1. An Example of Asynchronous and Synchronous Execution of
Processes
</p></div></div><p>
The synchronized scheduler is implemented as a new policy augmenting
the three existing policies in the scheduler associated with the
Linux kernel. The Linux scheduler is invoked when the process being
executed is blocked or voluntarily gives up the CPU, when the processor
receives an interrupt or at the end of a 10-millisecond timeslice.
The scheduler selects the next process to run based on the scheduling
policy applicable to that process and its priority. With the new
synchronization policy in place, Linux then selects from one of the
following scheduling policies, two for real-time processing and two
for conventional time-sharing processes, listed in order of decreasing
precedence:
</p><div class="orderedlist"><ol type="1"><li><p>
FIFO (first in, first out): a process marked FIFO runs until it
relinquishes control of the CPU. This priority is used for short
duration, real-time system processes. FIFO processes run ahead
of others.
</p></li><li><p>
Round-robin: a process using this policy receives a 10-millisecond
timeslice, in turn. It is available for real-time processing.
</p></li><li><p>
Synchronized: we added the synchronized policy to enable synchronized
scheduling of processes in a multiprocessor batch job. The workload
management system marks each process as using this policy when it is
started. These processes and their offspring gain the benefits of
synchronized scheduling.
</p></li><li><p>
Priority: priority scheduling is the familiar time-sharing mechanism
known to Linux users. Processes using this scheduling policy have
priorities associated with them and receive time proportional to their
priority. All user processes and virtually all system processes run
under this policy. The scheduler selects the next process to run from
the policy class with the greatest precedence. FIFO and round-robin
system processes run first. Processes marked for synchronized scheduling
run before processes using the normal priority scheduler.
</p></li></ol></div><p>
The new synchronized scheduling policy creates a scheduling frame
that dictates when batch jobs and other user and system processes are
executed. The frame includes a predefined number of time slots that are
cycled through in sequence. A time slot represents 10 milliseconds (one
system timer tick in Linux), during which the process assigned to the
time slot is executed. The current implementation has 128 time slots, 120
for the execution of batch jobs and eight for other processes. During the
latter time slots, the synchronized scheduling policy indicates there
are no runnable batch processes, and the conventional priority scheduling
policy takes over for all other housekeeping processes. When no batch
jobs exist, the behavior of the Cray scheduler is indistinguishable from
the conventional Linux scheduler.
</p><p>
The number of time slots in a scheduling frame is configurable, but it must
be a power of two. The ratio of time slots reserved for batch processing
versus other processes also may be adjusted. Figure 2 illustrates a
typical scheduling frame, with the locations of batch time slots shown in
red and housekeeping time slots in grey.
</p><div       class="mediaobject"><a href="7690f2.large.jpg"><img src="7690f2.jpg"></a><div class="caption"><p>
Figure 2. The Time Slots (128) with Eight Reserved Time Slots
</p></div></div><p>
A scheduling frame is created when the first batch process is started on
a node. All batch time slots are assigned to that process. The creation
of additional batch processes results in an even distribution of time slots
across processes. If <span   class="emphasis"><em>n</em></span> batch processes are created, the first batch
process receives the first 120/<span   class="emphasis"><em>n</em></span> time slots, the second receives the next
120/<span   class="emphasis"><em>n</em></span> time slots and so forth. The synchronized scheduler thus is able
to support batch jobs that require multiple processes on each CPU.
</p><p>
A batch process executes to the end of its allotted time, as long as
it makes no blocking or CPU-yielding system calls. If the batch process
yields the CPU, perhaps as a result of making a blocking system call,
another batch process is scheduled to run. If there are no runnable
batch processes, control passes to the conventional priority scheduler
to run housekeeping processes. Of course, batch processes regain
the CPU if they are unblocked by the handling of an interrupt.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x17ec580.0x18e55d8"></a>
Alignment of Scheduling Frames between Processors </h2></div></div><p>
So far, we have discussed only scheduling of batch jobs and system
processes within a single node. However, to stop the performance thievery,
this synchronized scheduler must work across all processors.
Here, we encounter a critical system design criteria that makes this
synchronized scheduler approach possible&mdash;the availability of global
time synchronization. In our design, global time synchronization is
carried out by communications processors designed within the
HPC system. These processors offload communications processing from
the application processors. Communications processors also run a
time synchronization protocol to achieve global clock
synchronization. Tight time synchronization can be achieved because
the communications processors have control over packet scheduling
and jitter&mdash;the difference in time between any pair of processors
is less than 1 microsecond. A further advantage of delegating time
synchronization to the communications processors is this load is
removed from the processors carrying the application workload, leaving
more time for application processing and further reducing interrupts to
the application processors.
</p><p>
The time synchronization protocol includes additional fields for time slot
alignment. The protocol uses a master-slave paradigm, where one node acts
as the source of the time and time slot information and all other nodes
in the system synchronize themselves to the master node's clock. The time
synchronization packets received from the master identify the time slot
being executed and the time elapsed since the start of the time slot,
enabling precise alignment of scheduling frames across the entire
HPC system.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x17ec580.0x18e5738"></a>
Performance Implications</h2></div></div><p>
This synchronized scheduler delivers synchronized execution of the
processes in a parallel application. How much performance degradation
can be avoided or how much potential performance can be gained is a
function of how frequently the application uses barriers and/or collective
operations, how much time is taken by system housekeeping processes
and the number of processors employed by the application.
</p><p>
Our research indicates significant speedup can be achieved. Figures 3
and 4 show the theoretical speedup that can be achieved through the use
of the synchronized scheduler, relative to the conventional priority
scheduler. Figure 3 assumes that background processing requires 1.5%
of the CPU, and Figure 4 assumes that 6.25% of the CPU is consumed by
background processing&mdash;this is a realistic metric on most clusters.
Curves are shown for applications encountering an average of 100, 200
and 300 barriers per second.
</p><div       class="mediaobject"><a href="7690f3.large.jpg"><img src="7690f3.jpg"></a><div class="caption"><p>
Figure 3. Theoretical Speedups with Process Synchronization with 1.5%
D&aelig;mon CPU Utilization
</p></div></div><div       class="mediaobject"><a href="7690f4.large.jpg"><img src="7690f4.jpg"></a><div class="caption"><p>
Figure 4. Theoretical Speedups with Process Synchronization with
6.25% D&aelig;mon CPU Utilization
</p></div></div><p>
As the number of processors increases, the performance
gain from the synchronized scheduler increases and asymptotically
approaches a maximum value. This reflects the fact that performance
doesn't continue to degrade with the conventional scheduler. After
a certain processor count is reached, the probability of at least
one processor being delayed by housekeeping increases to 100%. The
addition of more processors does not significantly add to the application
delay encountered at barriers.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x17ec580.0x18e5c60"></a>
Conclusions</h2></div></div><p>
By focusing on the interactions between the HPC application and the
system background processes, HPC researchers identified a major culprit
for performance losses in parallel applications. Additional research
identified ways of preventing this thievery, but none to date have provided
successful, real-life implementations. Global process synchronization
using the Linux scheduler eliminates wait time due to noise
and promises significant performance gains. By looking beyond the
application and into the role of the rest of the HPC system, we believe
we have found a scalable, real-life implementation. With Linux process
synchronization using a global clock synchronization and Linux running
on each processing node, the Cray implementation ensures application
processes run concurrently on all processors and housekeeping is
performed concurrently on all processors and bounded in time. Our process
synchronization solution can prevent performance theft and increase
application performance for fine-grained highly parallel applications
running on 32 processors or more by up to 50%.
</p><p>
<span   class="bold"><b>Resources for this article:</b></span> <a href="../127/7756.html" target="_self">/article/7756</a>.
</p></div></div>
<div class="authorblurb"><p>
Dr Paul Terry is the Chief Technology Officer for Cray Canada, Inc.,
previously OctigaBay Systems, which was acquired by Cray in April 2004.
He is a technology strategist for innovative computing architectures and
is responsible for establishing the company's technology vision and
leadership.
</p><p>
Amar Shan, Director of Product Management, Cray, Inc.,
is responsible for introducing Cray's leading-edge technical innovations
and creative business solutions into the marketplace. He has
more than 20 years' experience in the computing and telecommunications
industries in product management, development and architecture roles.

</p><p>
Pentti Huttunen, Benchmarking Specialist at Cray, Inc.,
is responsible for researching parallel computing
technologies and optimizing applications to ensure that they are running
efficiently on a variety of platforms at Cray, Inc.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../127/toc127.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>