<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Scale Out with GlusterFS
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Learn how to install, benchmark and optimize this popular,&#10;shared-nothing and scalable open-source distributed filesystem.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x15c5580.0x16bcac0"></a>
Scale Out with GlusterFS
</h1></div><div><div class="authorgroup"><div class="author"><h3 class="author">
Alex
 
Davies
</h3></div><div class="author"><h3 class="author">
Alessandro
 
Orsaria
</h3></div><div class="issuemoyr">Issue #235, November 2013</div></div></div><div><p>
Learn how to install, benchmark and optimize this popular,
shared-nothing and scalable open-source distributed filesystem.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16bd408"></a></h2></div></div><p>
GlusterFS is an open-source distributed filesystem, originally
developed by a small California startup, Gluster Inc. Two years ago,
Red Hat acquired Gluster, and today, it sponsors GlusterFS as an open-source product with commercial support, called Red Hat Storage
Server. This article refers to the latest community version of
GlusterFS for CentOS, which is 3.4 at the time of this writing. 
The material covered here also applies to Red Hat Storage Server as well as other
Linux distributions.
</p><div       class="mediaobject"><a href="11549f1.large.jpg"><img src="11549f1.jpg"></a><div class="caption"><p>
Figure 1. Simplified Architecture Diagram of a Two-Node GlusterFS Cluster
and Native Client
</p></div></div><p>
Figure 1 shows a simplified architecture of GlusterFS. GlusterFS provides a unified, global namespace that combines
the storage resources from multiple servers. Each node in the GlusterFS
storage pool exports one or more bricks via the
<tt  >glusterfsd</tt>
d&aelig;mon. Bricks are just local filesystems, usually indicated by the
hostname:/directory notation. They are the basic building blocks
of a GlusterFS volume, which can be mounted using the GlusterFS
Native Client. This typically provides the best performance and
is based on the Filesystem In Userspace (FUSE) interface built in to the
Linux kernel. GlusterFS also provides object access via the OpenStack
Swift API and connectivity through the NFS and CIFS protocols.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16bd880"></a></h2></div></div><div class="sidebar"><p class="title"><b>Distributed Filesystems and Metadata Servers</b></p><p>
While most distributed filesystems include a metadata server, GlusterFS
does not. In a filesystem architecture with a metadata server, data is
striped among different nodes, with another server keeping track of the
location of the metadata and controlling access to the storage nodes. When
a client issues an I/O operation to a file, it sends the request to the
metadata server, which in turn tells the client where to retrieve the
data from. With GlusterFS, native clients deterministically find the
correct node where a file is stored via a hashing algorithm. Eliminating
the metadata server is a big advantage, as this is typically a single
point of contact for clients and often becomes a bottleneck.
</p></div><p>
You can increase capacity or IOPS in a GlusterFS storage pool by adding
more nodes and expanding the existing volumes to include the new
bricks. As you provision additional storage nodes, you increase not
only the total storage capacity, but network bandwidth, memory
and CPU resources as well. GlusterFS scales &ldquo;out&rdquo; instead of
&ldquo;up&rdquo;.
</p><p>
Unlike traditional storage arrays, there is no hard limit provided by
the array controller; you are limited by the sum of all your storage
nodes. This scalability results in a small latency penalty per I/O
operation. Therefore, GlusterFS works very well where nearline storage
access is required, such as bandwidth-intensive HPC or general-purpose
archival storage. It is less suited for latency-sensitive applications,
such as highly transactional databases.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16bdbf0"></a>
Getting Started with GlusterFS</h2></div></div><p>
Installing GlusterFS is straightforward. You can download
packages for the most common distributions, including CentOS, Fedora,
Debian and Ubuntu. For CentOS and other distributions with the yum
package manager, just add the corresponding yum repo, as explained in the
official GlusterFS Quick Start Guide (see Resources).
Then, install the glusterfs-server-* package on
the storage servers and glusterfs-client-* on the clients. Finally,
start the <tt  >glusterd</tt> service on the storage nodes:

<pre     class="programlisting">
# service glusterd start
</pre>
</p><p>
The next task is to create a trusted storage pool, which is made from
the bricks of the storage nodes. From one of the servers, for instance
node1, just type:

<pre     class="programlisting">
# gluster peer probe node2
</pre>
</p><p>
And, repeat that for all the other storage nodes. The command
<tt  >gluster peer status</tt> lists all the other peers and their
connection status.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16bdf08"></a>
Provisioning a Volume</h2></div></div><p>
The first step to provision a volume consists of setting up the
bricks. As mentioned previously, these are mountpoints on the
storage nodes, with XFS being the recommended filesystem. GlusterFS uses
extended file attributes to store key/value pairs for several purposes,
such as volume replication or striping across different bricks. To make
this work with XFS, you should format the bricks using a larger inode
size&mdash;for example, 512 bytes:

<pre     class="programlisting">
# mkfs.xfs -i 512 /brick01
</pre>
</p><p>
GlusterFS volumes can belong to several different types. They can be
<span   class="emphasis"><em>distributed</em></span>, <span   class="emphasis"><em>replicated</em></span> or
<span   class="emphasis"><em>striped</em></span>. In addition,
they can be a combination of these&mdash;for instance, <span   class="emphasis"><em>distributed
replicated</em></span> or <span   class="emphasis"><em>striped replicated</em></span>. You can specify the type
of a volume using the <tt  >stripe</tt> and
<tt  >replica</tt> keywords of the
<tt  >gluster</tt> command. There is no keyword to indicate that a volume
is distributed, as this is the default configuration.
</p><p>
Distributed volumes store whole files across different bricks. This
is similar to striping, but the minimum &ldquo;unit of storage&rdquo; in a brick for
a distributed volume is a single file. Conversely, striped volumes
split files in chunks (of 128KB by default) and stripe those across
bricks. Although striped volumes increase complexity, you might use
them if you have very large files that do not fit in a single brick
or in scenarios where a lot of file contention is experienced. The
last volume type, replicated, provides redundancy by synchronously
replicating files across different bricks.
</p><p>
As an example, let's create a distributed replicated volume
using the bricks mounted on the /brick01 and /brick02
directories on two different storage nodes:

<pre     class="programlisting">
# gluster volume create MYVOL replica 2 \
&gt; node{1,2}:/brick01 node{1,2}:/brick02
# gluster volume start MYVOL
</pre>
</p><p>
The <tt  >replica 2</tt> option tells GlusterFS to dedicate half 
the bricks for two-way replication. Keep in mind that
the order of the bricks <span   class="emphasis"><em>does</em></span> matter. In the previous
example, 
<tt  >node1:/brick01</tt> will mirror
<tt  >node2:/brick01</tt>. The same logic
applies to /brick02.
</p><p>
Finally, you can make the new volume available on other machines using
the GlusterFS native client:

<pre     class="programlisting">
# mount -t glusterfs node1:/MYVOL /mnt/DIR
</pre>
</p><p>
You can pass the <tt  >-o backupvolfile-server</tt> option on the previous
command to specify an alternative server from which to mount the filesystem,
should node1 be unavailable.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16be7f8"></a>
Scaling GlusterFS</h2></div></div><p>
GlusterFS offers more flexibility and scalability than a traditional
monolithic storage solution. But, this comes at a complexity cost,
as you now have more control and need to consider several aspects for
best performance.
</p><p>
To check whether performance meets your expectations, start with a
baseline benchmark to measure throughput on the newly created GlusterFS
volume. You can use several tools to do this, such as
<tt  >iozone</tt>,
<tt  >mpirun</tt> or <tt  >dd</tt>. For example, run the following script after
changing directory into a GlusterFS mountpoint:

<pre     class="programlisting">
#!/bin/bash
for BS in 64 128 256 512 1024 2048 4096 8192 ; do
  iozone -R -b results-${BS}.xls -t 16 -r $BS -s 8g \
    -i 0 -i 1 -i 2
done
</pre>
</p><p>
The previous code runs <tt  >iozone</tt> with different block sizes and 16
active client threads, using an 8GB file size to test. To minimize the
effects of caching, you should benchmark I/O performance using a file
size at least twice the RAM installed on your servers. However, as RAM is
cheap and many new servers come equipped with a lot of memory, it may be
impractical to test with very large file sizes. Instead, you can limit
the amount of RAM seen by the kernel to 2GB&mdash;for example, by passing the
<tt  >mem=2G</tt> kernel option at boot.
</p><p>
Generally, you will discover that throughput is either limited by the
network or the storage layer. To identify where the system is constrained,
run <tt  >sar</tt> and <tt  >iostat</tt> in separate terminals for each
GlusterFS node, as illustrated below:

<pre     class="programlisting">
# sar -n DEV 2 | egrep "(IFACE|eth0|eth1|bond0)"
# iostat -N -t -x 2 | egrep "(Device|brick)"
</pre>
</p><p>
Keep an eye on the throughput of each network interface
reported by the <tt  >sar</tt> command above. Also, monitor the utilization
column of the <tt  >iostat</tt> output to check whether any of the bricks are
saturated. The commands <tt  >gluster volume top</tt> and
<tt  >gluster volume profile</tt> provide many other
useful performance metrics. Refer to the documentation on the Gluster
community Web site for more details.
</p><p>
Once you have identified where the bottleneck is on your system, you can do
some
simple tuning.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16b5818"></a>
Tuning the Network Layer</h2></div></div><p>
GlusterFS supports TCP/IP and RDMA as the transport protocols. TCP on
IPoIB (IP over InfiniBand) is also a viable option. Due to space
limitations,
here we mostly refer to TCP/IP over Ethernet, although some considerations
also apply to IPoIB.
</p><p>
There are several ways to improve network throughput on your storage
servers. You certainly should minimize per-packet overhead and balance
traffic across your network interfaces in the most effective way, so
that none of the NIC ports are underutilized. Under normal conditions,
no particular TCP tuning is required, as the Linux kernel enables TCP
window scaling by default and does a good job of automatically adjusting
buffer sizes for TCP performance.
</p><p>
To reduce packet overhead, you should put your storage nodes on the
same network segment as the clients and enable jumbo frames. To do so,
all the interfaces on your network switches and GlusterFS servers must
be set up with a larger MTU size. Typically a maximum value of 9,000
bytes is allowed on most switch platforms and network cards.
</p><p>
You also should ensure that TSO (TCP segment offload) or GSO (generic
segment offload) is enabled, if supported by the NIC driver. Use the
<tt  >ethtool</tt> command with the <tt  >-k</tt> option to verify the current
settings. TSO and GSO allow the kernel to manipulate much larger packet
sizes and offload to the NIC hardware the process of segmenting packets
into smaller frames, thus reducing CPU and packet overhead. Similarly,
large receive offload (LRO) and generic receive offload (GRO) can improve
throughput in the inbound direction.
</p><p>
Another important aspect to take into consideration is NIC bonding. The
Linux bonding driver provides several load-balancing policies, such as
active-backup or round-robin. In a GlusterFS setup, an active-backup
policy may result in a bottleneck, as only one of the Ethernet
interfaces will be used to transmit and receive packets. Conversely,
a pure round-robin policy provides per-packet load balancing and full
utilization of the bandwidth of all the interfaces if required. However,
depending on your network topology, round-robin may cause out-of-order
packet delivery, and your system would require extra CPU cycles to
re-assemble them.
</p><p>
Better alternatives in this case are IEEE 802.3ad dynamic link aggregation
(selected with <tt  >mode=4</tt> in the bonding driver) or adaptive load
balancing (<tt  >mode=6</tt>). To deploy IEEE 802.3ad link aggregation, an
essential requirement is that the switches connected to the servers must
also support this protocol. Then, the
<tt  >xmit_hash_policy</tt> parameter
of the Linux kernel bonding driver specifies the hash policy used to
balance traffic across the interfaces, when 802.3ad is in operation. For
better results, this parameter should be set to
<tt  >layer3+4</tt>. This
policy uses a hash of the source and destination IP addresses and TCP
ports to select the outbound interface. In this way, traffic is load-balanced per flow, rather than per packet, and the end result is that
packets from a certain connection socket always will be delivered out
of the same network interface.
</p><p>
In GlusterFS, when a client mounts a volume using the native protocol,
a TCP connection is established from the client for each brick hosted
on the storage nodes. On an eight-node cluster with two bricks per node, this
results in 16 TCP connections. Hence, the &ldquo;layer3+4&rdquo; hash transmit policy
typically would contain enough &ldquo;entropy&rdquo; to split those connections
evenly across the network interfaces, resulting in an efficient
and balanced usage of all the NIC ports.
</p><p>
A final consideration concerns management traffic. For
security purposes, you may want to separate GlusterFS and management
traffic (such as SSH, SNMP and so on) onto different subnets. To achieve best
performance, you also should use different physical interfaces; otherwise,
you may get contention between storage and management traffic over the
same NIC ports. If your servers come with only two network interfaces,
you still can split storage and management traffic onto different subnets
by using VLAN tagging.
</p><p>
Figure 2 illustrates a potential configuration to achieve this result. On
a server with two physical network ports, we have aggregated those
into a bond interface named bond0, for GlusterFS traffic. The
corresponding configuration is shown below:


<pre     class="programlisting">
# cat /etc/sysconfig/network-scripts/ifcfg-bond0
DEVICE=bond0
BONDING_OPTS="miimon=100 mode=4 xmit_hash_policy=layer3+4"
MTU=9000
BOOTPROTO=none
IPADDR=192.168.1.1
NETMASK=255.255.255.0
</pre>
</p><div       class="mediaobject"><img src="11549f2.jpg"><div class="caption"><p>
Figure 2. Network Configuration on a GlusterFS Server with Multiple Bond
Interfaces and VLAN Tagging
</p></div></div><p>
We have also created a second, logical interface named
bond0.10. The configuration is listed below:

<pre     class="programlisting">
# cat /etc/sysconfig/network-scripts/ifcfg-bond0.10
DEVICE=bond0.10
ONPARENT=yes
VLAN=10
MTU=1500
IPADDR=10.1.1.1
NETMASK=255.255.255.0
GATEWAY=10.1.1.254
</pre>
</p><p>
This interface is used for traffic management and is set up with a
standard MTU of 1,500 bytes and a VLAN tag. Conversely, bond0 is
not set with any VLAN tag, so it will send traffic untagged, which
on the switches will be assigned to the native VLAN of the corresponding
Ethernet trunk.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16b6268"></a>
Tuning the Storage Layer</h2></div></div><p>
As of today, the commodity x86 servers with the highest density of local
storage can fit up to 26 local drives in their chassis. If two drives
are dedicated to the OS, this leaves 24 drives to host the GlusterFS
bricks. You can add additional drives via Direct Attached Storage (DAS),
but before scaling &ldquo;up&rdquo;, you should check whether you have
enough spare network bandwidth. With large sequential I/O patterns, you may
run out of bandwidth well before exhausting the throughput of your local
drives.
</p><p>
The optimal RAID level and LUN size depend on your workload. If your
application generates lots of random write I/O, you may consider
aggregating all the drives in a single RAID 10 volume. However, efficiency
won't be the best, as you will lose 50% of your capacity. On the other
hand, you can go with a RAID 0 configuration for maximum efficiency
and performance, and rely on GlusterFS to provide data redundancy via
its replication capabilities.
</p><p>
It is not surprising that the best approach often lies in the middle. With
24 drives per node, Red Hat's recommended setup consists of 2 x
RAID 6 (10+2) volumes (see Resources). A
single volume with a RAID 50 or 60 configuration also can provide
similar performance.
</p><p>
On a LUN spanning a RAID 6 volume of 12 SAS hard disk drives, a typical sustained
throughput is 1&ndash;2GB per second, depending on the type and model of
the disk drives. To transfer this amount of data across the network,
a single 10Gbps interface easily would become a bottleneck. That's
why you should load-balance traffic across your network interfaces,
as we discussed previously.
</p><p>
It is important to configure a local LUN volume properly to get the most of
the parallel access benefits provided by the RAID controller. The chunk
size (often referred to as element stripe size) is the &ldquo;atomic&rdquo;
block of data that the RAID controller can write to each disk drive
before moving onto the next one. If the average I/O request size issued
by an application is smaller than the chunk size, only one drive
at a time would handle most of the I/O.
</p><p>
The optimal chunk size is obtained by dividing the average I/O request
size by the number of data-bearing disks. As an example, a RAID 6 (10+2)
volume configuration gives ten data-bearing disks. If the average I/O
request issued by your application is 1MB, this gives a chunk size
(approximated to the nearest power of 2) of 128KB. Then, the minimum
amount of data that the RAID controller distributes across the drives
is given by the number of data-bearing disks multiplied by the chunk
size. That is, in this example, 10 x 128 KB = 1280 KB, slightly larger
than the average I/O request of 1MB that we initially assumed.
</p><p>
Next, ensure that LVM properly aligns its metadata based on
the stripe value that was just calculated:

<pre     class="programlisting">
# pvcreate --dataalignment=1280k /dev/sdb
</pre>
</p><p>
When formatting the bricks, <tt  >mkfs</tt> can optimize the distribution
of the filesystem metadata by taking into account the layout of the
underlying RAID volume. To do this, pass the <tt  >su</tt> and
<tt  >sw</tt>
parameters to <tt  >mkfs.xfs</tt>. These indicate the chunk size in bytes
and the stripe width as a multiplier of the chunk size, respectively:

<pre     class="programlisting">
# mkfs.xfs -d su=128k,sw=10 -i 512 /brick01
</pre>
</p><p>
Another setting to consider is the default <span   class="emphasis"><em>read ahead</em></span> of the brick
devices. This tells the kernel and RAID controller how much data to 
pre-fetch when sequential requests come in. The actual value for the kernel
is available in the /sys virtual filesystem and is set to 128KB by default:

<pre     class="programlisting">
# cat /sys/block/sdb/queue/read_ahead_kb
128
</pre>
</p><p>
A typical SAS hard drive with 15k RPM of rotational speed has a standard
average response time of around 5ms and can read 128KB (the default read
ahead) in less than a millisecond. Under I/O contention, the drive may
spend most of its time positioning to a new sector for the next I/O
request and use only a small fraction of its time to transfer data. In
such situations, a larger read ahead can improve performance. The rationale
behind this change is that it is better to pre-fetch a big chunk of data in
the hope that it will be used later, as this will reduce the overall seek
time of the drives.
</p><p>
Again, there is no optimal value for all workloads, but as a starting
point,
it may be worth noting that Red Hat recommends to set this parameter to
64MB. However, if your application I/O profile is IOPS-intensive and mostly
made of small random requests, you may be better off tuning the read ahead
size to a lower value and deploying solid disk drives. The commercial
version of GlusterFS provides a built-in profile for the
<tt  >tuned</tt>
d&aelig;mon, which automatically increases read ahead to 64MB and configures
the bricks to use the deadline I/O elevator.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16b6aa8"></a>
Conclusion</h2></div></div><p>
GlusterFS is a versatile distributed filesystem, well supported on
standard Linux distributions. It is trivial to install and simple
to tune, as we have shown here. However, this article is merely a short
introduction. We did not have space to cover many of the
most exciting features of GlusterFS, such as asynchronous replication,
OpenStack Swift integration and CIFS/NFS exports&mdash;to mention just a
few! We hope this article enables you to give GlusterFS a go in your
own environment and explore some of its many capabilities in more detail.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16b6bb0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
GlusterFS Download Page: 
<a href="http://download.gluster.org" target="_self">download.gluster.org</a>
</p><p>
GlusterFS Quick Start Guide: <a href="http://www.gluster.org/community/documentation/index.php/QuickStart" target="_self">www.gluster.org/community/documentation/index.php/QuickStart</a>
</p><p>
B. England and N. Khare, <span   class="emphasis"><em>Best Practices for Red Hat Storage
Server Performance</em></span>: slides available from 
<a href="http://www.redhat.com/summit/2013/presentations" target="_self">www.redhat.com/summit/2013/presentations</a>
</p><p>
Dell Drive Characteristics and Metrics: <a href="http://www.dell.com/downloads/global/products/pvaul/en/enterprise-hdd-sdd-specification.pdf" target="_self">www.dell.com/downloads/global/products/pvaul/en/enterprise-hdd-sdd-specification.pdf</a>
</p><p>
Linux Ethernet Bonding Driver HOWTO: <a href="https://www.kernel.org/doc/Documentation/networking/bonding.txt" target="_self">https://www.kernel.org/doc/Documentation/networking/bonding.txt</a>
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x15c5580.0x16b7130"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Alex Davies and Alessandro Orsaria worked together for a large Web firm
before moving into technology within the algorithmic and hedge-fund
industries. When they
are not trekking in Scotland, mountain climbing in the Alps, playing
the piano or volunteering, they are typically in front of a computer
and can be reached at <a href="mailto:alex@davz.net">alex@davz.net</a> and
<a href="mailto:alex@linux.it">alex@linux.it</a>.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../235/toc235.html">Issue Table of Contents</a>
    <a class="link3" href="../235/11549.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>