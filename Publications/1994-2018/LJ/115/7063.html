<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>UpFront</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
<div class="article" lang="en">
  <div class="titlepage">
    <div>
      <h1 class="title">UpFront</h1>
    </div>
<a name="mpart"></a>
<ul class="mpart-list"><li class="mpart-listitem"><a href="#mpart1">diff -u: What's New in Kernel Development</a></li>
<li class="mpart-listitem"><a href="#mpart2">Stephen Hawking's Project Gets SGI Linux System</a></li>
<li class="mpart-listitem"><a href="#mpart3">Jabber:</a></li>
<li class="mpart-listitem"><a href="#mpart4">JFFNMS:</a></li>
<li class="mpart-listitem"><a href="#mpart5">Pushing Big Data at NASA Ames Research Center</a></li>
<li class="mpart-listitem"><a href="#mpart6">Escalating Computing Demands at SARA</a></li>
<li class="mpart-listitem"><a href="#mpart7">SolarWolf:</a></li>
<li class="mpart-listitem"><a href="#mpart8">synonym:</a></li>
<li class="mpart-listitem"><a href="#mpart9">Tkabber:</a></li></ul>

<a name="mpart1"></a>
<h2 class="title">diff -u: What's New in Kernel Development</h2>
<div class="titlepage"><div><div class="author"><h3 class="author">Zack Brown</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

<span   class="bold"><b>Linus Torvalds</b></span> has taken another step toward
the next <span   class="bold"><b>stable</b></span> series. The 2.5 series is
over, and a <span   class="bold"><b>2.6.0-test</b></span> series, in preparation
for 2.6.0 proper, is underway. Along with the
change in minor version number, Linus hopes to
become even more restrictive of the sorts of
patches he will accept. Although not exactly a code
freeze, he has made it clear that big changes probably will
be rejected, except in certain special
instances. He intends to release 2.6.0 before
the end of this year.
</p><p>
One of the longest, most bitter debates in Linux history is drawing to
a close. For years, users have been begging for some way, given only
a compiled kernel, to derive the <span   class="bold"><b>configuration
options</b></span> used to
compile that kernel. Many patches have been proposed, but finally
it seems that 2.6 will do it. <span   class="bold"><b>Randy Dunlap</b></span> wrote the code to give
the user freedom to expose the configuration through a /proc interface,
to attach the config file itself to the kernel binary or to do neither of
those things and simply ditch the data. After hovering uncertainly in
<span   class="bold"><b>Alan Cox</b></span>'s tree for a while, Randy's patch was accepted into Linus'
tree sometime in late July 2003.
</p><p>
Mounting encrypted filesystems over the loopback device is now easy, after
some work by <span   class="bold"><b>Andries Brouwer</b></span> and others on
<span   class="bold"><b>cryptoloop</b></span>. Users
enabling the BLK_DEV_CRYPTOLOOP configuration option will be able to mount
encrypted filesystems transparently. Encrypting a set of files will be as
easy as copying a directory tree from one place to another. Because this
is done over loopback, it also is convenient to keep a small portion of
one's system encrypted, leaving less-sensitive material in the clear. The
only potential problem is that, as of mid-August 2003, cryptoloop may change
the loopback APIs, forcing source-level changes in all drivers that rely
on the loopback device.
</p><p>
A new kernel driver for the <span   class="bold"><b>Synaptics
TouchPad</b></span> device came out
in June 2003, based largely on the corresponding XFree86 driver. The driver,
authored by <span   class="bold"><b>Peter Osterlund</b></span>, emulates a three-button mouse with two
scroll wheels; it supports multifinger tapping, vertical and horizontal
scrolling, edge scrolling during drag operations, palm detection and
corner tapping. Currently, it shows every sign of making its way
into the official tree before 2.6.0 comes out.
</p><p>
<span   class="bold"><b>Daniel Stekloff</b></span> wrote and released
<span   class="bold"><b>libsysfs</b></span>, a convenient
library to handle <span   class="bold"><b>SysFS</b></span> interfaces. He'd gotten sick of duplicating the
same code over and over in all his SysFS-enabled applications. libsysfs
makes the interfacing with SysFS much simpler. <span   class="bold"><b>Greg
Kroah-Hartman's udev</b></span> replacement for DevFS is one of the most prominent projects
currently using libsysfs, but I imagine many others will join in soon
enough. Daniel also had a hand in the early design of udev, on which
Greg's work has been based.
</p><p>
<span   class="bold"><b>David Howells</b></span> has produced <span   class="bold"><b>CacheFS</b></span>, a nifty filesystem
that makes any block device look like a disk, which in turn can be used
by any other filesystem. Although intended specifically to support the
AFS filesystem, David designed CacheFS to require no knowledge of the
filesystem layered above it. CacheFS looks like it's on the fast track
toward kernel inclusion, as Linus Torvalds has been wanting something
like it for a long time. Apparently, CacheFS is ideally positioned to
provide filesystem-based version control features, and Linus is quite
interested in this at the moment. However, because the code came out so late in
the unstable series, it will require some additional user feedback before
CacheFS can make it into the main tree. A backport to 2.4 also may be in
the offing, although the prime candidate to do this, <span   class="bold"><b>Jeff Garzik</b></span>,
has not yet committed to it.
</p></div>

<a name="mpart2"></a>
<h2 class="title">Stephen Hawking's Project Gets SGI Linux System</h2>
<div class="titlepage"><div><div class="author"><h3 class="author">Jason Pettit</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

Modeling the 14-billion-year history of the universe&mdash;from the very
first fractions of a second following the Big Bang to the present&mdash;calls
for a unique combination of genius and technology. In the UK COSMOS
Project, the genius comes courtesy of a group of British scholars
and scientists led by Stephen Hawking, professor at the University of
Cambridge, author of two best-selling books on the universe and its
origins and head of the project.
</p><p>
For technology, the COSMOS consortium turned to SGI, which recently
delivered a new 128-processor Altix 3000 supercluster running Linux.
The new Altix supercluster
represents the next phase of an SGI computational and visualization
grid supporting the COSMOS Project, which involves numerous UK
universities and is headquartered at Cambridge.
</p><p>
COSMOS investigators create competing models for the origin of galaxies
and other large-scale structures. They also study theories about the
creation of matter in the universe.
</p><p>
The acclaimed cosmologist, who holds the same Cambridge professorial
post once held by Isaac Newton, predicts the newly acquired system
&ldquo;will enable us to keep up with the dramatic data about our universe
which is now coming in, and the UK COSMOS team will accelerate their
world-class research in cosmology.&rdquo;
</p><div       class="mediaobject"><a href="7063hawkingf1.large.jpg"><img src="7063hawkingf1.jpg"></a></div></div>

<a name="mpart3"></a>
<h2 class="title">Jabber: <a href="http://www.jabber.org" target="_self">www.jabber.org</a></h2>
<div class="titlepage"><div><div class="author"><h3 class="author">David A. Bandel</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

If you've ever used MSN Messenger, AIM or Yahoo Messenger and would
like to set up an instant-messaging system in your company, Jabber
is your ticket. In fact, the Jabber protocol is more capable than
the proprietary ones. And as an added feature, Jabber can connect to
MSN, AIM or Yahoo, albeit through an account on those systems. Requires:
libcrypto, libdl, libresolv, glibc and libssl (optional).
</p></div>

<a name="mpart4"></a>
<h2 class="title">JFFNMS: <a href="http://jffnms.sourceforge.net" target="_self">jffnms.sourceforge.net</a></h2>
<div class="titlepage"><div><div class="author"><h3 class="author">David A. Bandel</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

The &ldquo;just for fun network management system&rdquo; really is for more than
just fun. It watches your systems and graphs activity of most any
sort that can be ascertained with SNMP. It stores the data in an SQL
database. It also can use tftp to back up configurations for devices
such as Cisco routers
or wireless access points. Requires: Apache, PHP with MySQL or
PostgreSQL, SQL server, SNMP, RRDTool and tftp (optional).
</p><div       class="mediaobject"><a href="7063jffnmsf1.large.jpg"><img src="7063jffnmsf1.jpg"></a></div></div>

<a name="mpart5"></a>
<h2 class="title">Pushing Big Data at NASA Ames Research Center</h2>
<div class="titlepage"><div><div class="author"><h3 class="author">Jason Pettit</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

In just 25 seconds, the turbopumps of the space shuttle's main engines can
empty a 26,000-gallon swimming pool. During the eight and a half minutes
of main engine burn time, those pumps will have moved about 528,600
gallons of liquid hydrogen and oxygen.
</p><p>
Optimizing the design of these engines for future spacecraft and then proving
that engine design even before it's prototyped is the job of scientists
and engineers at NASA. At NASA Ames Research Center, aerospace joins
other advanced research areas such as astrobiology, earth sciences and
space sciences. A NASA Center of Excellence for Information Sciences and
Technologies, Ames deploys some of the world's most powerful computing
systems to achieve things that, quite literally, have never been done
before. &ldquo;We deal with large-scale problems&rdquo;, says Bob Ciotti,
terascale applications lead at NASA Ames. He adds:
</p><div class="blockquote"><blockquote class="blockquote"><p>
When you're working in areas like
climate modeling, nanotechnology, vehicle ascent/descent analysis,
complex aeronautic stability and control problems or liquid rocket motor
designs, you require vast amounts of computation focused on a single
problem. So much, in fact, that we must limit the modeling complexity
of these problems to fit on today's most powerful supercomputing systems
so that they will complete in a reasonable amount of time.
</p></blockquote></div><p>
For much of that power, NASA Ames and SGI codeveloped a 1,024-processor
single system image (SSI) shared-memory SGI Origin 3000 system.
SSI means that all 1,024 processors share all the memory between them
and run one copy of the operating system. NASA currently is installing
a new 256-processor SGI Altix 3000 supercluster running Linux. The Altix system will see broad use at the facility, due in
large part to its high-bandwidth NUMAflex architecture.
</p><p>
&ldquo;Large-scale problems require an extremely low-latency
interface&rdquo;,
Ciotti says. He continues:
</p><div class="blockquote"><blockquote class="blockquote"><p>
That determines how well you'll scale on these tightly
coupled problems. And, by having a single system image, we benefit from
a more efficient and simpler programming development environment and a
more robust I/O architecture that's a good match for applications where
we're pushing lots of data.
</p></blockquote></div><p>
That sort of data-intensive work is a given at NASA Ames. For instance,
prior to prototyping an engine design, engineers will model key components
and digitally simulate their performance. Modelers would look in detail at
about five rotations of the shuttle's main engine turbopump by dividing it
up into many small snapshots. Think of it as an extraordinarily detailed
movie that runs about 5,000 frames, recording about 0.02 seconds of real
time on shuttle liftoff. It's not uncommon for a single simulation of this
type to generate over a terabyte of data. Still, NASA engineers know
it's worth it: &ldquo;Making small design improvements prior to prototyping
can have a dramatic impact on vehicle performance and significant cost
savings over its life.&rdquo;
</p><p>
The work underway at NASA also suggests a strong future for Linux as an
HPC environment. &ldquo;On Altix, there are ways we can do true memory sharing
between Linux systems and somewhat mitigate the need for very large SSI,
and that's a very powerful programming asset to have&rdquo;, Ciotti says.
&ldquo;This
platform will allow us to continue to leverage off the benefits of shared
memory and make full use of the development work we've done over the last
five years. We definitely see a path for doing ever more scalable
work.&rdquo;
</p><div       class="mediaobject"><a href="7063NASAf1.large.jpg"><img src="7063NASAf1.jpg"></a><div class="caption"><p>
The 256-processor SGI Altix 3000 supercluster Altix system will see
broad use at the facility, due in large part to its high-bandwidth
NUMAflex architecture.
</p></div></div></div>

<a name="mpart6"></a>
<h2 class="title">Escalating Computing Demands at SARA</h2>
<div class="titlepage"><div><div class="author"><h3 class="author">Jason Pettit</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

In 2002, Dutch scientists began what's known as a &ldquo;grand
challenge&rdquo;
experiment: comparing more than 400,000 proteins extracted from numerous
organisms. In less than a year, the task consumed more than half a
million CPU hours on an SGI server driven by
1,024 processors.
</p><p>
Still, the Netherlands Organisation for Scientific Research (NWO) was
hungry for more horsepower&mdash;a need that Professor Peter Nijkamp, chairman
of the NWO, describes as &ldquo;the constant demand for additional computational
capacity&rdquo;. So the group ordered an SGI Altix 3000 supercluster, powered by
416 new Intel Itanium 2 processors and bolstered with 832GB of memory.
The system recently was installed at SARA, Computing and Networking
Services (the Dutch National HPC and Networking Center). SARA's broad
span of research includes groundbreaking work in climate research,
medical science, water management and water quality calculations, fluid
dynamics and turbulence modeling, computational chemistry and genomics.
</p><p>
All of these applications benefit significantly from global shared
memory&mdash;a powerful enabling technology for which the SGI Altix 3000
system is uniquely equipped. Using global shared memory, the installed
Altix 3000 system, running a single standard Linux OS on each of its 64
processor nodes, will be integrated with SARA's existing 1,024-processor
SGI Origin 3000 server.
</p><div       class="mediaobject"><img src="7063SARAf1.jpg"><div class="caption"><p>
Hungry for horsepower, the Netherlands Organisation for Scientific Research
(NWO) added a 416-processor SGI Altix 3000 supercluster with 832GB of
memory to its existing SGI Origin 3000 server driven by 1,024 processors.
</p></div></div></div>

<a name="mpart7"></a>
<h2 class="title"><span class="emphasis"><em>SolarWolf</em></span>: <a href="http://pygame.org/shredwheat/solarwolf" target="_self">pygame.org/shredwheat/solarwolf</a></h2>
<div class="titlepage"><div><div class="author"><h3 class="author">David A. Bandel</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

Here's another great time killer. If you like arcade-style games, this
one provides hours of fun collecting boxes
while dodging fireballs. Animation and graphics are excellent, as is
the sound. If it only had a cannon, I could shoot at the fireballs.
Requires: Python and Pygame module.
</p><div       class="mediaobject"><a href="7063solarf1.large.jpg"><img src="7063solarf1.jpg"></a></div></div>

<a name="mpart8"></a>
<h2 class="title">synonym: <a href="http://www.modulo.ro/synonym" target="_self">www.modulo.ro/synonym</a></h2>
<div class="titlepage"><div><div class="author"><h3 class="author">David A. Bandel</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

I can't count the number of times I've been asked about a way to copy all
(and I do mean all) messages passing through a mail server, both incoming
and outgoing, to a file or database. Well, synonym is a sendmail milter
program that will do exactly that. Every message processed by sendmail is
copied to a user. It also adds X-Copied-To: headers for anyone
to see that the message has, in fact, been archived.
Requires: libpthread, libsm, libsmutil, libmilter, sendmail with
milter and glibc.
</p><div       class="mediaobject"><a href="7063synonymf1.large.jpg"><img src="7063synonymf1.jpg"></a></div></div>

<a name="mpart9"></a>
<h2 class="title">Tkabber: <a href="http://tkabber.jabber.ru/en" target="_self">tkabber.jabber.ru/en</a></h2>
<div class="titlepage"><div><div class="author"><h3 class="author">David A. Bandel</h3></div><div class="issuemoyr">Issue #115, November 2003</div></div></div><div class="simplesect" lang="en"><p>

For the Jabber server, you need a Jabber client.
After looking over several of the Linux offerings,
I found Tkabber to be among the easiest, most feature-rich of all, free or proprietary. Requires: tcl/tk, wish, tcllib and bwidget.
</p><div       class="mediaobject"><a href="7063tkabberf1.large.jpg"><img src="7063tkabberf1.jpg"></a></div></div>
  </div>
</div>


  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../115/toc115.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>