<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
At the Forge
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Want to analyze Apache logfiles using the latest data science&#10;techniques? Start off by importing and cleaning them.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1974580.0x1a6bac0"></a>
At the Forge
</h1></div><div><h3 class="subtitle"><i>
Analyzing Data
</i></h3></div><div><div class="author"><h3 class="author">
Reuven
 M. 
Lerner
</h3></div><div class="issuemoyr">Issue #263, March 2016</div></div><div><p>
Want to analyze Apache logfiles using the latest data science
techniques? Start off by importing and cleaning them.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1974580.0x1a6c3b0"></a></h2></div></div><p>
My first Web-related job was in 1995, developing Web applications for
a number of properties at Time Warner. When I first started there, we
had a handful of programmers and managers handling all of the
tasks. But over time, as happens in all growing companies and
organizations, we started to specialize. One of the developers took
charge of the logfiles&mdash;storing them and then performing some basic
analysis on them.
</p><p>
Although I recognized that this work was important, it took years for me
to realize that in some ways, his job was more important to the
business than the applications that I was writing. The developer who
worked on these logfiles, and who analyzed them for our bosses, made
it possible to know who was using our system, what they were viewing
and using, where they came from, and what correlations we could find
among the different data points provided by the logs. Sure, we were
providing the content and the applications that brought people to the
site, but it was the person analyzing the logfiles who was ensuring
that our work was paying for itself and meeting our business goals.
</p><p>
During the past decade, I've come to appreciate the need for such
analysis even more, as the Web has exploded in popularity, as
businesses have learned to use such data to increase profitability
and as data science has become a growing field. We're now drowning in
data, and being able to make sense of it using analytical tools and
libraries is more important than ever.
</p><p>
In this article, I start an exploration of data science using Python,
and how you can take something as ordinary as an Apache logfile and
extract information from it to understand your visitors better and what
they do. In upcoming articles, I plan to cover how you can use data science
methods to analyze this logfile in a number of different ways, gaining
insights into the raw data it provides and answering questions
about your Web application. I'll describe how this analysis also
can be presented to your managers and clients, providing powerful
visualizations of the analysis you've performed.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1974580.0x1a6c6c8"></a>
Data Science and Python</h2></div></div><p>
I studied something called &ldquo;learning sciences&rdquo; in graduate school.
While I was there, we often would joke that any discipline that
includes the word &ldquo;science&rdquo; in its name is probably not a real
science. Regardless of whether data science is a &ldquo;real&rdquo; science, it is
a large, important and growing field&mdash;one that allows businesses
to make decisions based on the data they have gathered. The more
data, and the more intelligently you use that data, the better you'll
be able to predict your users' and customers' wants and needs.
</p><p>
Data science has been defined loosely as the intersection of
programming and statistics, applied to a particular domain. You
gather some data and then use statistical methods to answer questions
the data might be able to answer. A background in statistics can
be helpful, not only because it'll show you the types of analysis
you might want to apply, but also because it gives you a healthy sense
of skepticism regarding the correlations you find. Did you really
discover that your Web site is popular only with people in a
particular area of the world? Or, did you just advertise it heavily in
one part of the world, influencing who is more likely to visit?
</p><p>
You can start a data science project by asking a question, or you can
start to explore the data in a variety of ways, hoping you will
find an interesting correlation. Regardless, data science expects you
to know a variety of methods from which you can choose one or more
that are appropriate for answering your questions. You then apply the
methods, using statistical tests to determine whether your answers are
significant&mdash;that is, whether they merely could have been random.
</p><p>
Python, long used by system administrators, Web developers and
researchers, is an increasingly popular choice among people working in
data science. This is the result of several factors coming together.
First, Python has a famously shallow learning curve, allowing
non-programmers to get started and do things in a short amount of
time.
</p><p>
Second, Python works easily and cleanly with a variety of data formats
and databases. Thus, whether your raw data is in a text file,
relational database, NoSQL database, CSV file, Excel file or
something more unusual, the odds are very good that Python will be
able to read from it easily and quickly.
</p><p>
Third, a number of libraries for analyzing data in Python, such as
NumPy, SciPy and Matplotlib, have been under development for many
years, providing a terrific balance of usability, expressive power
and high-efficiency execution. In recent years, the Pandas library
has added an even more useful layer on top of this.
</p><p>
Finally, the development of IPython, now known as Jupyter, has been
nothing short of revolutionary, providing developers and data
scientists with the ability to interact with their programs and data
(as with a traditional REPL), but to do so on a Web page that 
easily can be shared among collaborators or sent via e-mail for off-line
usage and analysis. Indeed, I now use the IPython Notebook in all of
my Python courses. Not only does it provide me with a high-quality way
to display the live coding demos I do during my classes, but I 
then can send the document to my students, who can replay, modify and
better understand what I discussed in class.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1974580.0x1a6cb40"></a>
Importing Data</h2></div></div><p>
The first step of any data science project is to get the data
ready. In the case of wanting to analyze Apache logfiles, you might
think it's enough just to get the file. However, Pandas&mdash;the
Python library that I'll be using to analyze the data for this example&mdash;is like many
other data science systems (for example, the R language) that expects the
data to be in CSV (comma-separated values) format. This means
you'll need to convert the logfile into a CSV file, in which the fields
from the Apache log are converted into fields in CSV.
</p><p>
Performing such a transformation is actually quite straightforward in
Python. Here is a sample from the Apache logfile from my blog:

<pre     class="programlisting">
122.179.187.119 - - [22/Jan/2016:11:57:26 +0200] "GET
 &#8618;/wp-content/uploads/2014/10/3D_book.jpg HTTP/1.1" 200 302222
 &#8618;"http://blog.lerner.co.il/turning-postgresql-rows-arrays-array/"
 &#8618;"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like 
 &#8618;Gecko) Chrome/47.0.2493.0 Safari/537.36"
122.179.187.119 - - [22/Jan/2016:11:57:27 +0200] "POST
 &#8618;/wp-admin/admin-ajax.php HTTP/1.1" 200 571
 &#8618;"http://blog.lerner.co.il/turning-postgresql-rows-arrays-array/"
 &#8618;"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like 
 &#8618;Gecko) Chrome/47.0.2493.0 Safari/537.36"
54.193.228.6 - - [22/Jan/2016:11:57:29 +0200] "GET 
 &#8618;/category/python/feed/ HTTP/1.1" 200 25856 "-" "Digg Feed 
 &#8618;Fetcher 1.0 (Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_1) 
 &#8618;AppleWebKit/534.48.3 (KHTML, like Gecko) Version/5.1
 &#8618;Safari/534.48.3)"
</pre>
</p><p>
Each line has the following components:
</p><div class="itemizedlist"><ul type="disc"><li><p>
IP address from which the request was made.
</p></li><li><p>
Two fields (represented with - characters) having to do with
authentication.
</p></li><li><p>
The timestamp.
</p></li><li><p>
The HTTP request, starting with the HTTP request method (usually
<tt  >GET</tt> or
<tt  >POST</tt>) and a URL.
</p></li><li><p>
The result code, in which 200 represents &ldquo;OK&rdquo;.
</p></li><li><p>
The number of bytes transferred.
</p></li><li><p>
The referrer, meaning the URL that the user came from.
</p></li><li><p>
The way in which the browser identifies itself.
</p></li></ul></div><p>
This information might seem a bit primitive and limited, but you can
use it to understand a large number of factors better having to do
with visitors to your blog. Note that it doesn't include information
that JavaScript-based analytics packages (for example, Google Analytics) can
provide, such as session, browser information and
cookies. Nevertheless, logfiles can provide you with some good basics.
</p><p>
Two of the first steps of any data science project are 1) importing
the data and 2) cleaning the data. That's because any data source
will have information that's not really useful or relevant for your
purposes, which will throw off the statistics or add useless
bloat to the data you're trying to import. Thus, here I'm going to try to read
the 
Apache logfile into Python, removing those lines that are irrelevant.
Of course, what is deemed to be &ldquo;irrelevant&rdquo; is somewhat subjective;
I'll get to that in just a bit.
</p><p>
Let's start with a very simple parsing of the Apache logfile. One of
the first things Python programmers learn is how to iterate over
the lines of a file:

<pre     class="programlisting">
infile = 'short-access-log'
for line in open(infile):
    print(line)
</pre>
</p><p>
The above will print the file, one line at a time. However, for this
example, I'm not
interested in printing it; rather, I'm interested in turning it into
a CSV file. Moreover, I want to remove the lines that are less
interesting or that provide spurious (junk) data.
</p><p>
In order to create a CSV file, I'm going to use the
<tt  >csv</tt> module that
comes with Python. One advantage of this module is that it can take
any separator; despite the name, I prefer to use tabs between my
columns, because there's no chance of mixing up tabs with the data I'm
passing.
</p><p>
But, how do you get the data from the logfile into the CSV module? A
simple-minded way to deal with this would be to break the input string
using the <tt  >str.split</tt> method. The good news is that split will work, at
least to some degree, but the bad news is that it'll parse things
far less elegantly than you might like. And, you'll end up with all
sorts of crazy stuff going on.
</p><p>
The bottom line is that if you want to read from an Apache logfile,
you'll need to figure out the logfile format and read it, probably
using a regular expression. Or, if you're a bit smarter, you can use an
existing library that already has implemented the regexp and logic.
I searched on PyPI (the Python Package Index) and found clfparser, a
package that knows how to parse Apache logfiles in what's known as the
&ldquo;common logfile format&rdquo; used by a number of HTTP servers for many
years. If the variable <tt  >line</tt> contains one line from my Apache
logfile, I can do the following:

<pre     class="programlisting">
from clfparser import CLFParser
infilename = 'short-access-log'
for line in open(infilename):
    print CLFParser.logDict(line)
</pre>
</p><p>
In this way, I have turned each line of my logfile into a Python
dictionary, with each key-value pair in the dictionary referencing a
different field from my logfile's row.
</p><p>
Now I can go back to my CSV module and employ the DictWriter class
that comes with it. DictWriter, as you probably can guess, allows you
to output CSV based on a dictionary. All you need to do is declare the
fields you want, allowing you to ignore some or even to set
their order in the resulting CSV file. Then you can iterate over your
file and create the CSV.
</p><p>
Here's the code I came up with:

<pre     class="programlisting">
import csv
from clfparser import CLFParser

infilename = 'short-access-log'
outfilename = 'access.csv'

with open(outfilename, 'w') as outfile, open(infilename) as infile:
    fieldnames = ['Referer', 'Useragent', 'b', 'h', 'l', 'r', 's', 
     &#8618;'t', 'time', 'timezone', 'u']
    writer = csv.DictWriter(outfile, fieldnames=fieldnames,
     &#8618;delimiter='\t')
    writer.writeheader()

    for line in infile:
        writer.writerow(CLFParser.logDict(line))
</pre>
</p><p>
Let's walk through this code, one piece at a time. It's not very
complex, but it does pull together a number of packages and
functionality that provide a great deal of power in a small
space:
</p><div class="itemizedlist"><ul type="disc"><li><p>
First, I import both the <tt  >csv</tt> module and the CLFParser
class from the <tt  >clfparser</tt> module. I'm going to be using both of these
modules in this program; the first will allow me to output CSV, and the
second will let me read from the Apache logs.
</p></li><li><p>
I set the names of the input and output files here, both to clean up
the following code a bit and to make it easier to reuse this code
later.
</p></li><li><p>
I then use the <tt  >with</tt> statement, which invokes what's known as a
&ldquo;context manager&rdquo; in Python. The basic idea here is that I'm
creating two file objects, one for reading (the logfile) and one for
writing (the CSV file). When the <tt  >with</tt> block ends, both files will
be closed, ensuring that no data has been left behind or is still in
a buffer.
</p></li><li><p>
Given that I'm going to be using the CSV module's DictWriter, I need
to indicate the order in which fields will be output. I do this in
a list; this list allows allow me to remove or reorder fields,
should I want to do so.
</p></li><li><p>
I then create the csv.DictWriter object, telling it that I want to
write data to outfile, using the field names I just defined and
using tab as a delimiter between fields.
</p></li><li><p>
I then write a header to the file; although this isn't crucial, I
recommend that you do so for easier debugging later. Besides,
all CSV parsers that I know of are able to handle such a thing
without any issues.
</p></li><li><p>
Finally, I iterate over the rows of the access log, turning each
line into a dictionary and then writing that dictionary to the CSV
file. Indeed, you could argue that the final line there is the
entire point of this program; everything up to that point is just a
preface.
</p></li></ul></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1974580.0x1e66440"></a>
Cleaning the Data</h2></div></div><p>
You've now seen that you can import the data from another form into a
CSV file, which is one of the most common formats used in data
science. However, as I mentioned previously, one of the key things that
you also need to do is clean the data; analyzing bogus data will give
you bogus results.
</p><p>
So, what sort of data here needs to be cleaned?
</p><p>
One obvious candidate is to remove anything that wasn't a real
human. Perhaps you're interested in finding out what Web crawlers, such
as those from Google and Yahoo, are up to. But it's more likely that
you want to know what humans are doing, which means removing all of
those robots.
</p><p>
Of course, this raises the question of how you can know whether a
request is coming from a robot. As humans, you can examine the
User-agent string and make an educated guess. But given that you're
trying to remove all of the robots, and that new ones constantly
are being added, something automatic would be better.
</p><p>
There's no perfect answer to this, but for the purposes of this article, I
decided to use another Python module from PyPI, albeit one that's a
bit out of date&mdash;one known as robot-detection. The idea is that you
import this module and then use the <tt  >is_robot</tt> function on the
<tt  >Useragent</tt> field. If it's a robot,
<tt  >is_robot</tt> will return True.
Here's my revised code:

<pre     class="programlisting">
import csv
from clfparser import CLFParser
from collections import Counter
import robot_detection

infilename = 'medium-access-log.txt'
outfilename = 'access.csv'
robot_count = Counter()

with open(outfilename, 'w') as outfile, open(infilename) as infile:
    fieldnames = ['Referer', 'Useragent', 'b', 'h', 'l', 'r', 's', 
     &#8618;'t', 'time', 'timezone', 'u']
    writer = csv.DictWriter(outfile, fieldnames=fieldnames, 
     &#8618;delimiter='\t')
    writer.writeheader()

    for line in infile:
        d = CLFParser.logDict(line)
        if robot_detection.is_robot(d['Useragent']):
            robot_count[d['Useragent']] += 1
        else:
            writer.writerow(d)
</pre>
</p><p>
The above code is mostly unchanged from the previous version; the two
modifications are that I'm now using <tt  >robot_detection</tt> to filter out the
robots, and I'm using the Python <tt  >Counter</tt> class to keep track of how
many times each robot is making a request. This alone might be useful
information to have&mdash;perhaps not now, but in the future. For
example, from examining the most recent 100,000 requests to my blog, I
found that there were more than 1,000 requests from the &ldquo;domain
re-animator bot&rdquo;, something I hadn't even heard of before.
</p><p>
Given that I'm currently concentrating on user data, filtering out
these bot requests made my data more reliable and also a great deal
shorter. Out of 100,000 records, only 27,000 were from actual humans.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1974580.0x1e66a18"></a>
Conclusion</h2></div></div><p>
The first step of any data-analysis project is to import and clean the
data. Here, I have taken the data and put it into CSV format, filtering
out some of the lines that are of less interest. But this is
just the start of my analysis, not its end. Next month, I'll explain how
you
can import this data into Python's Pandas package and start to
analyze the logfile in a number of different ways.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1974580.0x1e66b20"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
Data science is a hot topic, and many people have been writing good
books on the subject. I've most recently been reading and enjoying an
early release of the <span   class="emphasis"><em>Python Data Science Handbook</em></span> by Jake VanderPlas,
which contains great information on data science as well as its use
from within Python. Cathy O'Neil and Rachel Schutt's slightly older
book <span   class="emphasis"><em>Doing Data Science</em></span> is also excellent, approaching problems
from a different angle. Both are published by O'Reilly, and both are
great reads.
</p><p>
To learn more about the Python tools used in data science, check out
the sites for NumPy (<a href="http://numpy.org" target="_self">numpy.org</a>), SciPy (<a href="http://SciPy.org" target="_self">SciPy.org</a>), Pandas (<a href="http://pandas.pydata.org" target="_self">pandas.pydata.org</a>)
and IPython (<a href="http://IPython.org" target="_self">IPython.org</a>).
There is a great deal to learn, so be prepared for a deep dive and
lots of reading.
</p><p>
Python itself is available from <a href="http://python.org" target="_self">python.org</a>, and the PyPI package
index, from which you can download all of the packages mentioned here,
is at <a href="http://PyPI.python.org" target="_self">PyPI.python.org</a>.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1974580.0x1e670a0"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Reuven M. Lerner trains companies around the world in Python,
PostgreSQL, Git and Ruby. His ebook, &ldquo;Practice Makes Python&rdquo;,
contains
50 of his favorite exercises to sharpen your Python skills. Reuven
blogs regularly at <a href="http://blog.lerner.co.il" target="_self">blog.lerner.co.il</a> and tweets as
@reuvenmlerner. Reuven has a PhD in Learning Sciences from
Northwestern University, and he lives in Modi'in, Israel, with his wife
and three children.

</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../263/toc263.html">Issue Table of Contents</a>
    <a class="link3" href="../263/11982.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>