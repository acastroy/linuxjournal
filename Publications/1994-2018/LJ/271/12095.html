<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
At the Forge
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Before you can use machine-learning models, you need to clean the data.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x13da580.0x14d1ac0"></a>
At the Forge
</h1></div><div><h3 class="subtitle"><i>
Preparing Data for Machine Learning
</i></h3></div><div><div class="author"><h3 class="author">
Reuven
 M. 
Lerner
</h3></div><div class="issuemoyr">Issue #271, November 2016</div></div><div><p>
Before you can use machine-learning models, you need to clean the data.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x13da580.0x14d23b0"></a></h2></div></div><p>
When I go to Amazon.com, the online store often recommends products
I should buy. I know I'm not alone in thinking that these
recommendations can be rather spooky&mdash;often they're for products
I've already bought elsewhere or that I was thinking of buying.
How does Amazon do it? For that matter, how do Facebook and LinkedIn
know to suggest that I connect with people whom I already know, but
with whom I haven't yet connected online?
</p><p>
The answer, in short, is &ldquo;data science&rdquo;, a relatively new field that
marries programming and statistics in order to make sense of the huge
quantity of data we're creating in the modern world. Within the world
of data science, machine learning uses software to create statistical
models to find correlations in our data. Such correlations can help
recommend products, predict highway traffic, personalize pricing,
display appropriate advertising or identify images.
</p><p>
So in this article, I take a look at machine learning and some of the
amazing things it can do. I increasingly feel that machine
learning is sort of like the universe&mdash;already vast and expanding
all of the time. By this, I mean that even if you think you've missed
the boat on machine learning, it's never too late to start. Moreover,
everyone else is struggling to keep up with all of the technologies,
algorithms and applications of machine learning as well.
</p><p>
For this article, I'm looking at a simple application of categorization and
&ldquo;supervised learning&rdquo;, solving a problem that has vexed scientists and
researchers for many years: just what makes the perfect burrito?
Along the way, you'll hopefully start to understand some of the techniques
and ideas in the world of machine learning.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x13da580.0x14d2778"></a>
The Problem</h2></div></div><p>
The problem, as stated above, is a relatively simple one to
understand: burritos are a popular food, particularly in southern
California. You can get burritos in many locations, typically with a
combination of meat, cheese and vegetables. Burritos' prices vary
widely, as do their sizes and quality. Scott Cole, a PhD student in
neuroscience, argued with his friends not only over where they could
get the best burritos, but which factors led to a burrito being better
or worse. Clearly, the best way to solve this problem was by
gathering data.
</p><p>
Now, you can imagine a simple burrito-quality rating system, as used
by such services as Amazon: ask people to rate the burrito on a scale of
1&ndash;5.
Given enough ratings, that would indicate which burritos were
best and which were worst.
</p><p>
But Cole, being a good researcher, understood that a simple,
one-dimensional rating was probably not sufficient. A
multi-dimensional rating system would keep ratings closer together
(since they would be more focused), but it also would allow him to
understand which aspects of a burrito were most essential to its high
quality.
</p><p>
The result is documented on Cole's GitHub page
(<a href="https://srcole.github.io/100burritos" target="_self">https://srcole.github.io/100burritos</a>), in which he describes the
meticulous and impressive work that he and his fellow researchers did,
bringing tape measures and scales to lunch (in order to measure and
weigh the burritos) and sacrificing themselves for the betterment of
science.
</p><p>
Beyond the amusement factor&mdash;and I have to admit, it's hard for me
to stop giggling whenever I read about this project&mdash;this can be
seen as a serious project in data science. By creating a
machine-learning model, you can not only describe burrito quality,
but you also can determine, without any cooking or eating, the quality of
a potential or theoretical burrito.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x13da580.0x14d2a90"></a>
The Data</h2></div></div><p>
Once Cole established that he and his fellow researchers would rate
burritos along more than one dimension, the next obvious question was:
which dimensions should be measured?
</p><p>
This is a crucial question to ask in data science. If you measure the
wrong questions, then even with the best analysis methods, your output
and conclusions will be wrong. Indeed, a fantastic new book, <span   class="emphasis"><em>Weapons
of Math Destruction</em></span> by Cathy O'Neil, shows how the collection and
usage of the wrong inputs can lead to catastrophic results for
people's jobs, health care and safety.
</p><p>
So, you want to measure the <span   class="emphasis"><em>right</em></span> things. But just as important is
to measure <span   class="emphasis"><em>distinct</em></span> things. In order for statistical analysis to
work, you have to ensure that each of your measures is independent.
For example, let's assume that the size of the burrito will be
factored in to the quality measurement. You don't want to measure both
the volume and the length, because those two factors are related.
It's often difficult or impossible to separate two related
factors completely, but you can and should try to do so.
</p><p>
At the same time, consider how this research is being done.
Researchers are going into the field (which is researcher-speak for
&ldquo;going out to lunch&rdquo;) and eating their burritos. They might have only
one chance to collect data. This means it'll likely make sense
to collect more data than necessary, and then use only some of it in
creating the model. This is known as &ldquo;feature selection&rdquo; and is an
important aspect of building a machine-learning model.
</p><p>
Cole and his colleagues decided to measure ten different aspects of
burrito quality, ranging from volume to temperature to salsa quality.
They recorded the price as well to see whether price was a factor in quality.
They also had two general measurements: an overall rating and a
recommendation. All of these measurements were taken on a 0&ndash;5 scale,
with 0 indicating that it was very bad and 5 indicating that it was
very good.
</p><p>
It's important to point out the fact that they collected data on more
than ten dimensions doesn't mean all of those measurements needed
to be included in the model. However, this gave the researchers a
chance to engage in feature selection, determining which factors most
most affected the burrito quality.
</p><p>
I downloaded Cole's data, in which 30 people rated more than 100
burritos at 31 different restaurants, from a publicly viewable spreadsheet in
Google Docs, into a CSV file (burritos.csv). The spreadsheet's URL
is
<a href="https://docs.google.com/spreadsheets/d/18HkrklYz1bKpDLeL-kaMrGjAhUM6LeJMIACwEljCgaw/edit#gid=1703829449" target="_self">https://docs.google.com/spreadsheets/d/18HkrklYz1bKpDLeL-kaMrGjAhUM6LeJMIACwEljCgaw/edit#gid=1703829449</a>.
</p><p>
I then fired up the Jupyter (aka IPython) Notebook, a commonly used
tool in the data science world. Within the notebook, I ran the
following commands to set up my environment:

<pre     class="programlisting">
%pylab inline                         # load NumPy, display 
                                      # Matplotlib graphics
import pandas as pd                   # load pandas with an alias
from pandas import Series, DataFrame  # load useful Pandas classes
df = pd.read_csv('burrito.csv')       # read into a data frame
</pre>
</p><p>
At this point, the Pandas data frame contains all the information
about the burritos. Before I could continue, I needed to determine
which fields were the inputs (the &ldquo;independent variables&rdquo;, also known
as &ldquo;predictors&rdquo;) and which was the output (the
&ldquo;dependent variable&rdquo;).
</p><p>
For example, let's assume that the burritos were measured using a
single factor, namely the price. The price would be the
input/independent variable, and the quality rating would be the
output/dependent variable. The model then would try to map from the
input to the output.
</p><p>
Machine learning (and statistical models) works the same way, except
it uses multiple independent variables. It also helps you
determine just how much of an influence each input has on the
output.
</p><p>
First, then, you'll need to examine your data, and identify which
column is the dependent (output) variable. In the case of burritos,
I went with the 0&ndash;5 overall rating, in column X of the spreadsheet.
You can see the overall rating within Pandas with:

<pre     class="programlisting">
df['overall']
</pre>
</p><p>
This returns a Pandas series, representing the average overall score
from all of the samples at a particular restaurant.
</p><p>
Now that I have identified my output, which inputs should I choose?
This is what I described earlier, namely feature selection. Not only
do you want to choose a relatively small number of features (to make
the model work faster), but you also want to choose those features that
truly will influence the output and that aren't conflated with one
another.
</p><p>
Let's start by removing everything but the feature columns. Instead
of dropping the columns that I find uninteresting, I'll just create a
new data frame whose values are taken from the interesting columns on
this one. I'll want the columns with indexes of 11 through 23, which
means that I can issue the following command in Pandas:

<pre     class="programlisting">
burrito_data = df[range(11,23)]
</pre>
</p><p>
<tt  >range()</tt> is a Python function that returns an iterator; in this case,
the iterator will return 11 through 22 (that is, up to and not including
23). In this way, you can retrieve certain columns, in a smaller data
frame. However, you still need to pare down your features.
</p><p>
Notice that my new data frame contains only the independent (input)
variables; the overall score, which is our output variable, will
remain on the side for now.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x13da580.0x14d35e8"></a>
Feature Selection</h2></div></div><p>
Now that I have all of the input variables, which should I choose?
Which are more dependent on one another? I can create a &ldquo;correlation
matrix&rdquo;, giving me a numeric value between 0 (uncorrelated) and 1
(totally correlated). If I invoke the &ldquo;corr&rdquo; method on the data
frame, I'll get a new data frame back, showing the correlations among
all of them&mdash;with a correlation of 1.0 along the diagonal:

<pre     class="programlisting">
burrito_data.corr()
</pre>
</p><p>
Now, it's true that you can look through this and understand it to some
degree. But it's often easier for humans to understand images. Thus,
you can use matplotlib, invoking the following:

<pre     class="programlisting">
plt.matshow(burrito_data.corr())
</pre>
</p><p>
That produces a nice-looking, full-color correlation matrix in which
the higher the correlation, the redder the color. The reddish
squares show that (for example) there was a high correlation
between the &ldquo;length&rdquo; and &ldquo;volume&rdquo; (not surprisingly), and also between
the &ldquo;meat&rdquo; and the &ldquo;synergy&rdquo;.
</p><p>
Another consideration is this: how much does a particular input variable
vary over time? If it's always roughly the same, it's of no use
in the statistical model. For example, let's assume that the price of
a burrito is the same everywhere that the researchers ate. In such a
case, there's no use trying to see how much influence the price will
have.
</p><p>
You can ask Pandas to tell you about this, using the &ldquo;var&rdquo;
method on the
data frame. When I execute <tt  >burrito_data.var()</tt>, I get back a Pandas
series object:

<pre     class="programlisting">
burrito_data.var()

Length          4.514376
Circum          2.617380
Volume          0.017385
Tortilla        0.630488
Temp            1.047119
Meat            0.797647
Fillings        0.765259
Meat:filling    1.084659
Uniformity      1.286631
Salsa           0.935552
Synergy         0.898952
Wrap            1.384554
dtype: float64
</pre>
</p><p>
You can see that the burrito volume changes very little. So, you can 
consider ignoring it when it comes to building the model.
</p><p>
There's another consideration here, as well: is there enough input
data from all of these features? It's normal to have some missing
data; there are a few ways to handle this, but one of them is simply
to try to work without the feature that's missing data. You can use
the &ldquo;count&rdquo; method on the data frame to find which columns might have
too much missing data to ignore:

<pre     class="programlisting">
burrito_data.count()

Length          127
Circum          125
Volume          121
Tortilla        237
Temp            224
Meat            229
Fillings        236
Meat:filling    231
Uniformity      235
Salsa           221
Synergy         235
Wrap            235
dtype: int64
</pre>
</p><p>
As you can see, a large number of data points for the
three inputs that have to do with burrito size are missing. This, according to
Cole, is because the researchers didn't have a tape measure during many of
their outings. (This is but one of the reasons why I insist on
bringing a tape measure with me whenever I go out to dinner with
friends.)
</p><p>
Finally, you can ask scikit-learn to tell you which of these predictors
contributed the most, or the least, to the outputs. You provide
scikit-learn with inputs in a data frame and outputs in a
series&mdash;for example:

<pre     class="programlisting">
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

X = burrito_data
y = df[[23]]
</pre>
</p><p>
In the above code, I import some objects I'll need in order to help
with feature selection. I then use the names that are traditional
in scikit-learn, X and y, for the input matrix and output series. I
then ask to identify the most significant features:

<pre     class="programlisting">
sel = SelectKBest(chi2, k=7)
sel.fit_transform(X, y)
</pre>
</p><p>
Notice that when invoking <tt  >SelectKBest</tt>, you have to provide a value for
&ldquo;k&rdquo; that indicates how many predictors you want to get back. In this
way, you can try to reduce your large number of predictors to a small
number. But if you try to run with the above, you'll encounter a problem.
If there is missing data (NaN) in your input matrix,
<tt  >SelectKBest</tt> will
refuse to run. So it's a good thing to discover which of your
inputs are sometimes missing; if you remove those columns from the
input matrix, you can use some feature reduction.
</p><p>
Cole and his colleagues did this sort of analysis and found that they
could remove some of their input columns&mdash;the &ldquo;flavor
synergy&rdquo;, as
well as those having to do with burrito size. Having gone through the
above process, I'm sure you can easily understand why.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x13da580.0x14caa28"></a>
Conclusion</h2></div></div><p>
Now that you have a good data set&mdash;with an input matrix and an output
series&mdash;you can build a model. That involves choosing one or more
algorithms, feeding data into them and then testing the model to
ensure that it's not overfit.
</p><p>
In my next article, I plan to do exactly that&mdash;take the data from here and
see how to build a machine-learning model. I hope that
you'll see just how easy Python and scikit-learn make the process of
doing the actual development. However, I'll still have to spend time
thinking about what I'm doing and how I'm going to do it, as well
as which tools are most appropriate for the job.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x13da580.0x14cab88"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
I used Python (<a href="http://python.org" target="_self">python.org</a>) and the many parts of the SciPy
stack (NumPy, SciPy, Pandas, matplotlib and scikit-learn) in this
article. All are available from PyPI (<a href="http://PyPI.python.org" target="_self">PyPI.python.org</a>) or from
<a href="http://scipy.org" target="_self">scipy.org</a>.
</p><p>
I recommend a number of resources for people interested in data
science and machine learning.
</p><p>
One long-standing weekly email list is &ldquo;KDNuggets&rdquo; at
<a href="http://kdnuggets.com" target="_self">kdnuggets.com</a>. You also should consider the Data Science
Weekly newsletter (<a href="http://datascienceweekly.com" target="_self">datascienceweekly.com</a>) and This Week in Data
(<a href="https://datarepublicblog.com/category/this-week-in-data" target="_self">https://datarepublicblog.com/category/this-week-in-data</a>),
describing the latest data sets available to the public.
</p><p>
I am a big fan of podcasts and particularly love &ldquo;Partially
Derivative&rdquo;. Other good ones are &ldquo;Data Stores&rdquo; and
&ldquo;Linear
Digressions&rdquo;. I listen to all three on a regular basis and learn
from them all.
</p><p>
If you're looking to get into data science and machine learning, I
recommend Kevin Markham's Data School (<a href="http://dataschool.org" target="_self">dataschool.org</a>) and
Jason Brownlie's &ldquo;Machine Learning Mastery&rdquo;
(<a href="http://MachineLearningMaster.com" target="_self">MachineLearningMaster.com</a>), where he sells a number of short, dense but high-quality ebooks on these subjects.
</p><p>
As I mentioned in the body of this article, Cathy O'Neil's new
book, <span   class="emphasis"><em>Weapons of Math Destruction</em></span>, was thought-provoking and
interesting, as well as disturbing. I highly recommend it.
</p><p>
Finally, thanks are due to Scott Cole, whose burrito-rating work is marvelously
prepared, written and executed, and who shared his results with the online community
for everyone's professional and culinary benefit.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x13da580.0x14cb478"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Reuven M. Lerner offers training in Python, Git and PostgreSQL to
companies around the world. He blogs at <a href="http://blog.lerner.co.il" target="_self">blog.lerner.co.il</a>, tweets
at @reuvenmlerner and curates <a href="http://DailyTechVideo.com" target="_self">DailyTechVideo.com</a>.
Reuven lives in
Modi'in, Israel, with his wife and three children.

</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../271/toc271.html">Issue Table of Contents</a>
    <a class="link3" href="../271/12095.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>