<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>MOSIX: A Cluster Load-Balancing Solution for Linux</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Ibrahim introduces the MOSIX software package and describes&#10;    how it was installed on an experimental Linux cluster in the Ericsson&#10;    Systems Research Lab in Montr&eacute;al.&#10;    "><meta name="keywords" content="ARIES, MOSIX, load, balancing, cluster, Ericsson, nodes"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x2217580.0x230eab0"></a>MOSIX: A Cluster Load-Balancing Solution for Linux</h1></div><div><div class="authorgroup"><div class="author"><h3 class="author">Ibrahim F. Haddad</h3></div><div class="author"><h3 class="author">Evangeline Paquin</h3></div><div class="issuemoyr">Issue #85, May 2001</div></div></div><div><p>
    Ibrahim introduces the MOSIX software package and describes
    how it was installed on an experimental Linux cluster in the Ericsson
    Systems Research Lab in Montr&eacute;al.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x230f978"></a></h2></div></div><p>Software clustering technologies have
been evolving for the past few years and are currently gaining a
lot of momentum for several reasons. These reasons include the
benefits of deploying commodity, off-the-shelf hardware (high-power
PCs at low prices), using inexpensive high-speed networking such as
fast Ethernet, as well as the resulting benefits of using Linux.
Linux appears to be an excellent choice for its robust kernel, the
flexibility it offers, the various networking features it supports
and the early availability of its IP releases.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x230fa80"></a>ARIES Project</h2></div></div><p></p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x230fb88"></a>MOSIX</h2></div></div><p></p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x230fc90"></a>The ARIES Project</h2></div></div><p>With the growth of the popularity of clustering technologies,
we decided to start a project that aims at finding and prototyping
the necessary technology to prove the feasibility of a clustered
Linux internet server that demonstrates telecom-grade
characteristics. Thus, a star was born, and it was called ARIES
(advanced research on internet e-servers).</p><p>ARIES started at the Ericsson Core Unit of Research in
January 2000, and its objective was to use the Linux kernel as the
base technology and to rely on open-source software to build the
desirable system with characteristics that include guaranteed
availability and response time, linear scalability, high
performance and the capability of maintaining the system without
any impacts on its availability.</p><p>Traffic distribution and load balancing were two main areas
of investigation. The strategy followed was to survey the
open-source world, check the available solutions in those areas,
test them and determine to what extent they meet our requirements
for the targeted near telecom-grade Linux internet server.</p><p>This article covers our experience with MOSIX, a software
package developed at the Hebrew University of Jerusalem. We expose
the MOSIX technology, the algorithms developed and how they operate
and describe how we installed MOSIX on an experimental Linux
cluster. We also discuss MOSIX's strengths and weaknesses in order
to help others decide if MOSIX is right for them.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x230fea0"></a>The MOSIX Technology</h2></div></div><p>MOSIX is a software package for Linux that transforms
independent Linux machines into a cluster that works like a single
system and performs load balancing for a particular process across
the nodes of the cluster. MOSIX was designed to enhance the Linux
kernel with cluster computing capabilities and to provide means for
efficient management of the cluster-wide resources. It consists of
kernel-level, adaptive resource sharing algorithms that are geared
for high performance, overhead free scalability and ease of use of
a scalable computing cluster.</p><p>The core of MOSIX is its capability to make multiple server
nodes work cooperatively as if part of a single system. MOSIX's
algorithms are designed to respond to variations in the resource
usage among the nodes by migrating processes from one node to
another, pre-emptively and transparently, for load balancing and to
prevent memory exhaustion at any node. By doing so, MOSIX improves
the overall performance by dynamically distributing and
redistributing the workload and the resources among the nodes in
the cluster.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x2310000"></a>MOSIX Operation</h2></div></div><p>MOSIX operates transparently to the applications and allows
the execution of sequential and parallel applications without
regard for where the processes are running or what other cluster
users are doing.</p><p>Shortly after the creation of a new process, MOSIX attempts
to assign it to the best available node at that time. MOSIX then
continues to monitor the new process, as well as all the other
processes, and will move it among the nodes to maximize the overall
performance. This is done without changing the Linux interface, and
users can continue to see (and control) their processes as if they
run on their local node.</p><p>Users can monitor the process migration and memory usages on
the nodes using QPS, a contributed program that is not part of
MOSIX but supports MOSIX-specific fields. QPS is a visual process
manager, an X11 version of &ldquo;top&rdquo; and &ldquo;ps&rdquo; that displays
processes in a window and allows the user to sort and manipulate
them. It supports special fields (see Figure 1) such as on which
node a process was started, on which node it is currently running,
the percentage of memory it is using and its current working
directory.</p><p>
<div       class="mediaobject"><a href="4546f3.large.jpg"><img src="4546f3.jpg"></a><div class="caption"><p>Figure 1. QPS Supported Fields</p></div></div>
</p><p>Since MOSIX's algorithms are decentralized, each node is both
a master for processes that were created locally and a server for
processes that migrated from other nodes. This implies that nodes
can be added or removed from the cluster at any time, with minimal
disturbances to the running processes.</p><p>MOSIX improves the overall performance by better utilizing
the network-wide resources and by making the system easier to use.
MOSIX's system image model is based on the home-node model. In this
model, all the user's processes seem to run at the user's
login-node. Each new process is created at the same node(s) as its
parent process. Processes that have migrated interact with the
user's environment through the user's home-node, but where
possible, they use local resources. As long as the load of the
user's login-node remains below a threshold value, all the user's
processes are confined to that node. However, when this load rises
above a threshold value, some processes may be migrated
(transparently) to other nodes.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x2310478"></a>Scheduling Policy</h2></div></div><p>MOSIX schedules newly started programs in the node with the
lowest current load. However, if the machine with the lowest load
level announces itself to all the nodes in the cluster, then all
the nodes will migrate newly started jobs to the node with the
lowest load and soon enough this node will be overloaded. However,
MOSIX does not operate in this manner. Instead, every node sends
its current load status to a random list of nodes. This prevents a
single node from being seen by all other nodes as the least busy
and prevents any node from being overloaded.</p><p>How does MOSIX decide which node is the least busy among all
the cluster nodes? This is a good question; however, the answer is
a simple one.</p><p>MOSIX comes with its own monitoring algorithms that detect
the speed of each node, its used and free memory, as well as IPC
and I/O rates of each process. This information is used to make
near optimal decisions on where to place the processes. The
algorithms are very interesting because they try to reconcile
different resources (bandwidth, memory and CPU cycles) based on
economic principles and competitive analysis. Using this strategy,
MOSIX converts the total usage of several heterogeneous resources,
such as memory and CPU, into a single homogeneous cost. Jobs are
then assigned to the machine where they have the lowest cost. This
strategy provides a unified algorithm framework for allocation of
computation, communication, memory and I/O resources. It also
allows the development of near-optimal on-line algorithms for
allocation and sharing these resources.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x2310630"></a>MOSIX Filesystem (MFS)</h2></div></div><p>MOSIX uses its own filesystem, MFS, to make all the
directories and regular files throughout a MOSIX cluster available
from all nodes as if they were within a single filesystem. One of
the advantages of MFS is that it provides cache consistency for
files viewed from different nodes by maintaining one cache at the
server disk node.</p><p>MFS meets the direct file system access (DFSA) standards,
which extends the capability of a migrated process to perform some
I/O operations locally, in the current node. This provision reduces
the need of I/O-bound processes to communicate with their
home-node, thus allowing such processes (as well as mixed I/O and
CPU processes) to migrate more freely among the cluster's node, for
load balancing and parallel file and I/O operations. This also
allows parallel file access by proper distribution of files, where
each process migrates to the node that has its files.</p><p>By meeting the DFSA provision, allowing the execution of
system calls locally in the process' current node, MFS reduces the
extra overhead of executing I/O-oriented system calls of a migrated
process.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x23107e8"></a>Installation Environment</h2></div></div><p>In order to test MOSIX, we set up the following environment:
1) a cabinet that consists of 13 Pentium-class CPU cards running at
233MHz with 256MB of RAM each; and 2) a Pentium-based server
machine, PC1, running at 233MHz with 256MB of RAM. This machine was
used as an NFS and DHCP/TFTP server for the 13 diskless
CPUs.</p><p>When we start the CPUs, they boot from LAN and broadcast a
DHCP request to all addresses on the network. PC1, the DHCP server,
will be listening and will reply with a DHCP offer and will send
the CPUs the information needed to configure network settings such
as the IP addresses (one for each interface, eth0 and eth1),
gateway, netmask, domain name, the IP address of the boot server
(PC1) and the name of the boot file. The CPUs will then download
and boot the specified boot file in the DHCP configuration file,
which is a kernel image located under the /tftpboot directory on
PC1. Next, the CPUs will download a ramdisk and start three web
servers (Apache, Jigsaw and TomCat) and two streaming servers (Real
System Server and IceCast Internet Radio).</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x2310948"></a>Installation Steps</h2></div></div><p>For this setup, we used the Linux Kernel 2.2.14-5.0 that came
with Red Hat 6.2. At the time we conducted this activity, MOSIX was
not available for Red Hat; thus, we had to port MOSIX to work with
the Red Hat kernel. Our plan was to prepare a MOSIX cluster that
consists of the server, PC1 and the 13 diskless CPUs. For this
reason, we needed to have a MOSIX-enabled kernel on the server, and
we wanted to have the same MOSIX-enabled kernel image under the
TFTP server directory to be downloaded and started by the CPUs at
boot time. After porting MOSIX to Red Hat, we started the MOSIX
modified installation script &ldquo;mosix.install&rdquo; that applied the
patches to the 2.2.14-5.0 kernel tree on PC1.</p><p>Once we finished configuring the kernel and enabling the
MOSIX features (using $make xconfig), we compiled it to get a
kernel image:</p><pre     class="programlisting">
cd /usr/src/linux
make clean ; make dep ;
modules_install
</pre><p>Next, we copied the new kernel image from
/usr/src/linux/arch/i386/boot to /boot and we updated the
System.map file:
<pre     class="programlisting">
cp /usr/src/linux/arch/i386/boot/bzImage
cp /usr/src/linux/arch/i386/boot/System.map
ln /boot/System.map.mosix /boot/System.map
</pre>


One of the configuration files that was modified was lilo.conf. We
added a new entry for the MOSIX kernel to make the server boot as a
MOSIX node by default. The updated lilo.conf on PC1 looked like
Listing 1.
</p><p><a href="4546l1.html" target="_self">Listing 1. A MOSIX-Modified
lilo.conf.</a></p><p>Having done that, we needed to complete the configuration
steps. In /etc/profile, we added one line to specify the number of
nodes in the MOSIX cluster:</p><pre     class="programlisting">
# Add to /etc/profile NODES=1-14
</pre><p>We created /etc/mosix.map that allows the local MOSIX node to
see all other MOSIX nodes. The mosix.map looked as follows:
<pre     class="programlisting">
# Starting node  IP         Number of Nodes
1                x.x.x.x    13
14               y.y.y.y    1
</pre>


We created the /mfs directory to be used as a mount point for the
MOSIX filesystem. We added mosix.o to /lib/modules/2.2.14-5.0/misc/
so it can be loaded at boot time by the MOSIX startup file. Then we
applied the same modifications to the ramdisk that will be
downloaded by the diskless CPUs at boot time.
</p><p>Once we completed these steps, we rebooted PC1, and when it
was up and running, we rebooted the diskless CPUs. After reboot,
the diskless CPUs received their IP addresses, booted with the
MOSIX-enabled kernel, and downloaded the ramdisk using the TFTP
protocol. <span   class="emphasis"><em>Et voil&agrave;</em></span>! All 14 nodes
mounted /mfs as the MOSIX filesystem directory. Figure 2 shows a
snapshot of /mfs on CPU10.</p><div       class="mediaobject"><img src="4546f4.jpg"><div class="caption"><p>
Figure 2. MFS Mount Point on CPU10
</p></div></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x23078b8"></a>Testing the Installation</h2></div></div><p>After starting the 14 nodes as a MOSIX cluster, we wanted to
test our installation. By default, all the diskless CPUs mount an
NFS directory on PC1. So we placed the Linux kernel 2.2.14 source
code directory under that NFS space, making it visible to all
nodes, and we started the kernel compilation process using
<span   class="bold"><b>MExec/MPMake</b></span>, the parallel make, a
contributed software that assigns new processes to the best
available cluster nodes (available for download from MOSIX web
site).</p><p>Figures 3, 4, 5 and 6 show snapshots of
<span   class="bold"><b>mon</b></span>, a MOSIX tool that shows the
load on all the nodes. As Figure 3 shows, there was a high load on
node 14 because it was the node on which the compilation started. A
few seconds later, Figures 4 and 5 show less load on CPU 14, and
then Figure 6 shows a good distribution of the load among all the
nodes.</p><p>
<div       class="mediaobject"><img src="4546f5.jpg"></div>

<div       class="mediaobject"><img src="4546f6.jpg"></div>
</p><p>
<div       class="mediaobject"><img src="4546f7.jpg"><div class="caption"><p>
Figure 3, 4, 5 and 6. MOSIX at Work Distributing Loads
</p></div></div>

<div       class="mediaobject"><img src="4546f8.jpg"><div class="caption"><p>
Scalability
</p></div></div>
</p><p>MOSIX supports configurations with large numbers of computers
with minimal scaling overheads to impair the performance. You can
have a simple low-end setup composed of several PCs connected via
Ethernet, on the other hand, you can have larger configurations
that include workstations and servers connected via higher speed
LANs such as fast Ethernet. A high-end configuration may also
include a large number of SMP and non-SMP workstations and servers
connected via a high-performance LAN such as
Gigabit-Ethernet.</p><p>Our last experiment will include testing MOSIX on a new
self-contained NEBS-compliant cabinet that consists of 16 Pentium
III processors powered with 512MB of RAM and running at 500MHz.
Each CPU has two on-board Ethernet ports and is also paired with a
four-port ZNYX Ethernet card (used to provide Ethernet redundancy).
Eight of the CPUs have a RAID setup (RAID 0 and RAID 5) with three
18GB SCSI disks.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x23081a8"></a>Getting MOSIX</h2></div></div><p>MOSIX for Linux is subject to the GNU General Public License
version 2, as published by the Free Software Foundation. It is
available for download from the MOSIX web site (see
Resources).</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x23082b0"></a>Uninstalling MOSIX</h2></div></div><p>MOSIX allows us to do an uninstall and clean up the kernel
source it modified. During the initial installation, mosix.install
modifies the following system configuration files: /etc/inittab,
/etc/inetd.conf, /etc/lilo.conf, /etc/rc.d/init.d/atd and
/etc/cron.daily/slocate.cron.</p><p>The original contents of these files are saved with the
.pre_mosix extension and the changes made to kernel files are
logged to the mos_uninstall.log file in the kernel-source
directory. To uninstall MOSIX, you run the command
<b  >./mosix.install uninstall</b> and answer the
questions. When you are asked if you want to clean the Linux kernel
sources, answer &ldquo;yes&rdquo;. The script will then attempt to revert all
the changes that were made during a previous MOSIX installation. At
the end you need to reboot the node so start it as a plain Linux
node.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x2308468"></a>Conclusion</h2></div></div><p>Clustering offers several advantages that result in sharing
the processing and the ability to achieve higher performance. If
you are interested in clustering your servers with efficient
load-balancing software and you need support for high performance,
then MOSIX can certainly be useful for you. It is easy to install
and configure, and it works.</p><p>However, our initial interest with MOSIX was to understand
its algorithms and investigate the possibility of using it for
efficient distribution of web traffic over multiple processors. We
found that MOSIX is not directly suitable for the type of
functionality we want for a near telecom-internet server that we
are aiming to prototype, mainly because it is missing a front-end
tool for transaction-oriented load balancing such as the HTTP
requests.</p><p>There have been many requests to the MOSIX mailing list
asking about HTTP traffic distribution with MOSIX. I believe that
if the authors would add this functionality to MOSIX, MOSIX will be
one of the most popular software packages for Linux
clusters.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2217580.0x2308620"></a>Acknowledgements</h2></div></div><p>The Systems Research Department at Ericsson Research Canada
for providing the facilities and equipment as well as approving the
publication of this article. Marc Chatel, Ericsson Research Canada,
for his help and support in the lab.</p><p><a href="4546s1.html" target="_self">Resources</a></p></div></div>
<div class="authorblurb"><p>
          <div       class="mediaobject"><img src="4546aa.jpg"></div>
          <span   class="bold"><b>Ibrahim F. Haddad</b></span>
          (<a href="mailto:ibrahim.haddad@lmc.ericsson.se">ibrahim.haddad@lmc.ericsson.se</a>) works for
          Ericsson Research Canada in the Systems Research Department
          researching carrier class server nodes in real time on IP networks.
          He is currently a DrSc Candidate in the Computer Science Department
          at Concordia University.
        </p><p>
          <span   class="bold"><b>Evangeline Paquin</b></span>
          (<a href="mailto:lmcevpa@lmc.ericsson.se">lmcevpa@lmc.ericsson.se</a>) is a computer science
          student at UQAM University in Montr&eacute;al. She completed her
          coop training at Ericsson Research Canada working on Linux
          clustering solutions, and currently she is a part-time staff member
          in the System Research Department.
        </p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../085/toc085.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>