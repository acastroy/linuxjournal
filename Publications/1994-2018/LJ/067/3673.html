<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Working with LWP</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    This month Mr. Lerner takes a look at the library for web&#10;    programming and its associated modules.&#10;    "><meta name="keywords" content="LWP, library, web, server"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1151580.0x1248ab0"></a>Working with LWP</h1></div><div><div class="author"><h3 class="author">Reuven M. Lerner</h3></div><div class="issuemoyr">Issue #67, November 1999</div></div><div><p>
    This month Mr. Lerner takes a look at the library for web
    programming and its associated modules.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x1249558"></a></h2></div></div><p>Most of the time, this column discusses
ways in which we can improve or customize the work done by web
servers. Whether we are working with CGI programs or
<span   class="bold"><b>mod_perl</b></span> modules, we are usually
looking at things from the server's perspective.
</p><p>This month, we will look at LWP, the &ldquo;library for web
programming&rdquo; available for Perl, along with several associated
modules. The programs we will write will be web clients, rather
than web servers. Server-side programs receive HTTP requests and
generate HTTP responses; our programs this month will generate the
requests and wait for the responses generated by the server.</p><p>As we examine these modules, we will gain a better
understanding of how HTTP works, as well as how to use the various
modules of LWP to construct all sorts of programs that retrieve and
sort information stored on the Web.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x1249768"></a>Introduction to HTTP</h2></div></div><p>HTTP, the &ldquo;hypertext transfer protocol&rdquo;, makes the Web
possible. HTTP is one of many protocols used on the Internet and is
considered a high-level protocol, alongside SMTP (the simple mail
transfer protocol) and FTP (file transfer protocol). These are
considered high-level protocols because they sit on a foundation of
lower-level protocols that handle the more mundane aspects of
networking. HTTP messages don't have to worry about handling
dropped packets and routing, because TCP and IP take care of such
things for it. If there is a problem, it will be taken care of at a
lower level.</p><p>Dividing problems up in this way allows you to concentrate on
the important issues, without being distracted by the minute
details. If you had to think about your car's internals every time
you wanted to drive somewhere, you would quickly find yourself
concentrating on too many things at once and unable to perform the
task at hand. By the same token, HTTP and other high-level
protocols can ignore the low-level details of how the network
operates, and simply assume the connection between two computers
will work as advertised.</p><p>HTTP operates on a client-server model, in which the computer
making the request is known as the client, and the computer
receiving the request and issuing a response is the server. In the
world of HTTP, servers never speak before they are spoken to&mdash;and
they always get the last word. This means a client's request can
never depend on the server's response; a client interested in using
a previous response to form a new request must open a new
connection.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x1249920"></a>Sending a Request</h2></div></div><p>Given all of that theory, how does HTTP work in practice? You
can experiment for yourself, using the simple
<span   class="bold"><b>telnet</b></span> command.
<span   class="bold"><b>telnet</b></span> is normally used to access
another computer remotely, by typing:</p><pre     class="programlisting">
telnet remotehost
</pre><p>That demonstrates the default behavior, in which telnet opens
a connection to port 23, the standard port for such access. You can
use telnet to connect to other ports as well, and if there is a
server running there, you can even communicate with it.
</p><p>Since HTTP servers typically run on port 80, I can connect to
one with the command:</p><pre     class="programlisting">
telnet www.lerner.co.il 80
</pre><p>I get the following response on my Linux box:
<pre     class="programlisting">
Trying 209.239.47.145...
Connected to www.lerner.co.il.
Escape character is '^]'.
</pre>


Once we have established this connection, it is my turn to talk. I
am the client in this context, which means I must issue a request
before the server will issue any response. HTTP requests consist,
at minimum, of a method, an object on which to apply that method,
and an HTTP version number. For instance, we can retrieve the
contents of the file at / by typing
<pre     class="programlisting">
GET / HTTP/1.0
</pre>


This indicates we want the file at / to be returned to us, and that
the highest-numbered version of HTTP we can handle is HTTP/1.0. If
we were to indicate that we support HTTP/1.1, an advanced server
would respond in kind, allowing us to perform all sorts of nifty
tricks.
</p><p>If you pressed <b  >return</b> after issuing the
above command, you are probably still waiting to receive a
response. That's because HTTP/1.0 introduced the idea of &ldquo;request
headers&rdquo;, additional pieces of information that a client can pass
to a server as part of a request. These client headers can include
cookies, language preferences, the previous URL this client visited
(the &ldquo;referer&rdquo;) and many other pieces of information.</p><p>Because we will stick with a simple
<span   class="bold"><b>GET</b></span> request, we press
<b  >return</b> twice after our one-line command: once to
end the first line of our request, and another to indicate we have
nothing more to send. As with e-mail messages, a blank line
separates the headers&mdash;information about the message&mdash;from the
message itself.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x1249ef8"></a>Examining the response</h2></div></div><p>After typing <b  >return</b> a second time, you
should see the contents of http://www.lerner.co.il/ returned to
you. Once the document has been transferred to your terminal, the
connection is terminated. If you want to connect to the same server
again, you may do so. However, you will have to issue a new
connection and a new request.</p><p>Just as the client can send request headers before the
request itself, the server can send response headers before the
response. As in the case with request headers, there must be a
blank line between the response headers and the body of the
response.</p><p>Here are the headers I received after issuing the above GET
request:</p><pre     class="programlisting">
HTTP/1.1 200 OK
Date: Thu, 12 Aug 1999 19:36:44 GMT
Server: Apache/1.3.6 (UNIX) PHP/3.0.11 FrontPage/3.0.4.2 Rewrit/1.0a
Connection: close
Content-Type: text/html
</pre><p>The above lines are typical for a response.
</p><p>The first line produces general information about the
response, including an indication of what is yet to come. First,
the server tells us it is capable of handling anything up to
HTTP/1.1. If we ever want to send a request using HTTP/1.1, this
server will allow it. After the HTTP version number comes a
response code. This code can indicate a variety of possibilities,
including whether everything went just fine (200), the file has
moved permanently (301), the file was not found (404), or there was
an error on the server side (501).</p><p>The numeric code is typically followed by a text message,
which gives an indication of the meaning behind the numbers. Apache
and other servers might allow us to customize the page displayed
when an error occurs, but that customization does not extend to
this error code, which is standard and fixed.</p><p>Following the error code comes the date on which the response
was generated. This header is useful for proxies and caches, which
can then store the date of a document along with its contents. The
next time your browser tries to retrieve a file, it will compare
the <b  >Date:</b> header from the previous response,
retrieving the new version only if the server's version is
newer.</p><p>The server identifies itself in the
<b  >Server:</b> header. In this particular case, the
server tells us not only that it is Apache 1.3.6 running under a
form of UNIX (in this case, Linux), but also some modules that have
been installed. My web-space provider has chosen to install PHP,
FrontPage and Rewrit; as we have seen in previous months, mod_perl
is another popular module for server-side programming, and one
which advertises itself in this header.</p><p>As we have seen, an HTTP connection terminates after the
server finishes sending its response. This can be extremely
inefficient; consider a page of HTML that contains five
<b  >IMG</b> tags, indicating where images should be
loaded. In order to download this page in its entirety, a web
browser has to create six separate HTTP connections&mdash;one for the
HTML and one for each of the images. To overcome this inefficiency,
HTTP/1.1 allows for &ldquo;persistent connections&rdquo;, meaning that more
than one document can be retrieved in a single HTTP transaction.
This is signalled with the <b  >Connection</b> header,
which indicated it was ready to close the connection after a single
transaction in the example above.</p><p>The final header in the above output is
<b  >Content-type</b>, well-known to CGI programmers.
This header uses a MIME-style description to tell the browser what
kind of content to expect. Should it expect HTML-formatted text
(<b  >text/html</b>)? Or a JPEG image
(<b  >image/jpeg</b>)? Or something that cannot be
identified, which should be treated as binary data
(<b  >application/octet-stream</b>)? Without such a
header, your browser will not know how to treat the data it
receives, which is why servers often produce error messages when
<b  >Content-type</b> is missing.</p><p>HTTP/1.0 supports many methods other than GET, but the main
ones are GET, <span   class="bold"><b>HEAD</b></span>, and
<span   class="bold"><b>POST</b></span>. GET, as its name implies,
allows us to retrieve the contents of a link. This is the most
common method, and is behind most of the simple retrievals your web
browser performs. HEAD is the same as GET, but quits after printing
the response headers. Sending a request of</p><pre     class="programlisting">
HEAD / HTTP/1.0
</pre><p>is a good way to test your web server and see if it is
running correctly.
</p><p>POST not only names a path on the server's computer, but also
sends input in name,value pairs. (GET can also submit information
in name,value pairs, but it is considered less desirable in most
situations.) POST is usually invoked when a user clicks on the
&ldquo;submit&rdquo; button in an HTML form.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x124a8f0"></a>LWP::Simple</h2></div></div><p>Now that we have an understanding of the basics behind HTTP,
let's see how we can handle requests and responses using Perl.
Luckily, LWP contains objects for nearly everything we might want
to do, with code tested by many people.</p><p>If we simply want to retrieve a document using HTTP, we can
do so with the <b  >LWP::Simple</b> module. Here, for
instance, is a simple Perl program that retrieves the root document
from my web site:</p><pre     class="programlisting">
#!/usr/bin/perl --w
use strict;
use diagnostics;
use LWP::Simple;
# Get the contents
my $content = get "http://www.lerner.co.il/";
# Print the contents
print $content, "\n";
</pre><p>In this particular case, the startup and diagnostics code is
longer than the program. Importing <b  >LWP::Simple</b>
into our program automatically brings the
<span   class="bold"><b>get</b></span> function with it, which takes
a URL, retrieves its contents with GET, and returns the body of the
response. In this example, we print that output to the screen.
</p><p>Once the document's contents are stored in
<b  >$content</b>, we can treat it as a normal Perl
scalar, albeit one containing a fair amount of text. At this point,
we could search for interesting text, perform search-and-replace
operations on <b  >$content</b>, remove any parts we find
offensive, or even translate parts into Pig Latin. As an example,
the following variation of this simple program turns the contents
around, reversing every line so that the final line becomes the
first line and vice versa; and every character on every line so
that the final character becomes the first and vice versa:</p><pre     class="programlisting">
#!/usr/bin/perl -w
use strict;
use diagnostics;
use LWP::Simple;
# Get the contents
my $content = get "http://www.lerner.co.il/";
# Print the contents
print scalar reverse $content, "\n";
</pre><p>Note how we must put <b  >reverse</b> in scalar
context in order for it to do its job. Since
<span   class="bold"><b>print</b></span> takes a list of arguments,
we force scalar context with the <b  >scalar</b> keyword.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x1241758"></a>HTTP::Request and HTTP::Response</h2></div></div><p>There are times, however, when we will want to create more
sophisticated applications, which in turn require more
sophisticated means of contacting the server. Doing this will
require a number of different objects, each with its own
task.</p><p>First and foremost, we will have to create an
<b  >HTTP::Request</b> object. This object, as you can
guess from its name, handles everything having to do with an HTTP
request. We can create it most easily by saying:</p><pre     class="programlisting">
use HTTP::Request;
my $request = new HTTP::Request("GET",
   "http://www.lerner.co.il");
</pre><p>where the first argument indicates the request method we wish
to use, and the second argument indicates the target URL.
</p><p>Once we have created an HTTP request, we need to send it to
the server. We do this with a &ldquo;useragent&rdquo; object, which acts as a
go-between in this exchange. We have already looked at the
<b  >LWP::Simple</b> useragent in our example programs
above.</p><p>Normally, a useragent takes an
<b  >HTTP::Request</b> object as an argument, and returns
an <b  >HTTP::Response</b> object. In other words, given
<b  >$request</b> as defined above, our next two steps
would be the following:</p><pre     class="programlisting">
my $ua = new LWP::UserAgent;
my $response = $ua-&gt;request($request);
</pre><p>After we have created an <b  >HTTP::Response</b>
and assigned it to <b  >$response</b>, we can perform all
sorts of tests and operations.
</p><p>For starters, we probably want to know the response code we
received as part of the response, to ensure that our request was
successful. We can get the response code and the accompanying text
message with the <span   class="bold"><b>code</b></span> and
<span   class="bold"><b>message</b></span> methods:</p><pre     class="programlisting">
my $response_code = $response-&gt;code;
my $response_message = $response-&gt;message;
</pre><p>If we then say:
<pre     class="programlisting">
print qq{Code: "$code"\n};
print qq{Message: "$message"\n};
</pre>


we will get the output:
<pre     class="programlisting">
Code: "200"
Message: "OK"
</pre>


This is well and good, but it presents a bit of a problem: How do
we know how to react to different response codes? We know that 200
means everything was fine, but we must build up a table of values
in order to know which response codes mean we can continue, and
which mean the program should exit and signal an error.
</p><p>The <span   class="bold"><b>is_success</b></span> method for
<b  >HTTP::Response</b> handles this for us. With it, we
can easily check to see if our request went through and if we
received a response:</p><pre     class="programlisting">
if (!$response-&gt;is_success)
   {print "Success. \n";}
else
   {print "Error: " . $response-&gt;status_line . "\n";
}
</pre><p>The <span   class="bold"><b>status_line</b></span> method
combines the output from code and message to produce a numeric
response code and its printed description.
</p><p>We can examine the response headers with the
<span   class="bold"><b>headers</b></span> method. This returns an
instance of <b  >HTTP::Headers</b>, which offers many
convenient methods that allow us to retrieve individual header
values:</p><pre     class="programlisting">
my $headers = $response-&gt;headers;
print "Content-type:", $headers-&gt;content_type,
   "\n";
print "Content&mdash;length:",
   $headers-&gt;content_length, "\n";
print "Date:", $headers-&gt;date, "\n
print "Server:", $headers-&gt;server, "\n";
</pre></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x1242308"></a>Working with the Content</h2></div></div><p>Of course, the Web is not very useful without the contents of
the documents we retrieve. <b  >HTTP::Response</b> has
only one method for retrieving the content of the response,
unsurprisingly named <span   class="bold"><b>content</b></span>. We
can thus say:</p><pre     class="programlisting">
my $content = $response-&gt;content;
</pre><p>At this point, we are back to where we were with our
<b  >LWP::Simple</b> example earlier: We have the content
of the document inside of <b  >$content</b>, which stores
the text as a normal string.
</p><p><a href="3673l1.html" target="_self">Listing 1</a></p><p>If we were interested in using
<b  >HTTP::Request</b> and
<b  >HTTP::Response</b> to reverse a document, we could
do it as shown in Listing 1. If you were really interested in
producing a program like this one, you would probably stick with
<b  >LWP::Simple</b> and use the get method described
there. There is no point in weighing down your program, as well as
adding all sorts of method calls, if the object is simply to
retrieve the contents from a URL.</p><p>The advantage of using a more sophisticated user agent is the
additional flexibility it offers. Whether that flexibility is worth
the tradeoff in complexity will depend on your needs.</p><p>For example, many sites have a robots.txt in their root
directory. Such a file tells &ldquo;robots&rdquo;, or software-controlled web
clients, which sections of the site should be considered out of
bounds. These files are a matter of convention, but are a
long-standing tradition which should be followed. Luckily, LWP
includes the object <b  >LWP::RobotUA</b>, which is a
user agent that automatically checks the robots.txt file before
retrieving a file from a web site. If a file is excluded by
robots.txt, <b  >LWP::RobotUA</b> will not retrieve
it.</p><p><b  >LWP::RobotUA</b> also differs from
<b  >LWP::UserAgent</b> in its attempt to be gentle to
servers, by sending only one request per minute. We can change this
with the <span   class="bold"><b>delay</b></span> method, although
doing so is advisable only when you are familiar with the site and
its ability to handle a large number of automatically generated
requests.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x1242af0"></a>Extracting Tags</h2></div></div><p>Once we have retrieved the content from a web site, what can
we do with it? As demonstrated above, we can print it out or play
with the text. But many times, we want to analyze the tags in the
document, picking out the images, the hyperlinks or even the
headlines.</p><p>In order to do this, we could use regular expressions and
<b  >m//</b>, Perl's matching operator. But an easier way
is to use <b  >HTML::LinkExtor</b>, another object that
is designed for this purpose. Once we create an instance of
<b  >HTML::Extor</b>, we can then use the
<span   class="bold"><b>parse</b></span> method to retrieve each of
the tags in it.</p><p><b  >HTML::LinkExtor</b> works differently from
many modules you might have used before, in that it uses a
&ldquo;callback&rdquo;. In this case, a callback is a subroutine defined to
take two arguments&mdash;a scalar containing the name of the tag and a
hash containing the name,value pairs associated with that tag. The
subroutine is invoked each time <b  >HTML::LinkExtor</b>
finds a tag.</p><p>For example, given the HTML</p><pre     class="programlisting">
&lt;input type="text" value="Reuven"
     name="first_name" size="5"&gt;
</pre><p>our callback would have to be prepared to handle a scalar of
value <b  >input</b>, with a hash that looks like
<pre     class="programlisting">
(type =&gt; "text", value =&gt; "Reuven",
     name =&gt; "first_name", size =&gt; "5")
</pre>


<a href="3673l2.html" target="_self">Listing 2</a>
</p><p>If we are interested in printing the various HTML tags to the
screen, we could write a simple callback that looks like Listing 2.
How do we tell <b  >HTML::LinkExtor</b> to invoke our
callback subroutine each time it finds a match? The easiest way
would be for us to hand callback to the parse method as an
argument.</p><p>Perl allows us to pass subroutines and other blocks of code
as if they were data by creating references to them. A reference
looks and acts like a scalar, except that it can be turned into
something else. Perl has scalar, array and hash references;
subroutine references fit naturally into this picture as well.
<b  >HTML::LinkExtor</b> will dereference and use our
subroutine as we have defined it.</p><p>We turn a subroutine into a subroutine reference by prefacing
its name with <b  >\&amp;</b>. Perl 5 no longer requires
that you put <b  >&amp;</b> before subroutine names, but
it is required when you are passing a subroutine reference. The
backslash tells Perl we want to turn the object in question into a
reference. If <b  >&amp;callback</b> is defined as above,
then we can print out all of the links in a document with the
following:</p><pre     class="programlisting">
my $parser = HTML::LinkExtor-&gt;new(\&amp;callback);
$parser-&gt;parse($response-&gt;content);
</pre><p>Note that <b  >$content</b> might have all HTML
links that were returned with the HTTP response. However, that
response undoubtedly contained some relative URLs, which will not
be interpreted correctly out of context. How can we accurately view
the link?
</p><p><b  >HTML::LinkExtor</b> takes that into account,
and allows us to pass two arguments to its constructor
(<span   class="bold"><b>new</b></span>), rather than just one. The
second argument, which is optional, is the URL from which we
received this content. Passing this URL ensures all URLs we extract
will be complete. We must include the line</p><pre     class="programlisting">
use URI::URL;
</pre><p>in our application if we want to use this feature. We can
then say
<pre     class="programlisting">
my $parser = HTML::LinkExtor-&gt;new(\&amp;callback,
                       "http://www.lerner.co.il/");
    $parser-&gt;parse($response-&gt;content);
</pre>


and our callback will be invoked for each tag, with a full,
absolute URL even if the document contains a relative one.
</p><p>Our version of <b  >&amp;callback</b> above prints
out all links, not just hyperlinks. We can ignore all but
&ldquo;anchor&rdquo; tags, which allow us to create hyperlinks by modifying
<b  >&amp;callback</b> slightly, as shown in Listing
3.</p><p><a href="3673l3.html" target="_self">Listing 3</a></p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x1557da8"></a>A Small Demonstration</h2></div></div><p>With all of this under our belts, we will write an
application (Listing 4) that follows links recursively until the
program is stopped. This sort of program can be useful for checking
links on your site or harvesting information from documents.</p><p><a href="3673l4.html" target="_self">Listing 4</a></p><p>Our program, download-recursively.pl, starts at the URL
called <span   class="emphasis"><em>$origin</em></span> and collects the URLs contained
within it, placing them in the hash
<span   class="emphasis"><em>%to_be_retrieved</em></span>. It then goes through each of
those URLs one by one, collecting any hyperlinks that might be
contained within them. Each time it retrieves a URL,
download-recursively.pl places it in
<span   class="emphasis"><em>%already_retrieved</em></span>. This ensures we will not
download the same URL twice.</p><p>We create <b  >$ua</b>, our instance of
<b  >LWP::RobotUA</b>, outside of the &ldquo;while&rdquo; loop.
After all, our HTTP requests and responses will be changing with
each loop iteration, but our user agent can remain the same
throughout the life of the program.</p><p>We go through each URL in
<span   class="emphasis"><em>%to_be_retrieved</em></span> in a seemingly random order,
taking the first item returned by <b  >keys</b>. It is
obviously possible to sort the <b  >keys</b> before
taking the first element from the resulting list or to do a
depth-first or breadth-first search through the list of
URLs.</p><p>Inside the loop, the code is as we might expect: we create a
new instance of <b  >HTTP:Request</b> and pass it to
<b  >$ua</b>, receiving a new instance of
<b  >HTTP:Response</b> in return. Then we parse the
response content with <b  >HTML::LinkExtor</b>, putting
each new URL in <span   class="emphasis"><em>%to_be_retrieved</em></span>, but only on
the condition that it is not already a key in
<span   class="emphasis"><em>%already_retrieved</em></span>.</p><p>You may find it interesting to let this program go for a
while, following links from one of your favorite sites. The Web is
all about linking; see who is linking to whom. You might be
surprised by what you find.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1151580.0x15585e8"></a>Conclusion</h2></div></div><p>Our whirlwind tour of HTTP and LWP stops here. As you can
see, there is not very much to learn about either of them; the
trick is to use them to create interesting applications and avoid
the pitfalls when working with them. LWP is a package I do not
often need, but when I do need it, I find it indispensable.</p><p><a href="3673s1.html" target="_self">Resources</a></p></div></div>
<div class="authorblurb"><p>
        <div       class="mediaobject"><img src="3673aa.jpg"></div>


      <span   class="bold"><b>Reuven M. Lerner</b></span>
      is an Internet and Web
      consultant living in Haifa, Israel. His book Core Perl
      will be published by Prentice-Hall later this year. Reuven can be
      reached at reuven@lerner.co.il. The ATF home page is at
      http://www.lerner.co.il/atf/.</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../067/toc067.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>