<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Use Linux as a SAN Provider
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;At one-tenth the cost of&#10;the typical commercial appliance, Linux can deliver storage with speed&#10;and redundancy.&#10;Make the move toward a full-featured iSCSI SAN&#10;solution with what you already have in your server room.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x25df580.0x26d6ac0"></a>
Use Linux as a SAN Provider
</h1></div><div><div class="author"><h3 class="author">
Michael
 
Nugent
</h3></div><div class="issuemoyr">Issue #187, November 2009</div></div><div><p>
At one-tenth the cost of
the typical commercial appliance, Linux can deliver storage with speed
and redundancy.
Make the move toward a full-featured iSCSI SAN
solution with what you already have in your server room.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x25df580.0x26d72a8"></a></h2></div></div><p>
Storage Area Networks (SANs) are becoming commonplace in the industry.
Once restricted to large data centers and Fortune 100 companies, this
technology has dropped in price to the point that small startups are
using them for centralized storage. The strict definition of a SAN is
a set of storage devices that are accessible over the network at a
block level. This differs from a Network Attached Storage (NAS)
device in that a NAS runs its own filesystem and presents that volume
to the network; it does not need to be formatted by the client
machine. Whereas a NAS usually is presented with the NFS or CIFS
protocol, a SAN running on the same Ethernet often is presented as
iSCSI, although other technologies exist. 
</p><p>
iSCSI is the same SCSI
protocol used for local disks, but encapsulated inside IP to allow it
to run over the network in the same way any other IP protocol does.
Because of this, and because it is seen as a block device, it often
is almost indistinguishable from a local disk from the point of view of
the client's operating system and is completely transparent to
applications.
</p><p>
The iSCSI protocol is defined in RFC 3720 and runs over TCP ports 860
and 3260. In addition to the iSCSI protocol, many SANs implement
Fibre Channel as a mechanism. This is an improvement over Gigabit
Ethernet, mainly because it is 4 or 8Gb/s as opposed to 1Gb/s. In the
same vein, 10 Gigabit Ethernet would have an advantage over Fibre
Channel. 
</p><p>
The downside to Fibre Channel is the expense. A Fibre
Channel switch often runs many times the cost of a typical Ethernet
switch and comes with far fewer ports. There are other advantages to
Fibre Channel, such as the ability to run over very long distances, but
these aren't usually the decision-making factors when purchasing a
SAN. 
</p><p>
In addition to Fibre Channel and iSCSI, ATA over Ethernet (AoE)
also is starting to make some headway. In the same way that iSCSI
provides SCSI commands over an IP network, AoE provides ATA commands
over an Ethernet network. AoE actually is running directly on
Ethernet, not on top of IP the way iSCSI does. Because of this, it
has less overheard and often is faster than iSCSI in the same
environment. The downside is that it cannot be routed. AoE also is
far less mature than iSCSI, and fewer large networking companies are
looking to support AoE. Another disadvantage of AoE is that it has no
built-in security outside of MAC filtering. As it is relatively easy
to spoof a MAC address, this means anyone on the local network
can access any AoE volumes.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x25df580.0x26d75c0"></a>Should You Use a SAN?</h2></div></div><p>
The first step in moving down the road to a SAN is the choice of
whether to use it. Although a SAN often is faster than a NAS, it
also is less flexible. For example, the size of or the filesystem
of a NAS usually can be changed on the host system without the client system having
to make any changes. With a SAN, because it is seen as a block device
like a local disk, it is subject to a lot of the same rules as a local
disk. So, if a client is running its /usr filesystem on an
iSCSI device, it would have to be taken off-line and modified not just
on the server side, but also on the client side. The client would
have to grow the filesystem on top of the device. 
</p><p>
There are some
significant differences between a SAN volume and a local disk. A SAN
volume can be shared between computers. Often, this presents all
kinds of locking problems, but with an application aware that
its volume is shared out to multiple systems, this can be a powerful tool for
failover, load balancing or communication. Many filesystems exist
that are designed to be shared. GFS from Red Hat and OCFS from Oracle
(both GPL) are great examples of the kinds of these filesystems.
</p><p>
The network is another consideration in choosing a SAN. Gigabit
Ethernet is the practical minimum for running modern network storage.
Although a 100- or even a 10-megabit network theoretically would work, the
practical results would be extremely slow. If you are running many volumes or
requiring lots of reads and writes to the SAN,
consider running a dedicated gigabit network. This will prevent the
SAN data from conflicting with your regular IP data and, as an added
bonus, increase security on your storage.
</p><p>
Security also is a concern. Because none of the major SAN protocols
are encrypted, a network sniffer could expose your data. In theory,
iSCSI could be run over IPsec or a similar protocol, but without hardware
acceleration, doing so would mean a large drop in performance. In lieu of
this, at the very least, keeping the SAN data on its own VLAN is
required.
</p><p>
Because it is the most popular of the various SAN protocols available
for Linux, I use iSCSI in the examples in this article. But, the
concepts should transfer easily to AoE if you've selected that for
your systems. If you've selected Fibre Channel, things still
are similar, but not as similar. You will need to rely more on your
switch for most of your authentication and routing. On the positive
side, most modern Fibre Channel switches provide excellent setup tools
for doing this.
</p><p>
To this point, I have been using the terms client and
server, but
that is not completely accurate for iSCSI technology. In the iSCSI
world, people refer to clients as initiators and servers or other
iSCSI
storage devices as targets. Here, I use the Open-iSCSI
Project to provide the initiator and the iSCSI Enterprise Target (IET)
Project to provide the target. These pieces of software are available in the default
repositories of most major Linux distributions. Consult 
your distribution's documentation for the package names to install or
download the source from <a href="http://www.open-iscsi.org" target="_self">www.open-iscsi.org</a> and
<a href="http://iscsitarget.sourceforge.net" target="_self">iscsitarget.sourceforge.net</a>. Additionally, you'll need iSCSI
over TCP/IP in your kernel, selectable in the low-level SCSI drivers
section.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x25df580.0x26d7988"></a>Setting Up the Initiator and Target</h2></div></div><p>
In preparation for setting up the target, you need to provide it with a
disk. This can be a physical disk or you can create a disk image. In
order to set up a disk image, run the dd command:


<pre     class="programlisting">
dd if=/dev/zero of=/srv/iscsi.image.0 bs=1 seek=10M count=1
</pre>
</p><p>
This command creates a file about 10MB called /srv/iscsi.image.0
filled with zeros. This is going to represent the first iscsi disk.
To create another, do this:


<pre     class="programlisting">
dd if=/dev/zero of=/srv/iscsi.image.1 bs=1 seek=10M count=1
</pre>
</p><p>
Configuration for the IET software is located in /etc/ietd.conf.
Though a lot of tweaks are available in the file, the important
lines really are just the target name and LUN. For each target,
exported disks must have a unique LUN. Target names are formatted
specially. The official term for this name is the iSCSI Qualified
Name (IQN).
</p><p>
The format is:

<pre     class="programlisting">
iqn.yyyy-mm.(reversed domain name):label
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x25df580.0x26d7d50"></a></h2></div></div><p>
where iqn is required, yyyy signifies a four-digit year, followed by mm
(a two-digit month) and a reversed domain name, such as org.michaelnugent.
The label is a user-defined string in order to better identify the
target.
</p><p>
Here is an example ietd.conf file using the images created above and a
physical disk, sdd:

<pre     class="programlisting">
Target iqn.2009-05.org.michaelnugent:iscsi-target
       IncomingUser michael secretpasswd
       OutgoingUser michael secretpasswd
       Lun 0 Path=/srv/iscsi.images.0,Type=fileio
       Lun 1 Path=/srv/iscsi.images.1,Type=fileio
       Lun 2 Path=/dev/sdd,Type=blockio
</pre>
</p><p>
The IncomingUser is used during discovery to authenticate iSCSI
initiators. If it is not specified, any initiator will be allowed to
connect to open a session. The OutgoingUser is used during discovery
to authenticate the target to the initiator. For simplicity, I made
them the same in this example, but they don't need to be. Note that
both of these are required by the RFC to be 12 characters long. The
Microsoft initiator enforces this strictly, though the Linux one does
not.
</p><p>
Start the server using <tt  >/etc/init.d/iscsitarget start</tt> (this may change
depending on your distribution). Running <tt  >ps ax | grep
ietd</tt> will
show you that the server is running.
</p><p>
Now you can move on to setting up the initiator to receive data from
the target. To set up an initiator, place its name (in IQN format)
in the /etc/iscsi/initiatorname.iscsi file
(or possibly /etc/initiatorname.iscsi).
An example of a well-formatted file would be the following:

<pre     class="programlisting">
InitiatorName=iqn.2009-05.org.michaelnugent:iscsi-01
</pre>
</p><p>
In addition, you also need to modify the /etc/iscsi/iscsid.conf file to match the
user names and passwords set in the ietd.conf file above:

<pre     class="programlisting">
node.session.auth.authmethod = CHAP
node.session.auth.username = michael
node.session.auth.password = secretpasswd
node.session.auth.username_in = michael
node.session.auth.password_in = secretpasswd
discovery.sendtargets.auth.authmethod = CHAP
discovery.sendtargets.auth.username = michael
discovery.sendtargets.auth.password = secretpasswd
discovery.sendtargets.auth.username_in = michael
discovery.sendtargets.auth.password_in = secretpasswd
</pre>
</p><p>
Once this is done, run the iscsiadm command to discover the target.


<pre     class="programlisting">
iscsiadm -m discovery -t sendtargets -p 192.168.0.1 -P 1
</pre>
</p><p>
This should output the following:

<pre     class="programlisting">
Target: iqn.2009-05.org.michaelnugent:iscsi-target
     Portal: 192.168.0.1:32360,1
          IFace Name: default
</pre>
</p><p>
Now, at any time, you can run:

<pre     class="programlisting">
iscsiadm -m node -P1
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x25df580.0x26d8430"></a></h2></div></div><p>
which will redisplay the target information.
</p><p>
Now, run <tt  >/etc/init.d/iscsi restart</tt>. Doing so will connect to the new
block devices. Run <tt  >dmesg</tt> and <tt  >fdisk
-l</tt> to view them. Because these are
raw block devices, they look like physical disks to Linux. They'll
show up as the next SCSI device, such as /dev/sdb. They still need to
be partitioned and formatted to be usable. After this is done, mount
them normally and they'll be ready to use.
</p><p>
This sets up the average iSCSI volume. Often though, you may want machines to run
entirely diskless. For that, you need to run root on iSCSI as well.
This is a bit more involved. The easiest, but more expensive way is
to employ a network card with iSCSI built in. That allows the card
to mount the volume and present it without having to do any additional
work. On the downside, these cards are significantly more expensive
than the average network card.
</p><p>
To create a diskless system without an iSCSI-capable network card,
you need to employ PXE boot. This requires that a DHCP server be available in
order for the initiator to receive an address. That DHCP server will
have to refer to a TFTP server in order for the machine to download
its kernel and initial ramdisk. That kernel and ramdisk will have
iSCSI and discovery information in it. This enables the average
PXE-enabled card to act as a more expensive iSCSI-enabled network card.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x25df580.0x26d8748"></a>Multipathing</h2></div></div><p>
Another feature often run with iSCSI is multipathing. This allows
Linux to use multiple networks at once to access the iSCSI target. It
usually is run on separate physical networks, so in the event that
one fails, the other still will be up and the initiator will
not experience loss of a volume or a system crash. Multipathing can
be set up in two ways, either active/passive or active/active.
Active/active generally is the preferred way, as it can be set up not
only for redundancy, but also for load balancing. Like Fibre Channel,
multipath assigns World Wide Identifiers (WWIDs) to devices. These are
guaranteed to be unique and unchanging. When one of the paths is
removed, the other one continues to function. The initiator may
experience slower response time, but it will continue to function.
Re-integrating the second path allows the system to return to its
normal state.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x25df580.0x26d88a8"></a>RAID</h2></div></div><p>
When working with local disks, people often turn to Linux's software RAID
or LVM systems to provide redundancy, growth and snapshotting.
Because SAN volumes show up as block devices, it is possible to use
these tools on them as well. Use them with care though. Setting up
RAID 5 across three iSCSI volumes causes a great deal of network traffic
and almost never gives you the results you're expecting. Although, if
you have enough bandwidth available and you aren't doing many writes,
a RAID 1 setup across multiple iSCSI volumes may not be
completely out of the question. If one of these volumes drops,
rebuilding may be an expensive process. Be careful about how much
bandwidth you allocate to rebuilding the array if you're in a production
environment. Note that this could be used at the same time as
multipathing in order to increase your bandwidth.
</p><p>
To set up RAID 1 over iSCSI, first load the RAID 1 module:

<pre     class="programlisting">
modprobe raid1
</pre>
</p><p>
After partitioning your first disk, /dev/sdb, copy the partition table
to your second disk, /dev/sdc. Remember to set the partition type to
Linux RAID autodetect:

<pre     class="programlisting">
sfdisk -d /dev/sdb | sfdisk /dev/sdc
</pre>
</p><p>
Assuming you set up only one partition, use the mdadm command to create
the RAID group:


<pre     class="programlisting">
mdadm --create /dev/md0 --level=1 --raid-disks=2 /dev/sdb1 /dev/sdc1
</pre>
</p><p>
After that, <tt  >cat</tt> the /etc/mdstat file to watch the state of the
synchronization of the iSCSI volumes. This also is a good time to
measure your network throughput to see if it will stand up under
production conditions.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x25df580.0x26d8cc8"></a>Conclusion</h2></div></div><p>
Running a SAN on Linux is an excellent way to bring up a shared
environment in a reasonable amount of time using commodity parts.
Spending a few thousand dollars to create a multiterabyte array is a
small budget when many commercial arrays easily can extend into the
tens to hundreds of thousands of dollars. In addition, you gain
flexibility. Linux allows you to manipulate the underlying
technologies in ways most of the commercial arrays do not. If you're
looking for a more-polished solution, the Openfiler Project provides
a nice layout and GUI to navigate. It's worth noting that many
commercial solutions run a Linux kernel under their shell, so unless
you specifically need features or support that isn't available with
standard Linux tools, there's little reason to look to commercial
vendors for a SAN solution.
</p></div></div>
<div class="authorblurb"><p>
Michael Nugent has spent a good deal of his time designing large-scale
solutions to fit into tiny budgets, leveraging Linux to fulfill
roles that typically would be filled by large commercial appliances.
Recently, Michael has been working to design large, private clouds for
SaaS environments in the financial industry. When not building
systems, he likes sailing, scuba diving and hanging out with his cat,
MIDI. Michael can be reached at <a href="mailto:michael@michaelnugent.org">michael@michaelnugent.org</a>.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../187/toc187.html">Issue Table of Contents</a>
    <a class="link3" href="../187/10556.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>