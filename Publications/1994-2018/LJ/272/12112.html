<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
At the Forge
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;It's easier than you think to teach your computer what makes for a tasty burrito.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x14a7580.0x159eac0"></a>
At the Forge
</h1></div><div><h3 class="subtitle"><i>
Teaching Your Computer
</i></h3></div><div><div class="author"><h3 class="author">
Reuven
 M. 
Lerner
</h3></div><div class="issuemoyr">Issue #272, December 2016</div></div><div><p>
It's easier than you think to teach your computer what makes for a tasty burrito.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x14a7580.0x159f3b0"></a></h2></div></div><p>
As I have written in my last two articles, machine learning is
influencing our lives in numerous ways. As a consumer, you've
undoubtedly experienced machine learning, whether you know it or not&mdash;from recommendations
for what products you should buy from various
online stores, to the selection of postings that appear (and don't) on
Facebook, to the maddening voice-recognition systems that airlines
use, to the growing number of companies that offer to select clothing,
food and wine for you based on your personal preferences.
</p><p>
Machine learning is everywhere, and although the theory and practice 
both can take some time to learn and internalize, the basics are fairly
straightforward for people to learn.
</p><p>
The basic idea behind machine learning is that you build a model&mdash;a
description of the ways the inputs and outputs are
related. This model then allows you to ask the computer to analyze new
data and to predict the outputs for new sets of inputs. This is
essentially what machine learning is all about. In &ldquo;supervised
learning&rdquo;, the computer is trained to categorize data based on inputs
that humans had previously categorized. In &ldquo;unsupervised learning&rdquo;, you
ask the computer to categorize data on your behalf.
</p><p>
In my last article, I started exploring a data set created by Scott Cole, a
data scientist (and neuroscience PhD student) who measured burritos in
a variety of California restaurants. I looked at the different
categories of data that Cole and his fellow eater-researchers
gathered and considered a few ways one could pare down the
data set to something more manageable, as well as reasonable.
</p><p>
Here I describe how to take this smaller data set, consisting
solely of the features that were deemed necessary, and use
it to train the computer by creating a machine-learning model.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x14a7580.0x159f6c8"></a>
Machine-Learning Models</h2></div></div><p>
Let's say that the quality of a burrito is determined solely by its
size. Thus, the larger the burrito, the better it is; the smaller the
burrito, the worse it is. If you describe the size as a matrix X, and
the resulting quality score as y, you can describe this
mathematically as:

<pre     class="programlisting">
y = qX
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x14a7580.0x159f828"></a></h2></div></div><p>
where q is a factor describing the relationship between X and y.
</p><p>
Of course, you know that burrito quality has to do with more than just
the size. Indeed, in Cole's research, size was removed from the list
of features, in part because not every data point contained size
information.
</p><p>
Moreover, this example model will need to take several factors&mdash;not just one&mdash;into consideration, and may have to combine them in a sophisticated
way in order to predict the output value accurately. Indeed, there are
numerous algorithms that can be used to create models; determining
which one is appropriate, and then tuning it in the right way, is part
of the game.
</p><p>
The goal here, then, will be to combine the burrito data and an algorithm
to create a model for burrito tastiness. The next step will be to see if the model
can predict the tastiness of a burrito based on its inputs.
</p><p>
But, how do you create such a model?
</p><p>
In theory, you could create it from scratch, reading the appropriate
statistical literature and implementing it all in code. But because
I'm using Python, and because Python's scikit-learn has been tuned
and improved over several years, there are a variety of model
types to choose from that others already have created.
</p><p>
Before starting with the model building, however, let's get the data
into the necessary format. As I mentioned in my last article and alluded to
above, Python's machine-learning package (scikit-learn) expects that
when training a supervised-learning model, you'll need a set of sample
inputs, traditionally placed in a two-dimensional matrix called X
(yes, uppercase X), and a set of sample outputs, traditionally placed in
a vector called y (lowercase). You can get there as follows, inside 
the Jupyter notebook:

<pre     class="programlisting">
%pylab inline
import pandas as pd                     # load pandas with an alias
from pandas import Series, DataFrame    # load useful Pandas classes
df = pd.read_csv('burrito.csv')         # read into a data frame
</pre>
</p><p>
Once you have loaded the CSV file containing burrito data, you'll keep
only those columns that contain the features of interest, as well as
the output score:

<pre     class="programlisting">
burrito_data = df[range(11,24)]
</pre>
</p><p>
You'll then remove the columns that are highly correlated to one
another and/or for which a great deal of data is missing. In this
case, it means removing all of the features having to do with
burrito size:

<pre     class="programlisting">
burrito_data.drop(['Circum', 'Volume', 'Length'], axis=1, 
 &#8618;inplace=True)
</pre>
</p><p>
Let's also drop any of the samples (that is, rows) in which one or more
values is NaN (&ldquo;not a number&rdquo;), which will throw off the values:

<pre     class="programlisting">
burrito_data.dropna(inplace=True, axis=0)
</pre>
</p><p>
Once you've done this, the data frame is ready to be used in a
model. Separate out the X and y values:

<pre     class="programlisting">
y = burrito_data['overall']
X = burrito_data.drop(['overall'], axis=1)
</pre>
</p><p>
The goal is now to create a model that describes, as best as possible,
the way the values in X lead to a value in y. In other
words, if you look at <tt  >X.iloc[0]</tt> (that is, the input values for the first
burrito sample) and at <tt  >y.iloc[0]</tt> (that is, the output value for the
first burrito sample), it should be possible to understand how
those inputs map to those outputs. Moreover, after training the
computer with the data, the computer should be able to predict the
overall score of a burrito, given those same inputs.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x14a7580.0x15a0010"></a>
Creating a Model</h2></div></div><p>
Now that the data is in order, you can build a model. But which
algorithm (sometimes known as a &ldquo;classifier&rdquo;) should you use for the
model? This is, in many ways, the big question in machine learning,
and is often answerable only via a combination of experience and trial
and error. The more machine-learning problems you work to solve, the
more of a feel you'll get for the types of models you can
try. However, there's always the chance that you'll be wrong, which
is why it's often worth creating several different types of models,
comparing them against one another for validity. I plan to talk more
about validity testing in my next article; for now, it's important to
understand how to build a model.
</p><p>
Different algorithms are meant for different kinds of machine-learning
problems. In this case, the input data already has been ranked,
meaning that you can use a supervised learning model. The output from
the model is a numeric score that ranges from 0 to 5, which means that
you'll have to use a numeric model, rather than a categorical one.
</p><p>
The difference is that a categorical model's outputs will (as the name
implies) indicate into which of several categories, identified by
integers, the input should be placed. For example, modern political
parties hire data scientists who try to determine which way someone
will vote based on input data. The result, namely a political party,
is categorical.
</p><p>
In this case, however, you have numeric data. In this kind of model, you
expect the output to vary along a numeric range. A pricing model,
determining how much someone might be willing to pay for a particular
item or how much to charge for an advertisement, will use this sort
of model.
</p><p>
I should note that if you want, you can turn the numeric data into
categorical data simply by rounding or truncating the floating-point y
values, such that you get integer values. It is this sort of
transformation that you'll likely need to consider&mdash;and try, and
test&mdash;in a machine-learning project. And, it's this myriad of
choices and options that can lead to a data-science project being
involved, and to incorporate your experience and insights, as well as
brute-force tests of a variety of possible models.
</p><p>
Let's assume you're going to keep the data as it is. You cannot use
a purely categorical model, but rather will need to use one that
incorporates the statistical concept of &ldquo;regression&rdquo;, in which you
attempt to determine which of your input factors cause the output to
correlate linearly with the outputs&mdash;that is, assume that the
ideal is something like the &ldquo;y = qX&rdquo; that you saw above; given that
this isn't the case, how much influence did meat quality have vs. uniformity vs. temperature? Each of those factors affected the
overall quality in some way, but some of them had more influence than
others.
</p><p>
One of the easiest to understand, and most popular, types of models
uses the K Network Neighbors (KNN) algorithm. KNN basically says that
you'll take a new piece of data and compare its features with those of
existing, known, categorized data. The new data is then classified
into the same category as its K closest neighbors, where K is a number
that you must determine, often via trial and error.
</p><p>
However, KNN works only for categories; this example is dealing with a
regression problem, which can't use KNN. Except, Python's
scikit-learn happens to come with a version of KNN that is designed to
work with regression problems&mdash;the <tt  >KNeighborsRegressor</tt> classifier.
</p><p>
So, how do you use it? Here's the basic way in which all supervised
learning happens in scikit-learn:
</p><div class="orderedlist"><ol type="1"><li><p>
Import the Python class that implements the classifier.
</p></li><li><p>
Create a model&mdash;that is, an instance of the classifier.
</p></li><li><p>
Train the model using the &ldquo;fit&rdquo; method.
</p></li><li><p>
Feed data to the model and get a prediction.
</p></li></ol></div><p>
Let's try this with the data. You already have an X and a y, which you
can plug in to the standard <tt  >sklearn</tt> pattern:

<pre     class="programlisting">
from sklearn.neighbors import KNeighborsRegressor   # import classifier
KNR = KNeighborsRegressor()                         # create a model
KNR.fit(X, y)                                       # train the model
</pre>
</p><p>
Without the <tt  >dropna</tt> above (in which I removed any rows containing one or more
NaN values), you still would have &ldquo;dirty&rdquo; data, and
sklearn would be unable to proceed. Some classifiers can handle NaN
data, but as a general rule, you'll need to get rid of
NaN values&mdash;either to satisfy the classifier's rules, or to ensure that
your results are of high quality, or even (in some cases) valid.
</p><p>
With the trained model in place, you now can ask it: &ldquo;If you have a
burrito with really great ingredients, how highly will it rank?&rdquo;
</p><p>
All you have to do is create a new, fake sample burrito with all
high-quality ingredients:

<pre     class="programlisting">
great_ingredients = np.ones(X.iloc[0].count()) * 5
</pre>
</p><p>
In the above line of code, I took the first sample from X (that is,
<tt  >X.iloc[0]</tt>), and then counted how many items it contained. I then
multiplied the resulting NumPy array by 5, so that it contained all
5s. I now can ask the model to predict the overall quality of such a
burrito:

<pre     class="programlisting">
KNR.predict([great_ingredients])
</pre>
</p><p>
I get back a result of:

<pre     class="programlisting">
array([ 4.86])
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x14a7580.0x19990d0"></a></h2></div></div><p>
meaning that the burrito would indeed score high&mdash;not a 5, but high
nonetheless. What if you create a burrito with absolutely awful
ingredients? Let's find the predicted quality:

<pre     class="programlisting">
terrible_ingredients = np.zeros(X.iloc[0].count())
</pre>
</p><p>
In the above line of code, I created a NumPy array containing zeros,
the same length as the X's list of features. If you now ask the model
to predict the score of this burrito, you get:

<pre     class="programlisting">
array([ 1.96])
</pre>
</p><p>
The good news is that you have now trained the computer to predict
the quality of a burrito from a set of rated ingredients. The other
good news is that you can determine which ingredients are more
influential and which are less influential.
</p><p>
At the same time, there is a problem: how do you know that KNN
regression is the best model you could use? And when I say &ldquo;best&rdquo;, I
ask whether it's the most accurate at predicting burrito quality. For
example, maybe a different classifier will have a higher spread or
will describe the burritos more accurately.
</p><p>
It's also possible that the classifier is a good one, but that one of
its parameters&mdash;parameters that you can use to &ldquo;tune&rdquo; the model&mdash;wasn't
set correctly. And I suspect that you indeed could do better,
since the best burrito actually sampled got a score of 5, and the
worst burrito had a score of 1.5. This means that the model is not a
bad start, but that it doesn't quite handle the entire range that one
would have expected.
</p><p>
One possible solution to this problem is to adjust the parameters that
you hand the classifier when creating the model. In the case of any
KNN-related model, one of the first parameters you can try to tune is
<tt  >n_neighbors</tt>. By default, it's set to 5, but what if you set it to
higher or to lower?
</p><p>
A bit of Python code can establish this for you:

<pre     class="programlisting">
for k in range(1,10):
    print(k)
    KNR = KNeighborsRegressor(n_neighbors=k)
    KNR.fit(X, y)
    print("\tTerrible: {0}".format(KNR.predict([terrible_ingredients])))
    print("\tBest: {0}".format(KNR.predict([great_ingredients])))
</pre>
</p><p>
After running the above code, it seems like the model that has the
highest high and the lowest low is the one in which <tt  >n_neighbors</tt> is
equal to 1. It's not quite what I would have expected, but that's why it's
important to try different models.
</p><p>
And yet, this way of checking to see which value of <tt  >n_neighbors</tt> is
the best is rather primitive and has lots of issues. In my next article, I plan to 
look into checking the models, using more sophisticated
techniques than I used here.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x14a7580.0x1999758"></a>
Using Another Classifier</h2></div></div><p>
So far, I've described how you can create multiple models from a single
classifier, but scikit-learn comes with numerous classifiers, and
it's usually a good idea to try several.
</p><p>
So in this case, let's also try a simple regression model. Whereas KNN
uses existing, known data points in order to decide what outputs to
predict based on new inputs, regression uses good old statistical
techniques. Thus, you can use it as follows:

<pre     class="programlisting">
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X, y)
print("\tTerrible: {0}".format(KNR.predict([terrible_ingredients])))
print("\tBest: {0}".format(KNR.predict([great_ingredients])))
</pre>
</p><p>
Once again, I want to stress that just because you don't cover the
entire spread of output values, from best to worst, you can't discount
this model. And, a model that works with some data sets often will not
work with other data sets.
</p><p>
But as you can see, scikit-learn makes it easy&mdash;almost trivially
easy, in fact&mdash;to create and experiment with different models. You
can, thus, try different classifiers, and types of classifiers, in order
to create a model that describes your data.
</p><p>
Now that you've created several models, the big question is which one
is the best? Which one not only describes the data, but also does so well?
Which one will give the most predictive power moving forward, as you
encounter an ever-growing number of burritos? What ingredients
should a burrito-maker stress in order to maximize eater satisfaction,
while minimizing costs?
</p><p>
In order to answer these questions, you'll need to have a way of
testing your models. In my next article, I'll look at how to test your models,
using a variety of techniques to check the validity of a model and
even compare numerous classifier types against one another.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x14a7580.0x1999ac8"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
I used Python (<a href="http://python.org" target="_self">python.org</a>) and the many parts of the SciPy
stack (NumPy, SciPy, Pandas, matplotlib and scikit-learn) in this
article. All are available from PyPI (<a href="http://PyPI.python.org" target="_self">PyPI.python.org</a>) or from
SciPy.org (<a href="http://scipy.org" target="_self">scipy.org</a>).
</p><p>
I recommend a number of resources for people interested in data
science and machine learning.
One long-standing weekly e-mail list is &ldquo;KDNuggets&rdquo; at
<a href="http://kdnuggets.com" target="_self">kdnuggets.com</a>. You also should consider the &ldquo;Data Science
Weekly&rdquo; newsletter (<a href="http://datascienceweekly.com" target="_self">datascienceweekly.com</a>) and &ldquo;This Week in
Data&rdquo; 
(<a href="https://datarepublicblog.com/category/this-week-in-data" target="_self">https://datarepublicblog.com/category/this-week-in-data</a>),
describing the latest data sets available to the public.
</p><p>
I am a big fan of podcasts and particularly love &ldquo;Partially
Derivative&rdquo;. Other good ones are &ldquo;Data Stories&rdquo; and &ldquo;Linear
Digressions&rdquo;. I listen to all three on a regular basis and learn
from them all.
</p><p>
If you're looking to get into data science and machine learning, I
recommend Kevin Markham's &ldquo;Data School&rdquo; (<a href="http://dataschool.org" target="_self">dataschool.org</a>) and
Jason Brownlie's &ldquo;Machine Learning Mastery&rdquo;
(<a href="http://machinelearningmastery.com" target="_self">machinelearningmastery.com</a>), where he sells a number of short, dense,
but high-quality ebooks on these subjects.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x14a7580.0x199a360"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Reuven M. Lerner offers training in Python, Git and PostgreSQL to
companies around the world. He blogs at <a href="http://blog.lerner.co.il" target="_self">blog.lerner.co.il</a>, tweets
at @reuvenmlerner and curates <a href="http://DailyTechVideo.com" target="_self">DailyTechVideo.com</a>.
Reuven lives in
Modi'in, Israel, with his wife and three children.

</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../272/toc272.html">Issue Table of Contents</a>
    <a class="link3" href="../272/12112.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>