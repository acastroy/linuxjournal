<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Speech I/O for Embedded Applications 
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Is the world ready for speech-enabled embedded devices? Now the technology is&#10;here for usable speech recognition and synthesis. See&#10;how you can use it in your own embedded applications.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x1225580.0x131cac0"></a>
Speech I/O for Embedded Applications 
</h1></div><div><div class="author"><h3 class="author">
Rick
 
Rogers
</h3></div><div class="issuemoyr">Issue #206, June 2011</div></div><div><p>
Is the world ready for speech-enabled embedded devices? Now the technology is
here for usable speech recognition and synthesis. See
how you can use it in your own embedded applications.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x131d2a8"></a></h2></div></div><p>
Speech user interfaces are like the holy grail for computing. We talk to each
other to communicate, and sci-fi stories&mdash;from HAL in <span   class="emphasis"><em>2001: A
Space Odyssey</em></span>
to the ship's computer in <span   class="emphasis"><em>Star Trek</em></span>&mdash;point to talking computers as the
inevitable future. But, creating speech interfaces that are natural
and that people will use has proven to be difficult. Too often speech
technology is provided, or even preinstalled (as with Microsoft Windows
Speech Recognition), and never used, but there are glimmers of hope. The
technology to do &ldquo;decent&rdquo; speech recognition and speech
synthesis has existed for a while now, and users are trying it out,
at least in some application categories.
</p><p>
It feels like the opportunity is ripe for someone to get the speech
interface right. Maybe you're the one to invent a speech interface that
makes your embedded application as cool and unique as the iPhone touch
interface was when it first came out.
</p><p>
In some ways, embedded applications are particularly well suited for
speech. An embedded device often is physically small and may not have
a rich user interface. Almost by definition, embedded applications are
not general-purpose, so it's okay if a speech interface has a limited
vocabulary. Speech may be the only user interface provided, or it may
augment a display and keyboard.
</p><p>
Mobile phones are one class of embedded applications where speech works
as a user interface. Voice dialing (&ldquo;dial home&rdquo;) is almost a
trivial interface that works very well on phones. If you're driving
and want to send a text message, it's difficult (and in many places
illegal) to use the phone's soft keyboard to enter the message and its
destination. Speech recognition is good enough, and mobile phones are
powerful enough computers, that sending text messages by voice is a
valid use case people are starting to employ.
</p><p>
In this article, I examine technologies for speech synthesis and
recognition and see how they fit with today's embedded devices. As
an example application, and in step with the re-discovery of checklists
as productivity tools (thanks to Atul Gawande's best-seller <span   class="emphasis"><em>The
Checklist Manifesto</em></span>), we'll build a simple vocal
checklist that you can use the next time you do surgery, like Dr Gawande
(kids don't try this at home).
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x131d7d0"></a>
Speech Technologies</h2></div></div><p>
As with any other user interface, a speech interface has two components:
input and output (or recognition and synthesis). The two
technologies are closely related, sharing techniques, algorithms
and data models. As mentioned, speech has been a very popular computing
research topic, and I can't cover all the work here, but I take
a quick look at some different approaches, investigate some open-source
implementations and settle on input and output packages that seem well
suited for embedded applications. You don't have to be a computerized
speech expert (I certainly don't claim to be) to speech-enable your
embedded application.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x131d8d8"></a>
Speech Synthesis or Text-to-Speech (TTS)</h2></div></div><p>
Na&iuml;vely, you might think &ldquo;What's so hard about speech
synthesis?&rdquo; You
envision a hashmap with English words as the keys and speech utterances
as the values. But, it's not that easy. Any nontrivial TTS system needs
to be able to understand things like dates and numbers that are embedded
in the text and utter them properly. And, as any first-grader can tell
you, English is full of words whose pronunciation is context-dependent
(should &ldquo;lead&rdquo; be pronounced as rhyming with &ldquo;reed&rdquo; or
&ldquo;red&rdquo;?). We also vary the pitch of our
voices as we come to the end of a sentence or question, and we pause between
clauses and sentences (called the prosody of the speech).
</p><p>
A lot of smart people have thought this over and have come up with a basic
architecture for TTS:
</p><div class="orderedlist"><ol type="1"><li><p>
A front end to analyze the text, replace dates, numbers and abbreviations
with words, and emit a stream of phonemes and prosodic units that describes
the utterance.
</p></li><li><p>
A back end, or synthesizer, that takes the utterance stream and converts it
to sounds.
</p></li></ol></div><p>
The front end, sometimes called text normalization, is not an easy
problem. It's one of those pattern things that humans do easily and
computers have a difficult time mimicking. The algorithms used vary
from simple (word substitution) to complex (statistical hidden Markov
models). For applications where the text to be spoken is relatively
fixed (like our checklist), most TTS systems provide a way of marking
up the text to give the normalizer hints about how it should be spoken
(and, there is a standard Speech Synthesis Markup Language to do so;
see Resources).
</p><p>
A variety of schemes have been developed to build speech synthesizers. The
two most popular seem to be formant synthesis and concatenation.
</p><p>
Formant synthesizers can be quite small, because they don't actually store
any digitized voice. Instead, they model speech with a set of rules and
store time-based parameters for models of each phoneme. The prosodic
aspects of speech are relatively easy to introduce into the models, so
formant synthesizers are noted for their ability to mimic emotions. They
also are noted for sounding &ldquo;robotic&rdquo;, but very
intelligible. For our chosen application, intelligibility is more important
than &ldquo;naturalness&rdquo;.
</p><p>
Concatenative synthesizers have a database of speech snippets that are
strung together to create the final sound stream. The snippets can be
anything from a single phoneme to a complete sentence. They are known
for natural-sounding speech, although the technique can produce speech
with distracting glitches, which can interfere with intelligibility,
particularly at higher speeds. They also are typically larger than
formant synthesizers, due to the large database required for a large
vocabulary. The database can be minimized if the TTS is for a
domain-specific application, but, of course, that limits its usefulness.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x131dfb8"></a>
Speech Recognition</h2></div></div><p>
In contrast to TTS, there is one dominant algorithm for speech
recognition, hidden Markov models (HMMs). If you haven't run into HMMs
before, don't expect me to explain the math in detail here, because,
frankly, I don't completely understand it. I do understand the idea
behind HMMs, and that's more than what you need to know to use an HMM-based recognizer.
</p><p>
If you sample a speech waveform (say every 10ms) and do some fancy
math on the resulting waveform that extracts frequency and amplitude
components, you can end up with a vector of cepstral coefficients. You
then can model connected speech as a series of these cepstral vectors. A
Markov model is like a state machine where the probability of a particular
state transition is dependent only on the current state. In our case,
each state of the Markov model corresponds to a particular vector,
and as a Markov model moves probabilistically through its states, a
series of cepstral vectors and sounds are generated. A hidden Markov
model is one where you can't see the details of the state transitions,
you just see the output vectors.
</p><p>
The trick is to create a bunch of these HMMs, each trained to mimic the
sounds from a bunch of human-generated speech samples. Again, the math
is beyond me here, but the process is to expose a training algorithm to
a <span   class="emphasis"><em>lot</em></span> of speech samples for the language desired. As the 
sea of HMMs is
trained, they take on the ability to reproduce the sounds they
&ldquo;hear&rdquo;
in the training samples.
</p><p>
To use the HMMs to recognize speech, we use one last bit of mathematical
wizardry. For appropriate sets of HMMs, there are algorithms that, given
a waveform (that is hopefully speech), can tell you: 1)
which sequences of HMMs might have generated that waveform and 2)
the probability for each of those sequences.
</p><p>
So, HMMs won't give us a definitive answer of what words the speech
represents, but they'll give us a list from which to choose and tell us which
is most likely and by how much. How cool is that?
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x131e2d0"></a>
Open-Source TTS</h2></div></div><p>
Many commercial TTS packages are available, but they don't concern
us here. On the open-source side, there are still many candidates, with
a few that seem more popular:
</p><div class="itemizedlist"><ul type="disc"><li><p>
eSpeak is the TTS package that comes with Ubuntu and several other Linux
distros. It is of the formant flavor and, therefore, small (~1.4MB),
with the usual robotic formant voice. The eSpeak normalizer also can be
used with a diphone synthesizer (MBROLA) if desired, but we won't take
advantage of that for the checklist example here.
</p></li><li><p>
Flite is the embedded version of Festival, which is an open-source speech
synthesis package originating at University of Edinburgh, with Flite
done at Carnegie Mellon University. It is diphone-based concatenative,
and as you would expect, it has a more natural voice. CMU also offers a set
of scripts and tools for developing new voices, called FestVox.
</p></li></ul></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x131e590"></a>
Open-Source Speech Recognition</h2></div></div><p>
Most speech recognition packages are commercial software for Windows
and Mac OS X. I looked at two open-source speech recognition packages, both from the
Sphinx group at Carnegie Mellon:
</p><div class="itemizedlist"><ul type="disc"><li><p>
Sphinx-4 is a speech recognizer and framework that can use multiple
recognition approaches, written in Java. It is intended primarily for
server applications and for research.
</p></li><li><p>
PocketSphinx is a speech recognizer derived from Sphinx and written in
C. As such, it is much smaller than Sphinx (but still around 20MB for
a moderate vocabulary), and it runs in real time on small processors,
even those with no floating-point hardware.
</p></li></ul></div><p>
PocketSphinx is the obvious choice between the two implementations,
so that's what we'll use here.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x131e8a8"></a>
Atul&mdash;a Speech Checklist Application</h2></div></div><p>
In the interest of flexibility and speed, I've chosen a rather high-end
embedded platform for the example program. The Genesi LX is a nettop
with a rather generous configuration for an embedded device:
</p><div class="orderedlist"><ol type="1"><li><p>
Freescale i.MX515 (ARM Cortex-A8 800MHz).
</p></li><li><p>
3-D graphics processing unit.
</p></li><li><p>
WXGA display support (HDMI).
</p></li><li><p>
Multi-format HD video decoder and D1 video encoder.
</p></li><li><p>
512MB of RAM. 
</p></li><li><p>
8GB internal SSD.
</p></li><li><p>
10/100Mbit/s Ethernet.
</p></li><li><p>
802.11 b/g/n Wi-Fi. 
</p></li><li><p>
SDHC card reader.
</p></li><li><p>
2x USB 2.0 ports. 
</p></li><li><p>
Audio jacks for headset.
</p></li><li><p>
Built-in speaker. 
</p></li></ol></div><div       class="mediaobject"><img src="10988f1.jpg"><div class="caption"><p>
Figure 1. Genesi EFIKA MX Smarttop
</p></div></div><p>
On top of all that, there is a version of the Ubuntu 10.10 (Meerkat)
distro available to load and run on the system, which makes installation
and testing a lot easier. The download for Ubuntu and installation
instructions are on the Genesi Web site. Installation from an SD card is
straightforward through the U-boot bootloader.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x17176a8"></a>
Installing eSpeak</h2></div></div><p>
The eSpeak TTS system originally was developed for the Acorn RISC Machine
(can you say &ldquo;full circle&rdquo;?), comes with Ubuntu and is included in
the version for Genesi, so we get a pass there. That may not be true
for your embedded system, but the installation procedure for eSpeak is
straightforward and is given in the README of the download on the eSpeak
site (see Resources). Of course, you'll need to do the install in the
context of Scratchbox, or whatever native build environment you're using
for your embedded Linux.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x1717808"></a>
Installing PocketSphinx</h2></div></div><p>
To install PocketSphinx, you first need to install sphinxbase, which
also is available on the PocketSphinx site. Both are tarballs, and the
installation instructions are given in the READMEs. On systems like the Genesi,
you can download and use the target to build the package. I did have
to set LD_LIBRARY_PATH, so <tt  >ld</tt> could find the libraries:

<pre     class="programlisting">
export LD_LIBRARY_PATH=/usr/local/lib
</pre>
</p><p>
On smaller embedded systems, you might have to use a cross-compiler
or Scratchbox.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x1717a18"></a>
The Atul Program</h2></div></div><p>
We want to create a general-purpose spoken checklist program along
the lines of the checklists discussed in Dr Gawande's book. As an
example, let's use part of the World Health Organization's Surgical
Safety Checklist.
</p><div       class="mediaobject"><img src="10988f2.jpg"><div class="caption"><p>
Figure 2. Surgical Safety Checklist, Part I
</p></div></div><p>
Let's create a speech checklist program that reads a checklist and
listens to a reply for each item. We'll just match the reply with some
valid ones now and record it in a file, but this could be a springboard
for your own innovative speech user-interface ideas.
</p><p>
PocketSphinx comes with an application called pocketsphinx_continuous
that will do basic continuous speech recognition and print the results
to stdout, along with a lot of information about how it performed the
recognition. We'll create a small C program, atul.c, that uses the
libespeak library to speak the checklist items. We will have piped
pocketsphinx_continuous to atul, so atul can listen to the replies on
its stdin.
</p><p>
The compilation command for atul will vary depending on your development
environment. The invocation is:

<pre     class="programlisting">
pocketsphinx_continuous | ./atul SafeSurgery.ckl
</pre>
</p><p>
Let's keep the application simple by reading checklist items and
commands from a text file, whose name we'll pass as an argument to
the program. Let's mark commands with a # at the beginning of a
line. If the # is followed by a number, let's pause that number of seconds
(up to 9). We will record each item and the replies as text to stdout.
</p><p>
The espeak library depends on two development packages you'll need to
load into your target development environment. Both are readily available
as rpm or deb packages:
<tt  >portaudio-devel</tt> and
<tt  >espeak-devel</tt>.
</p><p>
The Safe Surgery Checklist file is shown in Listing 1, and
Listing 2 shows the source code for atul.c.

</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x17180a0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 1. SafeSurgery.ckl</b></p><pre     class="programlisting">
This is the Surgical Safety Checklist.
#
Before induction of anesthesia.
#
Has the patient confirmed his or her identity, site, 
procedure and consent?
# yes | no
Is the site marked?
# yes | notapplicable
Is the anesthesia machine and medication check complete?
# yes
Is the pulse oximeter on the patient and functioning?
# yes
Does the patient have a known allergy?
# yes | no
Does the patient have a difficult airway or aspiration risk?
# no | yes
Is there a risk of more than 500 milliliters of blood loss?
# no | yes
Thank you, that completes this portion of the checklist.
</pre></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x17182b0"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Listing 2. atul.c</b></p><pre     class="programlisting">

/*
   atul - a simple speech checklist for embedded systems
*/
#include &lt;string.h&gt;
#include &lt;malloc.h&gt;
#include &lt;stdio.h&gt;
#include &lt;speak_lib.h&gt;



espeak_POSITION_TYPE position_type;
espeak_AUDIO_OUTPUT output;
char *path=NULL;
int BuffLen=500, Options=0;
void* user_data;
t_espeak_callback *SynthCallback;
espeak_PARAMETER Parm;
FILE *ckfp;     /* Checklist file pointer */
char *ckBuf; /* Checklist item buffer */
char *mtchBuf; /* Checklist expected response buffer */
char *srBuf; /* Speech rec buffer */
char *reply; /* Trimmed reply */
int bsize=100; /* buffer length for all buffers */
int next;  /* flag - true if should go to next prompt */

char Voice[] = {"default"};
unsigned int size, position=0, end_position=0,
flags=espeakCHARS_AUTO|espeakENDPAUSE, *unique_identifier;

void recordreply(){
   /* read lines from stdin, which are piped in
    * from pocketsphinx_continuous.
    * Valid responses look like:
    * &lt;9 digits&gt;: reply (7 or 8 digits)
    * Returns a trimmed reply as char *reply
    * no spaces in return
    */
    int i, j;
    while (!feof(stdin)) {
       getline (&amp;srBuf, &amp;bsize, stdin);
       if (srBuf[9]!= ':') continue;
       j=0;
       for (i=0; i&lt;strlen(srBuf); i++) {
          if (isdigit(srBuf[i])) continue;
          if (srBuf[i]=='-') continue;
          if (srBuf[i]==':') continue;
          if (isspace(srBuf[i])) continue;
          if (srBuf[i]=='(') continue;
          if (srBuf[i]==')') continue;
          reply[j++] = srBuf[i];
       }
       reply[j] = '\0';
       break;
    }
}

int checkreply(){
    /* returns true if reply matches
    *         false if no match (try again)
    */
   char *tryagain="Please answer ";
   char *ans, *spBuf;

   /* if template blank, just sleep 2 sec */
   if (strlen(mtchBuf)==2) {
      sleep(2);
      return(1);
   }
   /* see if reply matches template */
   recordreply();
   printf("reply: '%s'\n", reply);
   if (strstr(mtchBuf, reply)==NULL){
      /* no match - tell user what we're looking for */
      spBuf = (char *) malloc (bsize+1);
      strcpy(spBuf, tryagain);
      if (ans=strtok(mtchBuf+2, "|")){
         strcat(spBuf, ans);
      }
      ans = strtok(NULL, "|");
      while (ans!=NULL){
         strcat(spBuf, " or ");
         strcat(spBuf, ans);
         ans = strtok(NULL, "|");
      }
      espeak_Synth( spBuf, size, position,
         position_type, end_position, flags,
         unique_identifier, user_data );
      espeak_Synchronize( );
      free(spBuf);
      return(0); /* repeat last prompt */
   }else return(1); /* go to next prompt */
}

int main(int argc, char* argv[] )
{
    printf("atul started.\n");

    /* allocate needed buffers */
    ckBuf = (char *) malloc (bsize+1);
    srBuf = (char *) malloc (bsize+1);
    mtchBuf = (char *) malloc (bsize+1);
    reply = (char *) malloc (bsize+1);

    /* open the checklist file */
    if (argc &lt; 2) {
        printf("Usage: atul &lt;checklist filename&gt;\n",
               argc);
        return 0;
    }
    ckfp = fopen(argv[1], "r");
    if (ckfp == NULL) {
        printf("Unable to open checklist file: %s\n",
               argv[1]);
        return 0;
    }

    /* Initialize the TTS subsystem */
    output = AUDIO_OUTPUT_PLAYBACK;
    espeak_Initialize(output, BuffLen, path, Options);
    espeak_SetVoiceByName(Voice);

    /* Initialize speech recognition
     *  piped in from pocketsphinx_continuous */
    while (!feof(stdin)) {
       getline (&amp;srBuf, &amp;bsize, stdin);
       if (strncmp(srBuf, "READY...", 8)==0) break;
    }

    /* Go through the checklist */
    next = 1;  /* advance to next prompt */
    while (!feof(ckfp)) {
       if (next) {
          getline (&amp;ckBuf, &amp;bsize, ckfp);
          getline (&amp;mtchBuf, &amp;bsize, ckfp);
       }
       size = strlen(ckBuf)+1;
       espeak_Synth( ckBuf, size, position,
          position_type, end_position, flags,
          unique_identifier, user_data );
          espeak_Synchronize( );
       fputs(ckBuf, stdout);
       next = checkreply();
    }
    fclose(ckfp);

    free(ckBuf);
    free(srBuf);
    free(mtchBuf);
    return 0;
}

</pre></div><p>
The code isn't very complex, although in retrospect, it might have been
clearer in Python or some other language that is better than C at string
manipulation. The main routine initializes the TTS subsystem and makes
sure that phoenix_continuous is ready to catch replies and forward them to
us. It then just cycles through the checklist file, reading prompts and
comparing the replies with the acceptable ones it finds in the checklist
file. If it doesn't find a match, it tells the user what it's looking
for and asks again. One thing to note, the string trimming routine in
<tt  >recordreply()</tt> throws out all spaces, so if your checklist is looking
for a multiword response, be sure to concatenate the words in the list
(like &ldquo;notapplicable&rdquo; in our checklist above). Everything of note
is recorded in stdout, which you might want to redirect to a log file.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x17185c8"></a>
From Here</h2></div></div><p>
We've barely scratched the surface of speech user interfaces, even
for a checklist application. Depending on your embedded system,
you'd have to give the user a way to start and end the checklist app,
and ideally you'd have a way of signaling to the user when the app is
listening. PocketSphinx prompts with &ldquo;Listening...&rdquo; and is supposed
to terminate on saying &ldquo;goodbye&rdquo; (that doesn't work for
me&mdash;maybe it's my Texas accent?). The source code for PocketSphinx (labeled
continuous.c) comes with the package, so you can experiment with it. There
are many, many optimizations you could make to both speech recognition
and synthesis, using restricted vocabularies, different voice databases
and just tuning the parameters.
</p><p>
And, what about a more general, practical speech user interface for
embedded devices? The tools are available&mdash;how creative can you be?
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x1225580.0x17187d8"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
eSpeak: <a href="http://espeak.sourceforge.net/index.html" target="_self">espeak.sourceforge.net/index.html</a>
</p><p>
Carnegie Mellon University's Sphinx Group:
<a href="http://cmusphinx.sourceforge.net" target="_self">cmusphinx.sourceforge.net</a>
</p><p>
Carnegie Mellon's Flite Page: <a href="http://www.speech.cs.cmu.edu/flite" target="_self">www.speech.cs.cmu.edu/flite</a>
</p><p>
Genesi EFIKA MX: <a href="http://www.genesi-usa.com/products/efika" target="_self">www.genesi-usa.com/products/efika</a>
</p><p>
World Health Organization Safe Surgery Checklist:
<a href="http://www.who.int/patientsafety/safesurgery/ss_checklist/en/index.html" target="_self">www.who.int/patientsafety/safesurgery/ss_checklist/en/index.html</a>
</p></div></div></div>
<div class="authorblurb"><p>
Rick Rogers has been a professional embedded developer for more than 30 years.
Now specializing in mobile application software, when Rick isn't
writing software for a living, he's writing books and magazine articles like
this one. He welcomes feedback on this article at
<a href="mailto:portmobileapps@gmail.com">portmobileapps@gmail.com</a>.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../206/toc206.html">Issue Table of Contents</a>
    <a class="link3" href="../206/10988.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>