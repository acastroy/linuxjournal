<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Using Perl to Check Web Links</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    Do you have many links on your web pages? If so, you're probably&#10;    finding that the pages at the ends of those links are disappearing&#10;    faster than&#10;    you can track them. Ah, but now you can get the computer to handle that&#10;    for you.&#10;    "><meta name="keywords" content="Perl, WWW, LWP, library, communication"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x19d6580.0x1acdab0"></a>Using Perl to Check Web Links</h1></div><div><div class="author"><h3 class="author">Jim Weirich</h3></div><div class="issuemoyr">Issue #36, April 1997</div></div><div><p>
    Do you have many links on your web pages? If so, you're probably
    finding that the pages at the ends of those links are disappearing
    faster than
    you can track them. Ah, but now you can get the computer to handle that
    for you.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x19d6580.0x1ace450"></a></h2></div></div><p>One of the first things I did when I got
my first Internet account was put together my own set of web pages.
The one I get the most comments about is called &ldquo;Weirichs on the
Web&rdquo; where I link to other Weirichs I have found on the Web.
Although a lot of fun, keeping the links up to date can be very
tedious. As web pages that I reference are moved or deleted, links
to them become stale. Without constant checking, it is difficult to
keep my links current.
</p><p>So, I began to wonder, is there a way to automatically find
the outdated links in a web page? What I needed was a script that
would scan all of my web pages and report every bad HTML link along
with the web page on which it was used.</p><p>There are several parts to this problem. Our script must be
able to:</p><div class="itemizedlist"><ul type="disc"><li><p>fetch a web document from the Web</p></li><li><p>extract a list of URLs from a web document</p></li><li><p>test a URL to see if it is valid</p></li></ul></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x19d6580.0x1ace870"></a>The LWP Library</h2></div></div><p>We could write code by hand to extract URLs and validate
them, but there is a much easier way. LWP is a Perl library
(available from any CPAN archive site) designed to make accessing
the World Wide Web very easy in Perl. LWP uses Perl objects to
provide Web-related services to a client. Perl objects are a recent
addition to the Perl language and many people might not be familiar
with them.</p><p>Perl objects are references to &ldquo;things&rdquo; that know what
class they belong to. These &ldquo;things&rdquo; are usually anonymous hashes
but <span   class="emphasis"><em>you don't need to know this to use an
object.</em></span> Classes are packages that provide the methods
the object uses to implement its behavior. And finally, a method is
a function (in the class package) that expects an object reference
(or sometimes a package name) as its first argument.</p><p>If this sounds confusing, don't worry. Using objects is very
easy. LWP defines a class called <b  >HTTP::Request</b>
that represents a request to be sent on the Web. The request to
<b  ><i><tt>GET</tt></i></b> a document at URL
http://w3.one.net/~jweirich can be created with the
statement:</p><pre     class="programlisting">
$req = new HTTP::Request GET,
 'http://w3.one.net/~jweirich';
</pre><p><b  >new</b> creates a new Request object
initialized with the
<b  ><i><tt>GET</tt></i></b> and
http://w3.one.net/~jweirich parameters. This new object is assigned
to the <b  >$req</b> variable.
</p><p>Calling a member function of an object is equally
straightforward. For example, if you want to examine the URL for
this request, you can invoke the <b  >url</b> method on
this object.</p><pre     class="programlisting">
print "The URL of this request is:
", $req-&gt;url, ",\n";
</pre><p>Notice that methods are invoked using the
<b  >-&gt;</b> syntax. C++ programmers should feel
comfortable with this.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x19d6580.0x1acef50"></a>Getting a Document</h2></div></div><p>All the knowledge about fetching a document across the Web is
stored in a UserAgent object. The UserAgent object knows how long
to wait for responses, how to handle errors, and what to do with
the document when it arrives. It does all the hard work&mdash;we just
need to give it the right information so that it can do its
job.</p><pre     class="programlisting">
use LWP::UserAgent;
use HTTP::Request;
$agent = new LWP::UserAgent;
$req = new HTTP::Request ('GET',
 'http://w3.one.net/~jweirich/');
$agent-&gt;request ($req, \&amp;callback);
</pre><p>This snippet of Perl code creates a UserAgent and a Request
object. The Request method of UserAgent issues the request and
calls a subroutine called <b  >callback</b> with a chunk
of data from the arriving document. The <b  >callback</b>
subroutine may be called many times until the complete document has
been received.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x19d6580.0x1acf1b8"></a>Parsing the Document</h2></div></div><p>We could use regular expressions to parse the incoming
document to determine the location of all the links, but when you
begin to consider that HTML tags may be broken across several lines
and all the little variations involved, it becomes a more difficult
task. Fortunately, there is an HTML parsing object available in the
LWP library, called <b  >HTML::LinkExtor</b>, which
extracts all the links from an HTML document.</p><p>The parser is created and then fed pieces of the document
until it reaches the end of the document. Whenever the parser
detects links buried in HTML tags, it calls another callback
subroutine that we provide. Here is an example that extracts and
prints all the links in a document.</p><pre     class="programlisting">
use HTML::LinkExtor
$parser = new HTML::LinkExtor (\&amp;LinkCallback);
$parser-&gt;parse ($chunk);
$parser-&gt;parse ($chunk);
$parser-&gt;parse ($chunk);
$parser-&gt;eof;
sub LinkCallback {
    my ($tag, %links) = @_;
    print join ("\n", values %links), "\n";
}
</pre></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x19d6580.0x1acf3c8"></a>Putting It Together</h2></div></div><p>We now have all the tools we need to build our
<b  >checklinks</b> script. We will define two operations
for URLs. When we scan a URL, we will fetch the document (using a
UserAgent) and scan it for internal HTML links. Every new link we
find will be added to a list of URLs to be checked.</p><p>Next, check a link to see if it points to a valid web
document. We could try retrieving the entire document to see if the
document exists, but the HTTP protocol defines a
<b  ><i><tt>HEAD</tt></i></b> request that
gets only the document's date, length and a few other attributes.
Since a <b  ><i><tt>HEAD</tt></i></b> request
can be much faster than a full
<b  ><i><tt>GET</tt></i></b> for large
documents, and since it tells us what we need to know, we will use
the <b  >head()</b> function of the
<b  >LWP::Simple</b> package to check a URL. If
<b  >head()</b> returns an undefined value, then the
document specified by the URL cannot be fetched and we add the URL
to a list of bad URLs. If <b  >head()</b> returns a list,
the URL is valid and it is added to the list of good URLs. Finally,
if the valid URL points to a page in our local web space and ends
with &ldquo;.html&rdquo; or &ldquo;.htm&rdquo;, we add the URL to a list of URLs to be
scanned.</p><p>The scanning process produces more URLs to be checked.
Checking these URLS produces more URLs that need to be scanned. As
URLs are checked, they are moved to the good or bad list. Since we
restrict scanning to URLs in our local web space, eventually we
will scan all local URLs that are reachable from our starting
document.</p><p>When there are no more URLs to be scanned and all URLs have
been checked, we can print the list of bad URLs and the list of
files that contain them.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x19d6580.0x1acf9a0"></a>Results</h2></div></div><p>The complete code to <b  >checklinks</b> is found
in <a href="2026l1.html" target="_self">Listing 1</a>. You will need Perl 5
to be able to run the <b  >checklinks</b> routine. You
will also need a recent copy of the LWP library. When I installed
LWP, I also had to update the IO and Net modules. You can find
Perl, and the LWP, IO and Net modules at
http://www.perl.com/perl.</p><p>You can invoke <b  >checklinks</b> on a single URL
with the command:</p><pre     class="programlisting">
checklinks url
</pre><p>If you wish to scan all local URLs reachable from the main
URL, add a <b  >-r</b> option.
</p><p>Running checklinks on my home system against my entire set of
web pages took about 13 minutes to complete. Most of that time was
spent waiting for the bad URLs to timeout. It scanned 76 pages,
checked 289 URLs, and found 31 links that were no longer valid. Now
all I have to do is find the time to clean up my web pages!</p></div></div>
<div class="authorblurb"><p>
      <span   class="bold"><b>Jim Weirich</b></span>
      is a software consultant for
      Compuware specializing in Unix and C++. When he is not working on
      his web pages, you can find him playing guitar, playing with his
      kids, or playing with Linux. Comments are welcome at
      jweirich@one.net or visit http://w3.one.net/~jweirich.</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../036/toc036.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>