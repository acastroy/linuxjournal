<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>HPF: Programming Linux Clusters the Easy Way</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;    All about high performance FORTRAN and how it is&#10;    used to write code to run efficiently on parallel computers.&#10;    "><meta name="keywords" content="PC, programming, cluster, network"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x18fd580.0x19f4ab0"></a>HPF: Programming Linux Clusters the Easy Way</h1></div><div><div class="author"><h3 class="author">Mike Delves</h3></div><div class="issuemoyr">Issue #45, January 1998</div></div><div><p>
    All about high performance FORTRAN and how it is
    used to write code to run efficiently on parallel computers.
    </p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18fd580.0x19f53f8"></a></h2></div></div><p>Many programmers use Linux as their
operating system of choice when developing applications software.
Increasingly, they run the application on the same PC when it is
developed. But what do they do if the application becomes too large
or runs too long?
</p><p>The obvious answer is to run it on a cluster of PCs, possibly
linked by Ethernet. To do this, you need to write code for each
processor. In principle, different code for each, but usually there
is common code for all except a selected one or two. The code must
include data passing from processor to processor as required. This
type of coding uses a &ldquo;message-passing paradigm&rdquo; and is in common
use on multiprocessor mainframes and workstation clusters. Standard
message-passing libraries, PVM (parallel virtual machine) and MPI
(message-passing interface), exist to ensure portability.</p><p>Writing message-passing code is much harder than writing
serial code, and it is easy to show why. Most scientific and
engineering programming makes heavy use of one and two dimensional
arrays, so we add two vectors this way:</p><pre     class="programlisting">
DO I = 1,10000
a(i) = b(i) + c(i)
END DO
</pre><p>This is clearly a parallel operation: all the elements of
<b  >b</b> and <b  >c</b> can be added in
parallel. For this to occur on a PC or workstation cluster, or
indeed any distributed-memory parallel system, the elements of
<b  >b</b>, <b  >c</b> and, most likely,
<b  >a</b> must be distributed over the available
processors. The programmer must arrange this in his code by dealing
with bits of each vector on each processor and keeping track of the
bits.
</p><p>A better solution would be for the compiler to do the
arranging for you. Then your code could still refer to the complete
vector or matrix object and thereby be easier to write, understand
and maintain.</p><p>The advantages of compiler-aided parallel programming are
widely accepted. In the scientific/engineering community, these
advantages have led to the development of a standard parallel
language called HPF, High Performance FORTRAN, and of HPF compilers
for a widening range of architectures.</p><p>For good reasons, HPF is deliberately based on the latest,
greatly upgraded, version of FORTRAN: FORTRAN95. Beginning with
FORTRAN90, FORTRAN contains a rich variety of array facilities so
that the loop above becomes the one line:</p><pre     class="programlisting">
a = b + c
</pre><p>which is easier for a compiler, as well as a human, to
understand.
</p><p>Compilers do have difficulty deciding how best to distribute
arrays across the processors; thus, HPF gives the programmer a way
of providing help. Given that help, the compiler can fully automate
the production of a data parallel program from a single
FORTRAN77/90/95 source.</p><p>HPF codes are portable across SIMD, MIMD shared memory and
MIMD Distributed Memory architectures. In particular, you can use
HPF on a Linux PC cluster.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18fd580.0x19f5a80"></a>The Flavour of the Language</h2></div></div><p>Here is a concocted example to demonstrate the facilities
provided by HPF:</p><pre     class="programlisting">
       REAL a(1000), b(1000), c(1000), &amp;
        x(500),y(0:501)
!HPF$ PROCESSORS procs(10)
!HPF$ DISTRIBUTE(BLOCK) ONTO procs :: a, b
!HPF$ DISTRIBUTE(CYCLIC) ONTO procs :: c
!HPF$ ALIGN x(i) WITH y(i+1)
...
       a(:) = b(:)              ! Statement 1
       x(1:500) = y(2:501)      ! Statement 2
       a(:) = c(:)              ! Statement 3
...
</pre><p>The lines starting with <b  >!HPF</b> are HPF
directives; the rest are standard FORTRAN90 and carry out three
array operations. What are the directives doing?
</p><p>The <b  >PROCESSORS</b> directive specifies a
linear arrangement of 10 virtual processors, which are mapped to
the available physical processors in a manner not specified by the
language. You might expect to need at least ten physical
processors, but most HPF compilers will run the code happily on one
or more (up to ten) physical processors. Grids of processors in any
number of dimensions up to seven can be defined. They should match
the problem being solved in some way&mdash;perhaps by helping to
minimize communication costs. The processor directive is optional;
the number of processors to use can be specified at runtime.</p><p>The <b  >DISTRIBUTE</b> directives tell (actually,
recommend to) the compiler how to distribute the elements of the
arrays. Arrays <b  >a</b>, <b  >b</b> are
distributed with blocks of 100 contiguous elements per processor,
while <b  >c</b> is distributed so that, for example,
c(1), c(11), c(21), ... are on processor procs(1) and so on.</p><p>Note that the distribution of the arrays <b  >x</b>
and <b  >y</b> is not specified explicitly, while the way
they are aligned to each other is specified. The
<b  >ALIGN</b> statement causes x(i) and y(i+1) to be
stored on the same processor for all values of i, regardless of the
actual distribution.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18fd580.0x19f6058"></a>How the HPF Directives Work</h2></div></div><p>In Statement 1, the identical distribution of
<b  >a</b> and <b  >b</b> ensures that for all
i, a(i) and b(i) are on the same processor; thus, the compiler does
not generate any message passing.</p><p>In statement 2, there is again no need for message passing.
If the <b  >ALIGN</b> statement had lined up x(i) with
y(i) rather than y(i+1), communication would have been needed for
some values of i.</p><p>Statement 3 looks very much like Statement 1; but the
communication requirements are very different because of the
different distribution of <b  >a</b> and
<b  >c</b>. The array elements a(i) and c(i) are on the
same processor for only 10 of the possible values of i, and hence
for nearly all of the elements; communication of data between
processors is needed. This is an unwise choice of distribution for
<b  >c</b>, if indeed this statement represents the bulk
of the work.</p><p>A good choice of distribution and alignment can greatly help
efficiency, and that is the point of having the directives. It is
much easier to write FORTRAN90 code and embellish it with HPF
directives than to write the equivalent message-passing
code.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18fd580.0x19f6478"></a>A Second Example</h2></div></div><p>In practice, the steps taken in writing an HPF program
are:</p><div class="orderedlist"><ol type="1"><li><p>Write FORTRAN90 code. Your existing FORTRAN77 code
will do in a pinch, but you will get better efficiency by cleaning
it up using the newer FORTRAN high-level constructs; tools exist to
help this conversion.</p></li><li><p>Decide how to configure the processors.</p></li><li><p>Declare one or more templates to act as guides for
distributing arrays.</p></li><li><p>Decide how to distribute and align the arrays onto
the template(s).</p></li></ol></div><p>This process is illustrated in the code shown in
<a href="2432l1.html" target="_self">Listing 1</a>, which represents a
subroutine to solve a set of linear equations. The subroutine is in
standard FORTRAN90 and will run happily through any FORTRAN90
compiler, which will treat the HPF directives as comments. The code
makes good use of the FORTRAN90 array facilities and has been
parallelized by adding just four HPF directives. The resulting HPF
code runs well on a Linux PC cluster, provided the size of the
problem being solved is large enough to warrant the use of
parallelism.</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x18fd580.0x19f6948"></a>Does It Really Work?</h2></div></div><p>HPF makes life easy for the programmer, by leaving nearly
everything to the compiler. So, can the compilers cope? Can you
really get parallel efficiency by using HPF? And, can you get
useful speedups on networked PCs with relatively high latency
communications?</p><p>Of course, no compiler can find parallelism where none
exists; you need to give it the parallelism in the beginning. Given
this, then the answer is yes, current HPF compilers are
surprisingly efficient. On a PC cluster connected by Ethernets, the
message-passing latency using PVM or MPI is typically around 0.6ms;
this translates to &ldquo;use fairly coarse-grain parallelism if you can
and don't expect to use too many PCs.&rdquo;</p><p><a href="2432t1.html" target="_self">Table 1</a> shows some timings
to illustrate what can be achieved. They were taken on a four-PC
Linux P100 cluster with 100Mb Ethernet. &ldquo;Serial&rdquo; times are those
given using the N. A. Software (NASL) FORTRANPlus F90 compiler,
release 1.3.57. These times are absent where the code uses HPF
extensions (<b  >FORALL</b>,
<b  >EXTRINSIC(HPFSERIAL)</b>) not supported in FORTRAN90
(for some, we timed equivalent FORTRAN90 versions). HPF times used
the NASL HPFPlus compiler, release 2.0. Optimization was set &ldquo;on&rdquo;
for both FORTRAN and HPF. Times are in seconds.</p><p>The overheads intrinsic to using HPF rather than FORTRAN are
shown by comparing the Serial and P = 1 times. These overheads are
quite low&mdash;often negligible and, for Gauss, even negative (we see
this on other platforms too). The gain in using HPF is shown by
comparing the Serial and P = 4 times. Speedups achieved relative to
the serial times range from 2.1 to 4.5.</p><p><a href="2432s1.html" target="_self">Resources</a></p></div></div>
<div class="authorblurb"><p>
      <span   class="bold"><b>Mike Delves</b></span>
      (delves@nasoftware.co.uk) spent
      twenty-five years at the University of Liverpool as Professor of
      Computational Mathematics and Director of the Institute for
      Advanced Scientific Computation. His research interests included
      numerical methods and their implementation in high-level languages
      (successively Algol68, Ada, FORTRAN90 and HPF&mdash;parallelism crept
      increasingly in along the way). He started N.A. Software in 1978 as
      a hobby and is now full-time chairman; the company currently has 23
      employees. Linux represents its biggest single market for FORTRAN
      and HPF compilers.</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../045/toc045.html">Issue Table of Contents</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>