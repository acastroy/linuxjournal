<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Taming the Beast
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;The right plan can determine the difference between a large-scale&#10;system administration nightmare and a good night's sleep for you&#10;and your sysadmin team.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x2654580.0x274bac0"></a>
Taming the Beast
</h1></div><div><div class="author"><h3 class="author">
Jason
 
Allen
</h3></div><div class="issuemoyr">Issue #191, March 2010</div></div><div><p>
The right plan can determine the difference between a large-scale
system administration nightmare and a good night's sleep for you
and your sysadmin team.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274c2a8"></a></h2></div></div><p>
As the appetite for raw computing power continues to grow, so do the
challenges associated with managing large numbers of systems, both physical
and virtual. Private industry, government and scientific research
organizations are leveraging larger and larger Linux environments for
everything from high-energy physics data analysis to cloud computing.
Clusters containing hundreds or even thousands of systems are becoming
commonplace. System administrators are finding that the old way of doing
things no longer works when confronted with massive Linux deployments. We
are forced to rethink common tasks because the tools and strategies that
served us well in the past are now crushed by an army of penguins. As
someone who has worked in scientific computing for the past nine years, I
know that large-scale system administration can at times be a
nightmarish endeavor, but for those brave enough to tame the monster, it can
be a hugely rewarding and satisfying experience.
</p><p>
People often ask me, &ldquo;How is your department able to manage so many
machines with such a small number of sysadmins?&rdquo; The answer is that my
basic philosophy of large-scale system administration is &ldquo;keep things
simple&rdquo;. Complexity is the enemy. It almost always means more system
management overhead and more failures. It's fairly straightforward for a
single experienced Linux sysadmin to single-handedly manage a cluster of
a thousand machines, as long as all of the systems are identical (or nearly
identical). Start throwing in one-off servers with custom partitioning or
additional NICs, and things start to become more difficult, and the number of
sysadmins required to keep things running starts to increase.
</p><p>
An arsenal of weapons in the form of a complete box of system
administration tools and techniques is vital if you plan to manage a large
Linux environment effectively. In the past, you probably would be forced to
roll your own large-scale system administration utilities. The good news is
that compared to five or six years ago, many open-source
applications now make managing even large clusters relatively
straightforward.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274c618"></a>Monitoring</h2></div></div><p>
System administrators know that monitoring is essential. I think Linux
sysadmins especially have a natural tendency to be concerned with every
possible aspect of their systems. We love to watch the number of running
processes, memory consumption and network throughput on all our machines,
but in the world of large-scale system administration, this mindset can be a
liability. This is especially true when it comes to alerting. The problem
with alerting on every potential hiccup is that you'll either go insane
from the constant flood of e-mail and pages, or even worse, you'll start
ignoring the alerts. Neither of those situations is desirable. The
solution? Configure your monitoring system to alert only on actionable
conditions&mdash;things that cause an interruption in service. For every
monitoring check you enable, ask yourself &ldquo;What action must be taken if
this check triggers an alert?&rdquo; If the answer is &ldquo;nothing&rdquo;, it's
probably better not to enable the check.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274c7d0"></a></h2></div></div><div class="sidebar"><p class="title"><b>Monitoring Tools</b></p><p>
If you were asked to name the first monitoring application that
comes to mind, it probably would be Nagios. Used by just about everyone,
Nagios is currently the king of open-source monitoring tools.
</p><p>
Zabbix sports a slick Web interface that is sure to make any manager happy.
Zabbix scales well and might be posed to give Nagios a run for its money.
</p><p>
Ganglia is one of those must-have tools for Linux environments of any size.
Its strengths include trending and performance monitoring.
</p></div><p>
I think it's smart to differentiate monitoring further into critical and
noncritical alerts. E-mail and pager alerts should be reserved for things
that require immediate action&mdash;for example, important systems that aren't pingable, full
filesystems, degraded RAIDs and so on. Noncritical things, like NIS timeouts,
instead should be displayed on a Web page that can be viewed when you get back
from lunch. Also consider writing checks that automatically correct
whatever condition they are monitoring. Instead of your script sending you
an e-mail when Apache dies, why not have it try restarting httpd
automatically? If you go the auto-correcting &ldquo;self-healing&rdquo;
route, I'd
recommend logging whatever action your script takes so you can troubleshoot
the failure later.
</p><p>
When selecting a monitoring tool in a large environment, you have to think
about scalability. I have seen both Zabbix and Nagios used to monitor in
excess of 1,500 machines and implement tens of thousands of checks. Even
with these tools, you might want to scale horizontally by dividing your
machines into logical groups and then running a single monitoring server
per group. This will increase complexity, but if done correctly, it 
will also prevent your monitoring infrastructure from going up in flames.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274cb98"></a>Configuration Management</h2></div></div><p>
In small environments, you can maintain Linux systems successfully without a
configuration management tool. This is not the case in large
environments. If you plan on running a large number of Linux systems
efficiently, I strongly encourage you to consider a configuration management
system. There are currently two heavyweights in this area, Cfengine and
Puppet. Cfengine is a mature product that has been around for years, and it
works well. The new kid on the block is Puppet, a Ruby-based tool that is
quickly gaining popularity. Your configuration management tools should,
obviously, allow you to add or modify system or application configuration
files to a single system or groups of machines. Some examples of files you
might want to manage are /etc/fstab, ntpd.conf, httpd.conf or
/etc/password. Your tool also should be able to manage symlinks and
software packages or any other node attributes that change frequently.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274cca0"></a></h2></div></div><div class="sidebar"><p class="title"><b>Configuration Management Tools</b></p><p>
Cfengine is the grandfather of configuration management systems. The
project started in 1993 and continues to be actively developed. Although I
personally find some aspects of Cfengine a little clunky, I've been using
it successfully for many years.
</p><p>
Puppet is a highly regarded Ruby-based tool that should be considered by
anyone considering a configuration management solution.
</p></div><p>
Regardless of which configuration management tool you use, it's important
to implement it early. Managing Linux configurations is something that
should be set up as the node is being installed. Retrofitting configuration
management on a node that is already in production can be a dangerous
endeavor. Imagine pushing out an incorrect fstab or password file, and you
get an idea of what can go wrong. Despite the obvious hazards of fat-fingering a configuration management tool, the benefits far outweigh the
dangers. Configuration management tools provide a highly effective way of
managing Linux systems and can reduce system administration
overhead dramatically. 
</p><p>
As an added bonus, configuration management systems also
can be
used as a system backup mechanism of sorts. Granted, you don't want to store
large amounts of data in a tool like Cfengine, but in the event of system
failure, using a configuration managment tool in conjunction with your node
installation tools should allow you to get the system into a known good state
in a minimal amount of time.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274d010"></a>Provisioning</h2></div></div><p>
Provisioning is the process of installing the operating system on a machine
and performing basic system configuration. At home, you probably boot your
computer from a DVD to install the latest version of your favorite Linux
distro. Can you imagine popping a DVD in and out of a data center full of
systems?
Not appealing. A more efficient approach is to install the OS over the
network, and you typically do this with with a combination of PXE and
Kickstart. There are numerous tools to assist with large-scale
provisioning&mdash;Cobbler and Spacewalk are two&mdash;but you may prefer to roll your own. Your
provisioning tools should be tightly coupled to your configuration
management system. The ultimate goal is to be able to sit at your desk, run
a couple commands, and see a hundred systems appear on the network a few
minutes later, fully configured and ready for production. 
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274d170"></a></h2></div></div><div class="sidebar"><p class="title"><b>Provisioning Tools</b></p><p>
Rocks is a Linux distribution with built-in network installation
infrastructure. Rocks is great for quickly deploying large clusters of
Linux servers though it can be difficult to use in mixed Linux distro
environments.
</p><p>
Spacewalk is Red Hat's open-source systems management solution. In
addition to provisioning, Spacewalk also offers system monitoring and
configuration file management.
</p><p>
Cobbler, part of the Fedora Project, is a lightweight system installation
server that works well for installing physical and virtual systems.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274d430"></a>Hardware</h2></div></div><p>
When it's time to purchase hardware for your new Linux super cluster,
there are many things to consider, especially when it comes to choosing
a good vendor. When selecting vendors, be sure to understand their
support offerings fully. Will they come on-site to troubleshoot issues, or do
they expect you to sit for hours on the phone pulling your hair out while they
plod through an endless series of troubleshooting scripts? In my
experience, the best, most responsive shops have been local whitebox
vendors. It doesn't matter which route you go, large corporate or whitebox
vendor, but it's important to form a solid business relationship, because
you're going to be interacting with each other on a regular basis. 
</p><p>
The odds are that old hardware is more likely to fail than newer hardware.
In my shop, we typically purchase systems with three-year support contracts
and then retire the machines in year four. Sometimes we keep machines
around longer and simply discard a system if it experiences any type of
failure. This is particularly true in tight budget years.
</p><p>
Purchasing the latest, greatest hardware is always tempting, but I suggest
buying widely adopted, field-tested systems. Common hardware usually means
better Linux community support. When your network card starts flaking out,
you're more likely to find a solution to the problem if 100,000
other Linux users also have the same NIC. In recent years, I've been very
happy with the Linux compatibility and affordability of Supermicro systems.
If your budget allows, consider purchasing a system with hardware RAID and
redundant power supplies to minimize the number of after-hours pages. Spare
systems or excess hardware capacity are a must for large shops, because the
fact of the matter is regardless of the quality of hardware, systems will
fail.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274d5e8"></a>Backups</h2></div></div><p>
Rethink backups. More than likely, when confronted with a large Linux
deployment, you're going to be dealing with massive amounts of data.
Deciding what data to back up requires careful coordination with
stakeholders. Communicate with users so they understand backup
limitations. Obviously, written policies are a must, but the occasional
e-mail reminder is a good idea as well. As a general rule, you want
to back up only absolutely essential data, such as home directories, unless
requirements dictate otherwise.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274d6f0"></a>Serial Console Access</h2></div></div><p>
Although it may seem antiquated, do not underestimate the value of serial
console access to your Linux systems. When you find yourself in a
situation where you can't access a system via SSH or other remote-access
protocol, a good-old serial console potentially could be a lifesaver,
particularly if you manage systems in a remote data center. Equally
important is the ability to power-cycle a machine remotely. Absolutely
nothing is more frustrating than having to drive to the data center at 3am
to push the power button on an unresponsive system. 
</p><p>
Many hardware devices exist for power-cycling systems remotely. I've had
good luck with Avocent and APC products, but your mileage may vary. Going
back to our &ldquo;keep it simple&rdquo; mantra, no matter what solution you select,
try to standardize one particular brand if possible. More than likely,
you're going to write a wrapper script around your power-cycling utilities,
so you can do things like <tt  >powercycle node.example.com</tt>, and having just
a single hardware type keeps implementation more straightforward.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274d900"></a>System Administrators</h2></div></div><p>
No matter how good your tools are, a solid system administration team is
essential to managing any large Linux environment effectively. The number
of systems managed by my group has grown from about a dozen Linux nodes eight years ago
to roughly 4,000 today. We currently operate with an approximate
ratio of 500 Linux servers to every one system administrator, and we
do this while maintaining a high level of user satisfaction. This simply
wouldn't be possible without a skilled group of individuals. 
</p><p>
When hiring new team members, I look for Linux professionals, not
enthusiasts. What do I mean by that? Many people might view Linux as a
hobby or as a source of entertainment, and that's great! But the people on
my team see things a little differently. To them, Linux is an awesomely
powerful tool&mdash;a giant hammer that can be used to solve massive problems. The
professionals on my team are curious and always thinking about more
efficient ways of doing things. In my opinion, the best large-scale
sysadmin is someone who wants to automate any task that needs to be
repeated more than once, and someone who constantly thinks about the big
picture, not just the single piece of the puzzle that they happen to be
working on. Of course, an intimate knowledge of Linux is mandatory, as is a
wide range of other computing skills.
</p><p>
In any large Linux shop, there is going to be a certain amount of mundane,
low-level work that needs to be performed on a daily basis: rebooting hung
systems, replacing failed hard drives and creating new user accounts.
The majority of the time, these routine tasks are better suited to your
junior admins, but it's beneficial for more senior people to be involved
from time to time as they serve as a fresh set of eyes, potentially
identifying areas that can streamlined or automated entirely. Senior admins
should focus on improving system management efficiency, solving difficult
issues and mentoring other team members.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2654580.0x274db10"></a>Conclusion</h2></div></div><p>
We've touched a few of the areas that make large-scale Linux system
administration challenging. Node installing, configuration management and
monitoring are all particularly important, but you still need reliable
hardware and great people. Managing a large environment can be nerve-racking at
times, but never lose sight of the fact that ultimately, it's just
a bunch of Linux boxes.
</p></div></div>
<div class="authorblurb"><p>
Jason Allen is CD/SCF/FEF Department Head at Fermi National Accelerator
Laboratory, which is managed by Fermi Research Alliance, LLC, under
Management and Operating Contract (DE-AC02-07CH11359) with the Department
of Energy. He has been working with Linux professionally for the past 12
years and maintains a system administration blog at <a href="http://savvysysadmin.com" target="_self">savvysysadmin.com</a>.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../191/toc191.html">Issue Table of Contents</a>
    <a class="link3" href="../191/10665.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>