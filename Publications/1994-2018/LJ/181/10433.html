<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
Paranoid Penguin
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;Get a Squid caching proxy up and running, securely.&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x2523580.0x261aac0"></a>
Paranoid Penguin
</h1></div><div><h3 class="subtitle"><i>
Building a Secure Squid Web Proxy, Part II
</i></h3></div><div><div class="author"><h3 class="author">
Mick
 
Bauer
</h3></div><div class="issuemoyr">Issue #181, May 2009</div></div><div><p>
Get a Squid caching proxy up and running, securely.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2523580.0x261b460"></a></h2></div></div><p>
Last month, I began a series of articles on Squid Web proxy security by
introducing the theory, benefits and architecture of Web proxies. This
month, we dive right in to basic Squid installation, configuration
and testing, and begin hardening our Squid proxy.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2523580.0x261b618"></a>
What We're Doing (Review)</h2></div></div><p>
As you'll recall from last month, a Web proxy provides a control
point for restricting which external Web sites your users can reach.
It allows you to permit Web access without allowing non-Web traffic
(or even publishing a default route to the Internet), and it provides
a convenient place to perform content filtering and transaction logging.
</p><p>
As you also may recall, unlike a firewall, a Web proxy doesn't need to
be a physical choke point through which all traffic must pass for a
physical path to the outside. Instead, you can use firewall rules or
router ACLs that allow only Web traffic, as a means of ensuring your
users will use the proxy. Accordingly, your Web proxy can be set up like
any other server, with a single network interface.
</p><p>
This is the case with the Web server I show you how to build in
this and subsequent columns. This month, we focus on Squid itself;
we'll cover add-ons like SquidGuard in future columns.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2523580.0x261b7d0"></a>
Obtaining and Installing Squid</h2></div></div><p>
So, where do you get Squid software? Naturally, the Squid Web site (see
Resources) is the definitive source. But, because Squid has been the gold
standard for Linux Web proxies for so many years, chances are it's
a fully supported package in your Linux distribution of choice. If so,
that's how I recommend getting it; it's easier to keep it patched that
way, and you'll have greater assurance of stability and compatibility
with the other things on your system.
</p><p>
On Ubuntu and other Debian variants (not to mention Debian itself), you need
the packages squid and squid-common. On Red Hat and its variants,
you need the package squid. And, on SUSE and OpenSUSE systems,
you need squid.
</p><p>
At the time of this writing, all three of these families of distributions (Debian,
Red Hat and SUSE) are maintaining separate packages for Squid version 3;
the packages cited above are for version 2. This is because although the
Squid development team recently declared Squid 3.0 to be a stable release
(in November 2008), at the time of these three distributions' most recent
production releases, Squid 3.0 still was considered to be a beta code
branch, with 2.6 or 2.7 as the preferred production versions.
</p><p>
On the one hand, by the time you read this, Squid 3.0 (maybe even
3.1, which is in beta right now) may be mainstreamed into your Linux
distribution of choice. On the other hand, maybe not. So for now,
I'm going to use examples from Squid 2.6.18, the version on my Ubuntu
system. They still should be perfectly valid for later
versions&mdash;generally, later versions have
<span   class="emphasis"><em>additional</em></span> options and features,
not <span   class="emphasis"><em>replaced</em></span> options. I can cover Squid 3.0 in a future column.
</p><p>
I leave it to you to use the package manager of choice to install
Squid packages on your RPM-based system, but on Debian-based systems,
the most direct way is usually with the command:

<pre     class="programlisting">
bash-$ sudo apt-get install squid
</pre>
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2523580.0x261bb40"></a></h2></div></div><p>
(apt-get automatically will determine that it also needs squid-common and
will install that too.)
</p><p>
By the way, you do <span   class="emphasis"><em>not</em></span> need to install Apache or any other Web
server package on your Squid server, unless, of course, you're also going to 
use it as a Web server or want to use some Web-based administration tool
or another. Squid itself does not need any external Web server software
or libraries in order to proxy and cache Web connections.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2523580.0x261bd50"></a>
Configuring Squid: Basic Functionality</h2></div></div><p>
Creating a basic, working configuration for Squid isn't much harder than
installing it. Like so much else in Linux, it's a matter of making small
changes to a single text file, in this case, squid.conf. In all three
distribution families I mentioned, its full path is /etc/squid/squid.conf.
</p><p>
To get started, first open a command window, and back up the default
squid.conf file (non-Ubuntu users can <tt  >su</tt> to root and
omit the <tt  >sudo</tt>
from these examples):


<pre     class="programlisting">
bash-$ cd /etc/squid
bash-$ sudo cp squid.conf squid.conf.default
</pre>
</p><p>
Next, open squid.conf with your text editor of choice. You actually
may prefer a graphical editor, such as gedit, but I've always used vi for its
simplicity and ubiquity&mdash;if it's UNIX-like, it's got vi.
</p><p>
(Note to the emacs-loving alpha geeks among you: yes, emacs is more
powerful; it's written in LISP; God kills a kitten every time someone
installs Gvim; you win! But, I still like vi.)
</p><p>
Believe it or not, all you need to do to get Squid running is add two
lines to the ACL (Access Control List) section of this file: an object
definition that describes your local network and an ACL allowing
members of this object to use your proxy. For my network, these lines
look like this:

<pre     class="programlisting">
acl mick_network src 10.0.2.0/24
http_access allow mick_network
</pre>
</p><p>
The first line is the object definition. The <tt  >acl</tt>
signifies that I'm 
about to define an ACL object. <tt  >mick_network</tt> is the name I've chosen for
this object. <tt  >src</tt> means that it represents the IP address or range of
addresses of hosts initiating TCP transactions with my proxy (that is,
proxy clients). Finally, <tt  >10.0.2.0/24</tt> is my LAN's network address in
CIDR notation, which in this case translates to &ldquo;the range of IP
addresses from 10.0.2.1 through 10.0.2.254&rdquo;.
</p><p>
The second line declares an actual ACL: allow transactions involving the
object mick_network&mdash;that is, transactions initiated by hosts having
IP addresses in the range 10.0.2.1 through 10.0.2.254.
</p><p>
If more than one network address comprises your local network, you can
specify them as a space-delimited list at the end of the acl statement,
for example:

<pre     class="programlisting">
acl mick_network src 10.0.2.0/24 192.168.100.0/24
</pre>
</p><p>
Because ACLs are parsed in the order in which they appear (going from top to
bottom) in squid.conf, do <span   class="emphasis"><em>not</em></span> simply add these acl and http_access
lines to the very end of squid.conf, which will put them after the default
&ldquo;http_access deny all&rdquo; statement that ends the ACL portion of the default
squid.conf file. On my Ubuntu system, this statement is on line 641,
so I inserted my custom acl and http_access lines right above that.
</p><p>
In case you haven't guessed, <tt  >all</tt> is a wild-card ACL object that means
&ldquo;all sources, all ports, all destinations&rdquo; and so forth. Any transaction
that is evaluated against any http_access statement containing
<tt  >any</tt>
<span   class="emphasis"><em>will</em></span> match it, and in this case, will be dropped,
unless, of course,
it matches a preceding http_access line.
</p><p>
Now that you've created an object and ACL for your local network, you
should save squid.conf and then restart Squid by typing this command
(see earlier note about su root shells vs. sudo):

<pre     class="programlisting">
bash-$ sudo /etc/init.d/squid restart
</pre>
</p><p>
In fact, if you're editing squid.conf from a <tt  >sudo vi
squid.conf</tt>
session, you don't even need to leave your editing session; just do a
<tt  >:w</tt> to save your work, then type <tt  >:!
/etc/init.d/squid restart</tt> to
restart Squid from within vi.
</p><p>
To test whether things are working, you need to configure a machine
other than the proxy itself to use your proxy. (Squid comes configured
by default to allow transactions from 127.0.0.1, the local loopback
address, to be proxied.)
</p><p>
Figure 1 shows the dialog for setting up Firefox to use our example proxy.
</p><div       class="mediaobject"><img src="10433f1.jpg"><div class="caption"><p>
Figure 1. Setting Up Firefox to Use Proxies
</p></div></div><p>
In Figure 1, we've selected Manual proxy configuration and entered in
an HTTP Proxy address (which can be either a hostname or IP address) of
10.0.2.2 and Port number 3128, which is Squid's default listening port for
client connections. We've also selected the box to Use this proxy server
for all protocols, resulting in the same values being
copied automatically to the subsequent settings for other types of proxies.
</p><p>
We've left No Proxy for: at its default value of localhost,
127.0.0.1. The reason for not proxying connections to Web pages hosted
locally on the client system is probably obvious, but you can additionally
list URLs or IP addresses elsewhere on your local network that there's
no need to use the proxy to reach.
</p><p>
At this point, you may be wondering, what does the connection between
a client and a Web proxy look like? Is there some special protocol,
or maybe a subset of HTTP commands or flags?
</p><p>
In fact, proxy connections are simpler than you may think. Normally, when
you click on a hyperlink or enter a URL, your browser resolves the URL
you typed or clicked on, using its own local DNS capabilities. It then
takes the IP address and sends an HTTP/HTTPS request to that IP address,
with the <span   class="emphasis"><em>original</em></span> (non-resolved) URL in the body of the request.
</p><p>
A proxied connection is the same without any DNS resolution. Your browser
simply sends its HTTP/HTTPS request to the proxy server without trying
to resolve the URL. The body of that request is
<span   class="emphasis"><em>identical</em></span> to the one
it would otherwise send directly to the Web server you're trying to reach.
</p><p>
Instead of configuring your Web browser's proxy settings directly, if
you use the GNOME desktop on your client test system, you can set global
proxy settings that can, in turn, be used by Firefox and other Internet
applications. Note, however, that the proxy settings you set in GNOME
will be applied only to applications that are, in turn, configured to use
system settings&mdash;for example, by selecting the option Use system
proxy settings shown in Figure 1. Other applications will continue to use
either their own proxy settings or no proxy at all.
</p><p>
GNOME's Network Proxy Preferences applet, which should appear in your
System&rarr;Preferences menu, is shown in Figure 2.
</p><div       class="mediaobject"><a href="10433f2.large.jpg"><img src="10433f2.jpg"></a><div class="caption"><p>
Figure 2. Setting Global Proxy Options in GNOME
</p></div></div><p>
It may seem like I'm spending a lot of ink explaining client-side
configuration just for testing purposes, given that this is an article
about building Squid servers. But, of course, the way you set up a proxy
client for testing is the same as for one in production, so I would have
had to explain this sooner or later anyhow.
</p><p>
In fact, future installments in this series may go further in covering
client configuration topics. Autoproxy.pac files, for example (which
is what Figure 1's Automatic proxy configuration URL setting is for),
can be very handy in managing very complex or very highly scaled proxy
environments.
</p><p>
Once you've configured your test client system to use your Squid proxy,
you can attempt to navigate to some Web page to see if everything
works. It's a good idea to tail Squid's access log simultaneously. To
do so, enter this command on your Squid system:

<pre     class="programlisting">
bash-$ sudo tail -f /var/log/squid/access.log
</pre>
</p><p>
If browsing works but nothing zings by in this log-tailing session,
your client-side configuration is incorrect&mdash;it isn't actually using
the proxy. If browsing doesn't work, you may see some useful server-side
message in the log-tailing session. Squid usually returns fairly useful
messages directly to client browsers as well.
</p><p>
If things don't work, your browser session is simply timing out and
nothing is showing up in access.log, try using the ping command from
your client to your proxy and vice versa. If pinging doesn't work,
the problem is at the network level and has nothing to do with Squid.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2523580.0x2a15548"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Squid's Performance Benefits</b></p><p>
The Paranoid Penguin is a security column, so naturally, security is our
primary focus in dealing with Squid (or it will be, once I've walked
you through the basics of getting it up and running). But, you should
be aware that Squid is not a security application per se. Squid's main
purpose in life is to cache commonly accessed Web and FTP content locally,
thereby both reducing Internet bandwidth usage and speeding up end users'
download times.
</p><p>
The negative side of this is that Squid doesn't have as rich of a security
feature set built in to it as commercial security-oriented Web proxies,
such as BlueCoat and Sidewinder. In fact, Squid (years ago) used to ship
with a default configuration that allowed completely open access.
</p><p>
The good side is that Squid can be configured, especially along with
add-ons like SquidGuard, to provide some of the most important Web proxy
security features. And, even if those features are your main reason for
deploying Squid, you'll still enjoy the performance benefits of having
commonly accessed Web content cached locally by Squid.
</p><p>
Seldom, in the security business, do we enhance end users' experience
when we add security controls.
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2523580.0x2a15860"></a>
Conclusion</h2></div></div><p>
With any luck, at this point, chances are that everything works! Your
Squid proxy software is installed, configured to accept only client
connections from itself and from hosts on your local network,
and it's hard at work proxying your users' connections and caching
commonly accessed content. Not a bad day's work!
</p><p>
Not difficult, was it? Like most server applications, Squid's default
configuration file is designed to maximize your chances for success, while
minimizing the odds of your shiny-new Squid server being hacked. But, also
like other server applications, there's certainly more that you
<span   class="emphasis"><em>can</em></span>
and <span   class="emphasis"><em>should</em></span> do to secure your Squid proxy than the default settings
will do for you.
</p><p>
That will be our starting point next month. Among other things, we'll
delve much deeper into Squid's Access Control List features to further
harden Squid. Until then, be safe!
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x2523580.0x2a15ac8"></a></h2></div></div><div class="sidebar"><p class="title"><b>
Resources</b></p><p>
The Squid home page, where you can obtain the latest source code and
binaries for Squid:
<a href="http://www.squid-cache.org" target="_self">www.squid-cache.org</a>
</p><p>
The Ubuntu Server Guide's Squid Chapter:
<a href="https://help.ubuntu.com/8.10/serverguide/C/squid.html" target="_self">https://help.ubuntu.com/8.10/serverguide/C/squid.html</a>
</p><p>
The Squid User's Guide: <a href="http://www.deckle.co.za/squid-users-guide/Main_Page" target="_self">www.deckle.co.za/squid-users-guide/Main_Page</a>
</p></div></div></div>
<div class="authorblurb"><p>
Mick Bauer (<a href="mailto:darth.elmo@wiremonkeys.org">darth.elmo@wiremonkeys.org</a>) is Network
Security
Architect for one of the US's largest banks. He is the author of
the O'Reilly book <span   class="emphasis"><em>Linux Server Security</em></span>, 2nd edition
(formerly called
<span   class="emphasis"><em>Building Secure Servers With Linux</em></span>), an occasional
presenter at
information security conferences and composer of the &ldquo;Network
Engineering Polka&rdquo;.

</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../181/toc181.html">Issue Table of Contents</a>
    <a class="link3" href="../181/10433.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>