<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"><html><head><META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>
ZFS and BTRFS
</title><link rel="stylesheet" href="../css/archive.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.57.0"><meta name="description" content="&#10;BTRFS and ZFS are two options for protecting against data corruption. Which&#10;one&#10;should you use, and how should you use it?&#10;"><link rel="stylesheet" href="../../css/archive.css" type="text/css"><script type="text/javascript" src="../../js/archive.js"></script><script type="text/javascript" src="../../js/highlight.js"></script></head><body onload="search_highlight();">
  <div class="headerdiv">
    <a href="../../index.html">
      <img class="topimg" src="../../images/CD_HeaderBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="tophrdiv">
  </div>
  
  <div id="top_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  <div class="article" lang="en"><div class="titlepage"><div><h1 class="title"><a name="N0x23a6580.0x249dac0"></a>
ZFS and BTRFS
</h1></div><div><div class="author"><h3 class="author">
Russell
 
Coker
</h3></div><div class="issuemoyr">Issue #245, September 2014</div></div><div><p>
BTRFS and ZFS are two options for protecting against data corruption. Which
one
should you use, and how should you use it?
</p></div></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x23a6580.0x249e1f8"></a></h2></div></div><p>
For a long time, the software RAID implementation in the Linux kernel has
worked well to protect data against drive failure. It provides great
protection against a drive totally failing and against the situation where
a drive returns read errors. But what it doesn't offer is protection
against silent data corruption (where a disk returns corrupt data and
claims it to be good). It also doesn't have good support for the
possibility of drive failure during RAID reconstruction. 
</p><p>
Drives have been
increasing in size significantly, without comparable increases in speed.
Modern drives have contiguous read speeds 300 times faster than drives from
1988 but are 40,000 times larger (I'm comparing a recent 4TB SATA disk with
a 100M ST-506 disk that can sustain 500K/s reads). So the RAID rebuild time
is steadily increasing, while the larger storage probably increases the risk
of data corruption.
</p><p>
Currently, there are two filesystems available on Linux that support
internal RAID with checksums on all data to prevent silent corruption: ZFS
and BTRFS. ZFS is from Sun and has some license issues, so it isn't included in
most Linux distributions. It is available from the ZFS On Linux Web site (<a href="http://zfsonlinux.org" target="_self">zfsonlinux.org</a>).
BTRFS has no license problems and is included in most recent distributions,
but it is at an earlier stage of development. When discussing BTRFS in this
article, I concentrate on the theoretical issues of data integrity and
not the practical issues of kernel panics (which happen regularly to me but
don't lose any data).
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x23a6580.0x249e408"></a>
Do Drives Totally Fail?</h2></div></div><p>
For a drive totally to fail (that is, be unable to read any data successfully at
all), the most common problem used to be &ldquo;stiction&rdquo;. That is when
the heads stick to the platters, and the drive motor is unable to spin the
disk. This seems to be very uncommon in recent times. I presume that drive
manufacturers fixed most of the problems that caused it.
</p><p>
In my experience, the most common reason for a drive to become totally
unavailable is due to motherboard problems, cabling or connectors&mdash;that is,
problems outside the drive. Such problems usually can be fixed but
may cause some downtime, and the RAID array needs to keep working with a
disk missing.
</p><p>
Serious physical damage (for example, falling on concrete) can cause a drive to
become totally unreadable. But, that isn't a problem that generally happens
to a running RAID array. Even when I've seen drives fail due to being in an
uncooled room in an Australian summer, the result has been many bad sectors,
not a drive that's totally unreadable. It seems that most drive
&ldquo;failures&rdquo; are really a matter of an increasing number of bad
sectors.
</p><p>
There aren't a lot of people who can do research on drive failure.
An individual can't just buy a statistically significant number of
disks and run them in servers for a few years. I couldn't find any
research on the incidence of excessive bad sectors vs. total drive failure.
It's widely regarded that the best research on the incidence of hard drive
&ldquo;failure&rdquo; is the Google Research paper &ldquo;Failure Trends in a
Large Disk Drive Population&rdquo; (<a href="http://research.google.com/pubs/pub32774.html" target="_self">research.google.com/pubs/pub32774.html</a>), which although very informative, gives no
information on the causes of &ldquo;failure&rdquo;. Google defines
&ldquo;failure&rdquo; as anything other than an upgrade that causes a drive to
be replaced. Not only do they not tell us the number of drives that totally
died vs. the number that had some bad sectors, but they also don't tell us
how many bad sectors would be cause for drive replacement.
</p><p>
Lakshmi N. Bairavasundaram, Garth R. Goodson, Bianca Schroeder, Andrea
C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau from the University of
Wisconsin-Madison wrote a paper titled &ldquo;An Analysis of Data Corruption
in the Storage Stack&rdquo; (<a href="http://research.cs.wisc.edu/adsl/Publications/corruption-fast08.html" target="_self">research.cs.wisc.edu/adsl/Publications/corruption-fast08.html</a>). That paper gives a lot of information about when
drives have corrupt data, but it doesn't provide much information about
the case of major failure (tens of thousands of errors), as distinct from
cases where there are dozens or hundreds of errors. One thing it does say
is that the 80th percentile of latent sector errors per disk with errors is
&ldquo;about 50&rdquo;, and the 80th percentile of checksum mismatches for disks
with errors is &ldquo;about 100&rdquo;. So most disks with errors have only a
very small number of errors. It's worth noting that this research was
performed with data that NetApp obtained by analyzing the operation of
its hardware in the field. NetApp has a long history of supporting a
large number of disks in many sites with checksums on all stored data.
</p><p>
I think this research indicates that the main risks of data loss are
corruption on disk or a small number of read errors, and that total drive
failure is an unusual case.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x23a6580.0x249ea90"></a>
Redundancy on a Single Disk</h2></div></div><p>
By default, a BTRFS filesystem that is created for a single device that's
not an SSD will use &ldquo;dup&rdquo; mode for metadata. This means that every
metadata block will be written to two parts of the disk. In practice, this
can allow for recovering data from drives with many errors. I recently had
a 3TB disk develop about 14,000 errors. In spite of such a large number of
errors, the duplication of metadata meant that there was little data loss.
About 2,000 errors in metadata blocks were corrected with the duplicates,
and the 12,000 errors in data blocks (something less than 48M of data) was
a small fraction of a 3TB disk. If an older filesystem was used in that
situation, a metadata error could corrupt a directory and send all it's
entries to lost+found.
</p><p>
ZFS supports even greater redundancy via the <tt  >copies=</tt> option. If
you specify <tt  >copies=2</tt> for a filesystem, then every data block will
be written to two different parts of the disk. The number of copies of
metadata will be one greater than the number of copies of data, so
<tt  >copies=2</tt> means that there will be three copies of every metadata
block. The maximum number of copies for data blocks in ZFS is three, which means
that the maximum number of copies of metadata is four.
</p><p>
The paper &ldquo;An Analysis of Data Corruption in the Storage
Stack&rdquo;
shows that for &ldquo;nearline&rdquo; disks (that is, anything that will be in a
typical PC or laptop), you can expect a 9.5% probability of read errors
(latent sector errors) and a 0.466% probability of silent data corruption
(checksum mismatches). The typical <span   class="emphasis"><em>Linux Journal</em></span> reader
probably can expect
to see data loss from hard drive read errors on an annual basis from the
PCs owned by their friends and relatives. The probability of silent data
corruption is low enough that every user has a less than 50% chance of
seeing it on their own PC during their life&mdash;unless they purchased one
of the disks with a firmware bug that corrupts data.
</p><p>
If you run BTRFS on a system with a single disk (for example, a laptop), you can
expect that if the disk develops any errors, they will result in no metadata
loss due to duplicate metadata, and any file data that is lost will be
reported to the application by a file read error. If you run ZFS on a
single disk, you can set <tt  >copies=2</tt> or
<tt  >copies=3</tt> for
the filesystem that contains your most important data (such as /home on a
workstation) to decrease significantly the probability that anything less
than total disk failure will lose data. This option of providing extra
protection for data is a significant benefit for ZFS when compared to
BTRFS.
</p><p>
If given a choice between a RAID-1 array with Linux software RAID (or any
other RAID implementation that doesn't support checksums) and a single
disk using BTRFS, I'd choose the single disk with BTRFS in most cases.
That is because on a single disk with BTRFS, the default configuration is to
use &ldquo;dup&rdquo; for metadata. This means that a small number of disk
errors will be unlikely to lose any metadata, and a scrub will tell you
which file data has been lost due to errors. Duplicate metadata alone can
make the difference between a server failing and continuing to run. It is
possible to run with &ldquo;dup&rdquo; for data as well, but this isn't a
well supported configuration (it requires mixed data and metadata chunks
that require you to create a very small filesystem and grow it).
</p><p>
It is possible to run RAID-1 on two partitions on a single disk if you are
willing to accept the performance loss. I have a 2TB disk running as a 1TB
BTRFS RAID-1, which has about 200 bad sectors and no data loss.
</p><p>
Finally, it's worth noting that a &ldquo;single disk&rdquo; from the filesystem
perspective can mean a RAID array. There's nothing wrong with running BTRFS
or ZFS over a RAID-5 array. The metadata duplication that both those
filesystems offer will reduce the damage if a RAID-5 array suffers a read
error while replacing a failed disk. A hardware RAID array can offer
features that ZFS doesn't offer (such as converting from RAID-1 to RAID-5
and then RAID-6 by adding more disks), and hardware RAID arrays often
include a write-back disk cache that can improve performance
for RAID-5/6 significantly. There's also nothing stopping you from using BTRFS or ZFS
RAID-1 over a pair of hardware RAID-5/6 arrays.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x23a6580.0x249f220"></a>
Drive Replacement</h2></div></div><p>
When you replace a disk in Linux Software RAID, the old disk will be marked
as faulty first, and all the data will be reconstructed from other
disks.
This is fine if the other disks are all good, but if the other disks have
read errors or corrupt data, you will lose data. What you really need is
to have the new disk directly replace the old disk, so the data for the new
disk can be read from the old disk or from redundancy in the array,
whichever works.
</p><p>
ZFS has a <tt  >zpool replace</tt> command that will rebuild the array from
the contents of the old disk and from the other disks in a redundant set.
BTRFS supports the same thing with the <tt  >btrfs replace</tt> command. In
the most common error situations (where a disk has about 50 bad sectors),
this will give you the effect of having an extra redundant disk in the
array. So a RAID-5 array in BTRFS or in ZFS (which they call a RAID-Z)
should give as much protection as a RAID-6 array in a RAID implementation
that requires removing the old disk before adding a new disk. At this time,
RAID-5 and RAID-6 support in BTRFS is still fairly new, and I don't expect
it to be ready to use seriously by the time this article is published. But the
design of RAID-5 in BTRFS is comparable to RAID-Z in ZFS, and they should
work equally well when BTRFS RAID-5 code has been adequately tested and
debugged.
</p><p>
Hot spare disks are commonly used to allow replacing a disk more quickly
than someone can get to the server. The idea is that the RAID array might
be reconstructed before anyone even can get to it. But it seems to me that
the real benefit of a hot-spare when used with a modern filesystem, such as
ZFS or BTRFS, is that the system has the ability to read from the disk with
errors as well as the rest of the array while constructing the new disk. If
you have a server where every disk bay contains an active disk (which is a
very common configuration in my experience), it is unreasonably
difficult to support a disk replacement operation that reads from the
failing disk (using an eSATA device for the rebuild isn't easy). Note that
BTRFS doesn't have automatic hot-spare support yet, but it presumably will
get it eventually. In the meantime, a sysadmin has to instruct it
to replace the disk manually.
</p><p>
As modern RAID systems (which on Linux servers means ZFS as the only fully
functional example at this time) support higher levels of redundancy, one
might as well use RAID-Z2 (the ZFS version of RAID-6) instead of RAID-5
with a hot spare, or a RAID-Z3 instead of a RAID-6 with a hot-spare. When a
disk is being replaced in a RAID-6/RAID-Z2 array with no hot-spare, you are
down to a RAID-5/RAID-Z array, so there's no reason to use a disk as a
hot-spare instead of using it for extra redundancy in the array.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x23a6580.0x249f538"></a>
How Much Redundancy Is Necessary?</h2></div></div><p>
The way ZFS works is that the <tt  >copies=</tt> option (and the related
metadata duplication) is applied on top of the RAID level that's used for
the storage &ldquo;pool&rdquo;. So if you use <tt  >copies=2</tt> on a ZFS filesystem that
runs on a RAID-1, there will be two copies of the data on each of the disks.
The allocation of the copies is arranged such that it covers different
potential failures to the RAID level, so if you had
<tt  >copies=3</tt> for data
stored on a three-disk RAID-Z pool, each disk in the pool would have a copy
of the data (and parity to help regenerate two other copies). The amount of
space required for some of these RAID configurations is impractical for
most users. For example, a RAID-Z3 array composed of six 1TB disks would have
3TB of RAID-Z3 capacity. If you then made a ZFS filesystem with
<tt  >copies=3</tt>,
you would get 1TB of usable capacity out of 6TB of disks. 5/6 disks is
more redundancy than most users need.
</p><p>
If data is duplicated in a RAID-1 array, the probability of two disks having
errors on matching blocks from independent random errors is going to be
very low. The paper from the University of Wisconsin-Madison notes that
firmware bugs can increase the probability of corrupt data on matching
blocks and suggests using staggered stripes to cover that case. ZFS does
stagger some of its data allocation to deal with that problem. Also, it's
fairly common for people to buy disks from two different companies for a
RAID-1 array to prevent a firmware bug or common manufacturing defect from
corrupting data on two identical drives. The probability of both disks in a
BTRFS RAID-1 array having enough errors that data is lost is very low. With
ZFS, the probability is even lower due to the mandatory duplication of
metadata on top of the RAID-1 configuration and the option of duplication
of data. At this time, BTRFS doesn't support duplicate metadata on a RAID
array.
</p><p>
The probability of hitting a failure case that can't be handled by RAID-Z2
but that can be handled by RAID-Z3 is probably very low. In many
deployments, the probability of the server being stolen or the building
catching on fire will be greater than the probability of a RAID-Z2 losing
data. So it's worth considering when to spend more money on extra disks and
when to spend money on better off-site backups.
</p><p>
In 2007, Val Bercovici of NetApp suggested in a StorageMojo interview that
&ldquo;protecting online data only via RAID 5 today verges on professional
malpractice&rdquo; (<a href="http://storagemojo.com/2007/02/26/netapp-weighs-in-on-disks" target="_self">storagemojo.com/2007/02/26/netapp-weighs-in-on-disks</a>). During the past seven years, drives have become bigger, and the
difficulties we face in protecting data have increased. While Val's claim
is hyperbolic, it does have a basis in fact. If you have only the RAID-5
protection (a single parity block protecting each stripe), there is a
risk of having a second error before the replacement disk is brought
on-line. However, if you use RAID-Z (the ZFS equivalent of RAID-5), every
metadata block is stored at least twice in addition to the RAID-5 type
protection, so if a RAID-Z array entirely loses a disk and then has a read
error on one of the other disks, you might lose some data but won't lose
metadata. For metadata to be lost on a RAID-Z array, you need to have one
disk die entirely and then have matching read errors on two other disks. If
disk failures are independent, it's a very unlikely scenario. If,
however, the disk failures are not independent, you could have a problem
with all disks (and lose no matter what type of RAID you use).
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x23a6580.0x249fa08"></a>
Snapshots</h2></div></div><p>
One nice feature of BTRFS and ZFS is the ability to make snapshots of BTRFS
subvolumes and ZFS filesystems. It's not difficult to write a cron job that
makes a snapshot of your important data every hour or even every few
minutes. Then when you accidentally delete an important file, you easily
can get it back. Both BTRFS and ZFS can be configured such that files can be
restored from snapshots without root access so users can recover their own
files without involving the sysadmin.
</p><p>
Snapshots aren't strictly related to the the topic of data integrity, but
they solve the case of accidental deletion, which is the main reason for
using backups. From a sysadmin perspective, snapshots and RAID are entirely
separate issues. From the CEO perspective, &ldquo;is the system working or
not?&rdquo;,
they are part of the same issue.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x23a6580.0x249fc18"></a>
Comparing BTRFS and ZFS</h2></div></div><p>
For a single disk in a default configuration, both BTRFS and ZFS will store
two copies of each metadata block. They also use checksums to detect when
data is corrupted, which is much better than just providing corrupt data to
an application and allowing errors to propagate. ZFS supports storing as
many as three copies of data blocks on a single disk, which is a significant
benefit.
</p><p>
For a basic RAID-1 installation, BTRFS and ZFS offer similar features by
default (storing data on both devices with checksums to cover silent
corruption). ZFS offers duplicate metadata as a mandatory feature and the
option of duplicate data on top of the RAID configuration.
</p><p>
BTRFS supports RAID-0, which is a good option to have when you are working
with data that is backed up well. The combination of the use of BTRFS
checksums to avoid data corruption and RAID-0 for performance would be good
for a build server or any other system that needs large amounts of
temporary file storage for repeatable jobs but for which avoiding data
corruption is important.
</p><p>
BTRFS supports dynamically increasing or decreasing the size of the
filesystem. Also, the filesystem can be rebalanced to use a different RAID
level (for example, migrating between RAID-1 and RAID-5). ZFS, however, has a very
rigid way of managing storage. For example, if you have a RAID-1 array in a
pool, you can never remove it, and you can grow it only by replacing all the
disks with larger ones. Changing between RAID-1 and RAID-Z in ZFS requires
a backup/format/restore operation, while on BTRFS, you can just add new disks
and rebalance.
</p><p>
ZFS supports different redundancy levels (via the <tt  >copies=</tt> setting) on
different &ldquo;filesystems&rdquo; within the same &ldquo;pool&rdquo; (where
a &ldquo;pool&rdquo; is group of
one or more RAID sets). BTRFS &ldquo;subvolumes&rdquo; are equivalent in design to ZFS
&ldquo;filesystems&rdquo;, but BTRFS doesn't support different RAID parameters for
subvolumes at this time.
</p><p>
ZFS supports RAID-Z and RAID-Z2, which are equivalent to BTRFS RAID-5,
RAID-6&mdash;except that RAID-5 and RAID-6 are new on BTRFS, and many people
aren't ready to trust important data to them. There is no feature in BTRFS
or planned for the near future that compares with RAID-Z3 on ZFS. There
are plans for future development of extreme levels of redundancy in BTRFS
at some future time, but it probably won't happen soon.
</p><p>
Generally, it seems that ZFS is designed to offer significantly greater
redundancy than BTRFS supports, while BTRFS is designed to be easier to
manage for smaller systems.
</p><p>
Currently, BTRFS doesn't give good performance. It lacks read optimization
for RAID-1 arrays and doesn't have any built-in support for using SSDs to
cache data from hard drives. ZFS has many performance features and is as
fast as a filesystem that uses so much redundancy can be.
</p><p>
Finally, BTRFS is a new filesystem, and people are still finding bugs in
it&mdash;usually not data loss bugs but often bugs that interrupt service. I haven't
yet deployed BTRFS on any server where I don't have access to the console,
but I have Linux servers running ZFS in another country.
</p></div><div class="simplesect" lang="en"><div class="titlepage"><div><h2 class="title"><a name="N0x23a6580.0x2496a80"></a></h2></div></div><div class="sidebar"><p class="title"><b></b></p><p>Send comments or feedback via <a href="http://www.linuxjournal.com/contact" target="_self">www.linuxjournal.com/contact</a> or to
<a href="mailto:info@linuxjournal.com">info@linuxjournal.com</a>.
</p></div></div></div>
<div class="authorblurb"><p>
Russell Coker has been working on NSA Security Enhanced Linux since 2001
and
has been working on the Bonnie++ benchmark suite since 1999.
</p></div>

  <div class="toclinks">
    <a class="link1" href="../tocindex.html">Archive Index</a>
    <a class="link2" href="../245/toc245.html">Issue Table of Contents</a>
    <a class="link3" href="../245/11708.html">Article</a>
  </div>
  <div class="bottomhrdiv">
  </div>
  
  <div id="bottom_search">
  <table class="page_search" summary="">
    <tr>
      <td valign="top" align="left">
        <p class="small_shutdown"><a href="/.exit">Shutdown Archive web server</a></p>
      </td>
      <td valign="top" align="right">
        <form method="get" action="/zoom/search.cgi">
          <input type="hidden" name="zoom_sort" value="0" />
          <input type="hidden" name="zoom_xml" value="0" />
          <input type="hidden" name="zoom_per_page" value="10" />
          <input type="hidden" name="zoom_and" value="1" />
          Search: <input type="text" name="zoom_query" size="20" value="" class="zoom_searchbox" />
          <input type="submit" value="Submit" />
        </form>
      </td>
    </tr>
  </table>
  </div>
  
  <div class="footerdiv">
    <a href="../../index.html">
      <img class="bottomimg" src="../../images/CD_FooterBanner.png" alt="LJ Archive"/>
    </a>
  </div>
  
  <div class="copyright">
    Copyright &copy; 1994 - 2018 <cite>Linux Journal</cite>.  All rights reserved.
  </div>
  </body></html>